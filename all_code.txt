# tests/test_energy_model.py
import pytest
import numpy as np
from typing import Optional
import logging
from src import energy_model, constants, config_manager, utils

# Define a common mock config for tests in this file
@pytest.fixture
def mock_energy_config():
    return {
        "vascular_properties": {
            "blood_viscosity": 0.004,  # Pa.s - Use a slightly different value for testing
            "min_radius": 0.001,       # mm
            "k_murray_scaling_factor": 0.6, # s^(1/3)
            "murray_law_exponent": 3.0,
            "min_segment_length": 0.01
        },
        "gbo_growth": {
            # Ensure this C_met is in units consistent with E_flow from calculate_segment_flow_energy
            # If E_flow inputs (L,R,Q) are mm, mm, mm^3/s and mu is Pa.s, E_flow is in (Pa.mm^3/s) or (10^-9 W)
            # Then C_met here should be in (Pa.mm^3/s) / mm^3 = Pa/s (if we treat energy as power)
            # Or if energy is just a "cost value", units need to be balanced by user.
            # Let's pick a value for testing.
            "energy_coefficient_C_met_vessel_wall": 100.0, # Arbitrary units, assuming consistency
            "bifurcation_candidate_points": 5 
        }
    }

# --- Tests for calculate_segment_flow_energy ---
def test_calc_flow_energy_known_values(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    # L=1mm, R=0.1mm, Q=0.01 mm^3/s
    length, radius, flow = 1.0, 0.1, 0.01
    # Expected: (8 * 0.004 * 1.0 * 0.01^2) / (pi * 0.1^4)
    # = (8 * 0.004 * 1e-4) / (pi * 1e-4) = (3.2e-6) / (pi * 1e-4) = 3.2e-2 / pi 
    expected_energy = (8 * viscosity * length * flow**2) / (constants.PI * radius**4)
    assert np.isclose(
        energy_model.calculate_segment_flow_energy(length, radius, flow, viscosity),
        expected_energy
    )

def test_calc_flow_energy_zero_radius(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    assert energy_model.calculate_segment_flow_energy(1.0, 0.0, 0.01, viscosity) == np.inf
    assert energy_model.calculate_segment_flow_energy(1.0, constants.EPSILON / 2, 0.01, viscosity) == np.inf

def test_calc_flow_energy_zero_flow(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    assert energy_model.calculate_segment_flow_energy(1.0, 0.1, 0.0, viscosity) == 0.0

def test_calc_flow_energy_zero_length(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    assert energy_model.calculate_segment_flow_energy(0.0, 0.1, 0.01, viscosity) == 0.0

# --- Tests for calculate_segment_metabolic_energy ---
def test_calc_metabolic_energy_known_values(mock_energy_config):
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]
    length, radius = 1.0, 0.1
    # Expected: c_met * pi * 0.1^2 * 1.0
    expected_energy = c_met * constants.PI * radius**2 * length
    assert np.isclose(
        energy_model.calculate_segment_metabolic_energy(length, radius, c_met),
        expected_energy
    )

def test_calc_metabolic_energy_zero_radius(mock_energy_config):
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]
    assert energy_model.calculate_segment_metabolic_energy(1.0, 0.0, c_met) == 0.0

def test_calc_metabolic_energy_zero_length(mock_energy_config):
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]
    assert energy_model.calculate_segment_metabolic_energy(0.0, 0.1, c_met) == 0.0

# --- Tests for calculate_bifurcation_loss ---
def test_calc_bifurcation_loss_basic(mock_energy_config):
    parent_pos = np.array([0,0,0])
    c1_pos, c1_r, c1_q = np.array([1,0,0]), 0.1, 0.01
    c2_pos, c2_r, c2_q = np.array([0,1,0]), 0.08, 0.008
    
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]

    l_c1 = utils.distance(parent_pos, c1_pos)
    e_flow_c1 = energy_model.calculate_segment_flow_energy(l_c1, c1_r, c1_q, viscosity)
    e_met_c1 = energy_model.calculate_segment_metabolic_energy(l_c1, c1_r, c_met)

    l_c2 = utils.distance(parent_pos, c2_pos)
    e_flow_c2 = energy_model.calculate_segment_flow_energy(l_c2, c2_r, c2_q, viscosity)
    e_met_c2 = energy_model.calculate_segment_metabolic_energy(l_c2, c2_r, c_met)
    
    expected_loss = e_flow_c1 + e_met_c1 + e_flow_c2 + e_met_c2
    
    calculated_loss = energy_model.calculate_bifurcation_loss(
        parent_pos, c1_pos, c1_r, c1_q, c2_pos, c2_r, c2_q, mock_energy_config
    )
    assert np.isclose(calculated_loss, expected_loss)

def test_calc_bifurcation_loss_one_child_zero_flow(mock_energy_config):
    parent_pos = np.array([0,0,0])
    c1_pos, c1_r, c1_q = np.array([1,0,0]), 0.1, 0.01
    c2_pos, c2_r, c2_q = np.array([0,1,0]), 0.08, 0.0 # Child 2 has zero flow
    
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]

    l_c1 = utils.distance(parent_pos, c1_pos)
    e_flow_c1 = energy_model.calculate_segment_flow_energy(l_c1, c1_r, c1_q, viscosity)
    e_met_c1 = energy_model.calculate_segment_metabolic_energy(l_c1, c1_r, c_met)

    # For child 2, flow energy should be 0, metabolic energy depends on radius/length
    l_c2 = utils.distance(parent_pos, c2_pos)
    e_flow_c2 = 0.0 
    e_met_c2 = energy_model.calculate_segment_metabolic_energy(l_c2, c2_r, c_met)
    
    expected_loss = e_flow_c1 + e_met_c1 + e_flow_c2 + e_met_c2
    
    calculated_loss = energy_model.calculate_bifurcation_loss(
        parent_pos, c1_pos, c1_r, c1_q, c2_pos, c2_r, c2_q, mock_energy_config
    )
    assert np.isclose(calculated_loss, expected_loss)

# --- Sanity Tests for find_optimal_bifurcation_for_new_region ---
@pytest.fixture
def mock_parent_terminal():
    # Using a simple class or dict for parent_terminal_gbo_data for the test
    class MockParent:
        id = "p_test"
        pos = np.array([0.,0.,0.])
        radius = 0.2 # Not directly used by find_optimal, but good for context
        flow = 0.01  # Flow to *existing* territory, also not directly used by find_optimal for *new* region
    return MockParent()

@pytest.fixture
def mock_tissue_data_for_bif_search():
    # Minimal tissue data for bifurcation search
    # Two distinct demand points in the "new growth region"
    voxel_vol = 0.001 # mm^3 (0.1mm sided voxels)
    
    # These are global flat indices for the new growth region
    new_growth_indices_flat = np.array([0, 1]) 
    
    # World coords for these two specific voxels
    # Place them such that KMeans or random selection should pick them or points near them
    new_growth_world_coords = np.array([
        [1.0, 0.5, 0.0], # Demand point 1
        [1.0, -0.5, 0.0] # Demand point 2
    ])
    
    # Corresponding 3D indices (dummy for this test, must match shape of demand map)
    new_growth_3d_indices = np.array([
        [0,0,0],
        [0,1,0]
    ])
    
    # Metabolic demand (q_met) for these points
    demand_q_met_values = np.array([0.02, 0.02]) # 1/s
    
    # Create a minimal 3D demand map that contains these
    demand_map_3d = np.zeros((1,2,1))
    demand_map_3d[new_growth_3d_indices[:,0], 
                  new_growth_3d_indices[:,1], 
                  new_growth_3d_indices[:,2]] = demand_q_met_values

    return {
        'world_coords_flat': new_growth_world_coords, # For this test, it *only* contains the new region voxels
        'voxel_indices_flat': new_growth_3d_indices, # Corresponding 3D indices for the new region voxels
        'metabolic_demand_map': demand_map_3d, # q_met map
        'voxel_volume': voxel_vol,
        'shape': demand_map_3d.shape # Shape of the minimal demand map
    }

def test_find_optimal_bif_empty_new_region(mock_parent_terminal, mock_tissue_data_for_bif_search, mock_energy_config):
    result = energy_model.find_optimal_bifurcation_for_new_region(
        mock_parent_terminal, 
        np.array([], dtype=int), # Empty new growth region
        mock_tissue_data_for_bif_search, 
        mock_energy_config,
        k_murray_factor=mock_energy_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=mock_energy_config["vascular_properties"]["murray_law_exponent"]
    )
    assert result is None

def test_find_optimal_bif_negligible_demand(mock_parent_terminal, mock_tissue_data_for_bif_search, mock_energy_config):
    # Modify tissue data to have zero demand
    tissue_data_zero_demand = mock_tissue_data_for_bif_search.copy()
    tissue_data_zero_demand['metabolic_demand_map'] = np.zeros_like(tissue_data_zero_demand['metabolic_demand_map'])
    
    new_growth_indices = np.array([0,1]) # Referring to indices in this zero-demand setup

    result = energy_model.find_optimal_bifurcation_for_new_region(
        mock_parent_terminal, 
        new_growth_indices,
        tissue_data_zero_demand, 
        mock_energy_config,
        k_murray_factor=mock_energy_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=mock_energy_config["vascular_properties"]["murray_law_exponent"]
    )
    assert result is None

def test_find_optimal_bif_basic_search(mock_parent_terminal, mock_tissue_data_for_bif_search, mock_energy_config, caplog):
    caplog.set_level(logging.DEBUG, logger="src.energy_model") # For verbose output from this test
    
    new_growth_indices = np.array([0,1]) # The two demand points defined in fixture

    result = energy_model.find_optimal_bifurcation_for_new_region(
        mock_parent_terminal, 
        new_growth_indices,
        mock_tissue_data_for_bif_search, 
        mock_energy_config,
        k_murray_factor=mock_energy_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=mock_energy_config["vascular_properties"]["murray_law_exponent"]
    )
    assert result is not None, "Expected a bifurcation result for simple two-point demand"
    c1_pos, c1_r, c1_q, c2_pos, c2_r, c2_q, loss = result

    # Check child positions are somewhat reasonable (e.g., near the demand points)
    # Demand points were [1,0.5,0] and [1,-0.5,0]. Parent at [0,0,0].
    # Children should be somewhere around X=1.
    assert c1_pos[0] > 0.5 and c2_pos[0] > 0.5 
    
    # Check flows sum to total demand of the new region
    total_demand_q_met = np.sum(mock_tissue_data_for_bif_search['metabolic_demand_map'])
    total_demand_flow = total_demand_q_met * mock_tissue_data_for_bif_search['voxel_volume']
    assert np.isclose(c1_q + c2_q, total_demand_flow), "Child flows should sum to new region's total demand"

    # Check radii are positive and follow Murray's scaling roughly
    min_r = mock_energy_config["vascular_properties"]["min_radius"]
    assert c1_r >= min_r and c2_r >= min_r
    if c1_q > constants.EPSILON:
        expected_r1 = mock_energy_config["vascular_properties"]["k_murray_scaling_factor"] * (c1_q ** (1.0/3.0))
        assert np.isclose(c1_r, max(min_r, expected_r1), rtol=0.05) # Allow some tolerance due to min_radius clamping
    if c2_q > constants.EPSILON:
        expected_r2 = mock_energy_config["vascular_properties"]["k_murray_scaling_factor"] * (c2_q ** (1.0/3.0))
        assert np.isclose(c2_r, max(min_r, expected_r2), rtol=0.05)

    assert loss < np.inf and loss > 0 # Loss should be a finite positive number# tests/test_energy_model.py
import pytest
import numpy as np
from typing import Optional

from src import energy_model, constants, config_manager, utils

# Define a common mock config for tests in this file
@pytest.fixture
def mock_energy_config():
    return {
        "vascular_properties": {
            "blood_viscosity": 0.004,  # Pa.s - Use a slightly different value for testing
            "min_radius": 0.001,       # mm
            "k_murray_scaling_factor": 0.6, # s^(1/3)
            "murray_law_exponent": 3.0,
            "min_segment_length": 0.01
        },
        "gbo_growth": {
            # Ensure this C_met is in units consistent with E_flow from calculate_segment_flow_energy
            # If E_flow inputs (L,R,Q) are mm, mm, mm^3/s and mu is Pa.s, E_flow is in (Pa.mm^3/s) or (10^-9 W)
            # Then C_met here should be in (Pa.mm^3/s) / mm^3 = Pa/s (if we treat energy as power)
            # Or if energy is just a "cost value", units need to be balanced by user.
            # Let's pick a value for testing.
            "energy_coefficient_C_met_vessel_wall": 100.0, # Arbitrary units, assuming consistency
            "bifurcation_candidate_points": 5 
        }
    }

# --- Tests for calculate_segment_flow_energy ---
def test_calc_flow_energy_known_values(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    # L=1mm, R=0.1mm, Q=0.01 mm^3/s
    length, radius, flow = 1.0, 0.1, 0.01
    # Expected: (8 * 0.004 * 1.0 * 0.01^2) / (pi * 0.1^4)
    # = (8 * 0.004 * 1e-4) / (pi * 1e-4) = (3.2e-6) / (pi * 1e-4) = 3.2e-2 / pi 
    expected_energy = (8 * viscosity * length * flow**2) / (constants.PI * radius**4)
    assert np.isclose(
        energy_model.calculate_segment_flow_energy(length, radius, flow, viscosity),
        expected_energy
    )

def test_calc_flow_energy_zero_radius(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    assert energy_model.calculate_segment_flow_energy(1.0, 0.0, 0.01, viscosity) == np.inf
    assert energy_model.calculate_segment_flow_energy(1.0, constants.EPSILON / 2, 0.01, viscosity) == np.inf

def test_calc_flow_energy_zero_flow(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    assert energy_model.calculate_segment_flow_energy(1.0, 0.1, 0.0, viscosity) == 0.0

def test_calc_flow_energy_zero_length(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    assert energy_model.calculate_segment_flow_energy(0.0, 0.1, 0.01, viscosity) == 0.0

# --- Tests for calculate_segment_metabolic_energy ---
def test_calc_metabolic_energy_known_values(mock_energy_config):
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]
    length, radius = 1.0, 0.1
    # Expected: c_met * pi * 0.1^2 * 1.0
    expected_energy = c_met * constants.PI * radius**2 * length
    assert np.isclose(
        energy_model.calculate_segment_metabolic_energy(length, radius, c_met),
        expected_energy
    )

def test_calc_metabolic_energy_zero_radius(mock_energy_config):
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]
    assert energy_model.calculate_segment_metabolic_energy(1.0, 0.0, c_met) == 0.0

def test_calc_metabolic_energy_zero_length(mock_energy_config):
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]
    assert energy_model.calculate_segment_metabolic_energy(0.0, 0.1, c_met) == 0.0

# --- Tests for calculate_bifurcation_loss ---
def test_calc_bifurcation_loss_basic(mock_energy_config):
    parent_pos = np.array([0,0,0])
    c1_pos, c1_r, c1_q = np.array([1,0,0]), 0.1, 0.01
    c2_pos, c2_r, c2_q = np.array([0,1,0]), 0.08, 0.008
    
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]

    l_c1 = utils.distance(parent_pos, c1_pos)
    e_flow_c1 = energy_model.calculate_segment_flow_energy(l_c1, c1_r, c1_q, viscosity)
    e_met_c1 = energy_model.calculate_segment_metabolic_energy(l_c1, c1_r, c_met)

    l_c2 = utils.distance(parent_pos, c2_pos)
    e_flow_c2 = energy_model.calculate_segment_flow_energy(l_c2, c2_r, c2_q, viscosity)
    e_met_c2 = energy_model.calculate_segment_metabolic_energy(l_c2, c2_r, c_met)
    
    expected_loss = e_flow_c1 + e_met_c1 + e_flow_c2 + e_met_c2
    
    calculated_loss = energy_model.calculate_bifurcation_loss(
        parent_pos, c1_pos, c1_r, c1_q, c2_pos, c2_r, c2_q, mock_energy_config
    )
    assert np.isclose(calculated_loss, expected_loss)

def test_calc_bifurcation_loss_one_child_zero_flow(mock_energy_config):
    parent_pos = np.array([0,0,0])
    c1_pos, c1_r, c1_q = np.array([1,0,0]), 0.1, 0.01
    c2_pos, c2_r, c2_q = np.array([0,1,0]), 0.08, 0.0 # Child 2 has zero flow
    
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]

    l_c1 = utils.distance(parent_pos, c1_pos)
    e_flow_c1 = energy_model.calculate_segment_flow_energy(l_c1, c1_r, c1_q, viscosity)
    e_met_c1 = energy_model.calculate_segment_metabolic_energy(l_c1, c1_r, c_met)

    # For child 2, flow energy should be 0, metabolic energy depends on radius/length
    l_c2 = utils.distance(parent_pos, c2_pos)
    e_flow_c2 = 0.0 
    e_met_c2 = energy_model.calculate_segment_metabolic_energy(l_c2, c2_r, c_met)
    
    expected_loss = e_flow_c1 + e_met_c1 + e_flow_c2 + e_met_c2
    
    calculated_loss = energy_model.calculate_bifurcation_loss(
        parent_pos, c1_pos, c1_r, c1_q, c2_pos, c2_r, c2_q, mock_energy_config
    )
    assert np.isclose(calculated_loss, expected_loss)

# --- Sanity Tests for find_optimal_bifurcation_for_new_region ---
@pytest.fixture
def mock_parent_terminal():
    # Using a simple class or dict for parent_terminal_gbo_data for the test
    class MockParent:
        id = "p_test"
        pos = np.array([0.,0.,0.])
        radius = 0.2 # Not directly used by find_optimal, but good for context
        flow = 0.01  # Flow to *existing* territory, also not directly used by find_optimal for *new* region
    return MockParent()

@pytest.fixture
def mock_tissue_data_for_bif_search():
    # Minimal tissue data for bifurcation search
    # Two distinct demand points in the "new growth region"
    voxel_vol = 0.001 # mm^3 (0.1mm sided voxels)
    
    # These are global flat indices for the new growth region
    new_growth_indices_flat = np.array([0, 1]) 
    
    # World coords for these two specific voxels
    # Place them such that KMeans or random selection should pick them or points near them
    new_growth_world_coords = np.array([
        [1.0, 0.5, 0.0], # Demand point 1
        [1.0, -0.5, 0.0] # Demand point 2
    ])
    
    # Corresponding 3D indices (dummy for this test, must match shape of demand map)
    new_growth_3d_indices = np.array([
        [0,0,0],
        [0,1,0]
    ])
    
    # Metabolic demand (q_met) for these points
    demand_q_met_values = np.array([0.02, 0.02]) # 1/s
    
    # Create a minimal 3D demand map that contains these
    demand_map_3d = np.zeros((1,2,1))
    demand_map_3d[new_growth_3d_indices[:,0], 
                  new_growth_3d_indices[:,1], 
                  new_growth_3d_indices[:,2]] = demand_q_met_values

    return {
        'world_coords_flat': new_growth_world_coords, # For this test, it *only* contains the new region voxels
        'voxel_indices_flat': new_growth_3d_indices, # Corresponding 3D indices for the new region voxels
        'metabolic_demand_map': demand_map_3d, # q_met map
        'voxel_volume': voxel_vol,
        'shape': demand_map_3d.shape # Shape of the minimal demand map
    }

def test_find_optimal_bif_empty_new_region(mock_parent_terminal, mock_tissue_data_for_bif_search, mock_energy_config):
    result = energy_model.find_optimal_bifurcation_for_new_region(
        mock_parent_terminal, 
        np.array([], dtype=int), # Empty new growth region
        mock_tissue_data_for_bif_search, 
        mock_energy_config,
        k_murray_factor=mock_energy_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=mock_energy_config["vascular_properties"]["murray_law_exponent"]
    )
    assert result is None

def test_find_optimal_bif_negligible_demand(mock_parent_terminal, mock_tissue_data_for_bif_search, mock_energy_config):
    # Modify tissue data to have zero demand
    tissue_data_zero_demand = mock_tissue_data_for_bif_search.copy()
    tissue_data_zero_demand['metabolic_demand_map'] = np.zeros_like(tissue_data_zero_demand['metabolic_demand_map'])
    
    new_growth_indices = np.array([0,1]) # Referring to indices in this zero-demand setup

    result = energy_model.find_optimal_bifurcation_for_new_region(
        mock_parent_terminal, 
        new_growth_indices,
        tissue_data_zero_demand, 
        mock_energy_config,
        k_murray_factor=mock_energy_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=mock_energy_config["vascular_properties"]["murray_law_exponent"]
    )
    assert result is None

def test_find_optimal_bif_basic_search(mock_parent_terminal, mock_tissue_data_for_bif_search, mock_energy_config, caplog):
    caplog.set_level(logging.DEBUG, logger="src.energy_model") # For verbose output from this test
    
    new_growth_indices = np.array([0,1]) # The two demand points defined in fixture

    result = energy_model.find_optimal_bifurcation_for_new_region(
        mock_parent_terminal, 
        new_growth_indices,
        mock_tissue_data_for_bif_search, 
        mock_energy_config,
        k_murray_factor=mock_energy_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=mock_energy_config["vascular_properties"]["murray_law_exponent"]
    )
    assert result is not None, "Expected a bifurcation result for simple two-point demand"
    c1_pos, c1_r, c1_q, c2_pos, c2_r, c2_q, loss = result

    # Check child positions are somewhat reasonable (e.g., near the demand points)
    # Demand points were [1,0.5,0] and [1,-0.5,0]. Parent at [0,0,0].
    # Children should be somewhere around X=1.
    assert c1_pos[0] > 0.5 and c2_pos[0] > 0.5 
    
    # Check flows sum to total demand of the new region
    total_demand_q_met = np.sum(mock_tissue_data_for_bif_search['metabolic_demand_map'])
    total_demand_flow = total_demand_q_met * mock_tissue_data_for_bif_search['voxel_volume']
    assert np.isclose(c1_q + c2_q, total_demand_flow), "Child flows should sum to new region's total demand"

    # Check radii are positive and follow Murray's scaling roughly
    min_r = mock_energy_config["vascular_properties"]["min_radius"]
    assert c1_r >= min_r and c2_r >= min_r
    if c1_q > constants.EPSILON:
        expected_r1 = mock_energy_config["vascular_properties"]["k_murray_scaling_factor"] * (c1_q ** (1.0/3.0))
        assert np.isclose(c1_r, max(min_r, expected_r1), rtol=0.05) # Allow some tolerance due to min_radius clamping
    if c2_q > constants.EPSILON:
        expected_r2 = mock_energy_config["vascular_properties"]["k_murray_scaling_factor"] * (c2_q ** (1.0/3.0))
        assert np.isclose(c2_r, max(min_r, expected_r2), rtol=0.05)

    assert loss < np.inf and loss > 0 # Loss should be a finite positive number# tests/test_energy_model.py
import pytest
import numpy as np
from typing import Optional

from src import energy_model, constants, config_manager, utils

# Define a common mock config for tests in this file
@pytest.fixture
def mock_energy_config():
    return {
        "vascular_properties": {
            "blood_viscosity": 0.004,  # Pa.s - Use a slightly different value for testing
            "min_radius": 0.001,       # mm
            "k_murray_scaling_factor": 0.6, # s^(1/3)
            "murray_law_exponent": 3.0,
            "min_segment_length": 0.01
        },
        "gbo_growth": {
            # Ensure this C_met is in units consistent with E_flow from calculate_segment_flow_energy
            # If E_flow inputs (L,R,Q) are mm, mm, mm^3/s and mu is Pa.s, E_flow is in (Pa.mm^3/s) or (10^-9 W)
            # Then C_met here should be in (Pa.mm^3/s) / mm^3 = Pa/s (if we treat energy as power)
            # Or if energy is just a "cost value", units need to be balanced by user.
            # Let's pick a value for testing.
            "energy_coefficient_C_met_vessel_wall": 100.0, # Arbitrary units, assuming consistency
            "bifurcation_candidate_points": 5 
        }
    }

# --- Tests for calculate_segment_flow_energy ---
def test_calc_flow_energy_known_values(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    # L=1mm, R=0.1mm, Q=0.01 mm^3/s
    length, radius, flow = 1.0, 0.1, 0.01
    # Expected: (8 * 0.004 * 1.0 * 0.01^2) / (pi * 0.1^4)
    # = (8 * 0.004 * 1e-4) / (pi * 1e-4) = (3.2e-6) / (pi * 1e-4) = 3.2e-2 / pi 
    expected_energy = (8 * viscosity * length * flow**2) / (constants.PI * radius**4)
    assert np.isclose(
        energy_model.calculate_segment_flow_energy(length, radius, flow, viscosity),
        expected_energy
    )

def test_calc_flow_energy_zero_radius(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    assert energy_model.calculate_segment_flow_energy(1.0, 0.0, 0.01, viscosity) == np.inf
    assert energy_model.calculate_segment_flow_energy(1.0, constants.EPSILON / 2, 0.01, viscosity) == np.inf

def test_calc_flow_energy_zero_flow(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    assert energy_model.calculate_segment_flow_energy(1.0, 0.1, 0.0, viscosity) == 0.0

def test_calc_flow_energy_zero_length(mock_energy_config):
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    assert energy_model.calculate_segment_flow_energy(0.0, 0.1, 0.01, viscosity) == 0.0

# --- Tests for calculate_segment_metabolic_energy ---
def test_calc_metabolic_energy_known_values(mock_energy_config):
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]
    length, radius = 1.0, 0.1
    # Expected: c_met * pi * 0.1^2 * 1.0
    expected_energy = c_met * constants.PI * radius**2 * length
    assert np.isclose(
        energy_model.calculate_segment_metabolic_energy(length, radius, c_met),
        expected_energy
    )

def test_calc_metabolic_energy_zero_radius(mock_energy_config):
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]
    assert energy_model.calculate_segment_metabolic_energy(1.0, 0.0, c_met) == 0.0

def test_calc_metabolic_energy_zero_length(mock_energy_config):
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]
    assert energy_model.calculate_segment_metabolic_energy(0.0, 0.1, c_met) == 0.0

# --- Tests for calculate_bifurcation_loss ---
def test_calc_bifurcation_loss_basic(mock_energy_config):
    parent_pos = np.array([0,0,0])
    c1_pos, c1_r, c1_q = np.array([1,0,0]), 0.1, 0.01
    c2_pos, c2_r, c2_q = np.array([0,1,0]), 0.08, 0.008
    
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]

    l_c1 = utils.distance(parent_pos, c1_pos)
    e_flow_c1 = energy_model.calculate_segment_flow_energy(l_c1, c1_r, c1_q, viscosity)
    e_met_c1 = energy_model.calculate_segment_metabolic_energy(l_c1, c1_r, c_met)

    l_c2 = utils.distance(parent_pos, c2_pos)
    e_flow_c2 = energy_model.calculate_segment_flow_energy(l_c2, c2_r, c2_q, viscosity)
    e_met_c2 = energy_model.calculate_segment_metabolic_energy(l_c2, c2_r, c_met)
    
    expected_loss = e_flow_c1 + e_met_c1 + e_flow_c2 + e_met_c2
    
    calculated_loss = energy_model.calculate_bifurcation_loss(
        parent_pos, c1_pos, c1_r, c1_q, c2_pos, c2_r, c2_q, mock_energy_config
    )
    assert np.isclose(calculated_loss, expected_loss)

def test_calc_bifurcation_loss_one_child_zero_flow(mock_energy_config):
    parent_pos = np.array([0,0,0])
    c1_pos, c1_r, c1_q = np.array([1,0,0]), 0.1, 0.01
    c2_pos, c2_r, c2_q = np.array([0,1,0]), 0.08, 0.0 # Child 2 has zero flow
    
    viscosity = mock_energy_config["vascular_properties"]["blood_viscosity"]
    c_met = mock_energy_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"]

    l_c1 = utils.distance(parent_pos, c1_pos)
    e_flow_c1 = energy_model.calculate_segment_flow_energy(l_c1, c1_r, c1_q, viscosity)
    e_met_c1 = energy_model.calculate_segment_metabolic_energy(l_c1, c1_r, c_met)

    # For child 2, flow energy should be 0, metabolic energy depends on radius/length
    l_c2 = utils.distance(parent_pos, c2_pos)
    e_flow_c2 = 0.0 
    e_met_c2 = energy_model.calculate_segment_metabolic_energy(l_c2, c2_r, c_met)
    
    expected_loss = e_flow_c1 + e_met_c1 + e_flow_c2 + e_met_c2
    
    calculated_loss = energy_model.calculate_bifurcation_loss(
        parent_pos, c1_pos, c1_r, c1_q, c2_pos, c2_r, c2_q, mock_energy_config
    )
    assert np.isclose(calculated_loss, expected_loss)

# --- Sanity Tests for find_optimal_bifurcation_for_new_region ---
@pytest.fixture
def mock_parent_terminal():
    # Using a simple class or dict for parent_terminal_gbo_data for the test
    class MockParent:
        id = "p_test"
        pos = np.array([0.,0.,0.])
        radius = 0.2 # Not directly used by find_optimal, but good for context
        flow = 0.01  # Flow to *existing* territory, also not directly used by find_optimal for *new* region
    return MockParent()

@pytest.fixture
def mock_tissue_data_for_bif_search():
    # Minimal tissue data for bifurcation search
    # Two distinct demand points in the "new growth region"
    voxel_vol = 0.001 # mm^3 (0.1mm sided voxels)
    
    # These are global flat indices for the new growth region
    new_growth_indices_flat = np.array([0, 1]) 
    
    # World coords for these two specific voxels
    # Place them such that KMeans or random selection should pick them or points near them
    new_growth_world_coords = np.array([
        [1.0, 0.5, 0.0], # Demand point 1
        [1.0, -0.5, 0.0] # Demand point 2
    ])
    
    # Corresponding 3D indices (dummy for this test, must match shape of demand map)
    new_growth_3d_indices = np.array([
        [0,0,0],
        [0,1,0]
    ])
    
    # Metabolic demand (q_met) for these points
    demand_q_met_values = np.array([0.02, 0.02]) # 1/s
    
    # Create a minimal 3D demand map that contains these
    demand_map_3d = np.zeros((1,2,1))
    demand_map_3d[new_growth_3d_indices[:,0], 
                  new_growth_3d_indices[:,1], 
                  new_growth_3d_indices[:,2]] = demand_q_met_values

    return {
        'world_coords_flat': new_growth_world_coords, # For this test, it *only* contains the new region voxels
        'voxel_indices_flat': new_growth_3d_indices, # Corresponding 3D indices for the new region voxels
        'metabolic_demand_map': demand_map_3d, # q_met map
        'voxel_volume': voxel_vol,
        'shape': demand_map_3d.shape # Shape of the minimal demand map
    }

def test_find_optimal_bif_empty_new_region(mock_parent_terminal, mock_tissue_data_for_bif_search, mock_energy_config):
    result = energy_model.find_optimal_bifurcation_for_new_region(
        mock_parent_terminal, 
        np.array([], dtype=int), # Empty new growth region
        mock_tissue_data_for_bif_search, 
        mock_energy_config,
        k_murray_factor=mock_energy_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=mock_energy_config["vascular_properties"]["murray_law_exponent"]
    )
    assert result is None

def test_find_optimal_bif_negligible_demand(mock_parent_terminal, mock_tissue_data_for_bif_search, mock_energy_config):
    # Modify tissue data to have zero demand
    tissue_data_zero_demand = mock_tissue_data_for_bif_search.copy()
    tissue_data_zero_demand['metabolic_demand_map'] = np.zeros_like(tissue_data_zero_demand['metabolic_demand_map'])
    
    new_growth_indices = np.array([0,1]) # Referring to indices in this zero-demand setup

    result = energy_model.find_optimal_bifurcation_for_new_region(
        mock_parent_terminal, 
        new_growth_indices,
        tissue_data_zero_demand, 
        mock_energy_config,
        k_murray_factor=mock_energy_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=mock_energy_config["vascular_properties"]["murray_law_exponent"]
    )
    assert result is None

def test_find_optimal_bif_basic_search(mock_parent_terminal, mock_tissue_data_for_bif_search, mock_energy_config, caplog):
    caplog.set_level(logging.DEBUG, logger="src.energy_model") # For verbose output from this test
    
    new_growth_indices = np.array([0,1]) # The two demand points defined in fixture

    result = energy_model.find_optimal_bifurcation_for_new_region(
        mock_parent_terminal, 
        new_growth_indices,
        mock_tissue_data_for_bif_search, 
        mock_energy_config,
        k_murray_factor=mock_energy_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=mock_energy_config["vascular_properties"]["murray_law_exponent"]
    )
    assert result is not None, "Expected a bifurcation result for simple two-point demand"
    c1_pos, c1_r, c1_q, c2_pos, c2_r, c2_q, loss = result

    # Check child positions are somewhat reasonable (e.g., near the demand points)
    # Demand points were [1,0.5,0] and [1,-0.5,0]. Parent at [0,0,0].
    # Children should be somewhere around X=1.
    assert c1_pos[0] > 0.5 and c2_pos[0] > 0.5 
    
    # Check flows sum to total demand of the new region
    total_demand_q_met = np.sum(mock_tissue_data_for_bif_search['metabolic_demand_map'])
    total_demand_flow = total_demand_q_met * mock_tissue_data_for_bif_search['voxel_volume']
    assert np.isclose(c1_q + c2_q, total_demand_flow), "Child flows should sum to new region's total demand"

    # Check radii are positive and follow Murray's scaling roughly
    min_r = mock_energy_config["vascular_properties"]["min_radius"]
    assert c1_r >= min_r and c2_r >= min_r
    if c1_q > constants.EPSILON:
        expected_r1 = mock_energy_config["vascular_properties"]["k_murray_scaling_factor"] * (c1_q ** (1.0/3.0))
        assert np.isclose(c1_r, max(min_r, expected_r1), rtol=0.05) # Allow some tolerance due to min_radius clamping
    if c2_q > constants.EPSILON:
        expected_r2 = mock_energy_config["vascular_properties"]["k_murray_scaling_factor"] * (c2_q ** (1.0/3.0))
        assert np.isclose(c2_r, max(min_r, expected_r2), rtol=0.05)

    assert loss < np.inf and loss > 0 # Loss should be a finite positive number# tests/test_vascular_growth.py
import pytest

import logging
import re
import numpy as np
import networkx as nx
import os
import shutil # For cleaning up output directory
from typing import Tuple, List, Optional # For type hinting
from src import vascular_growth, data_structures, io_utils, utils, constants, config_manager

try:
    import pyvista as pv
    PYVISTA_AVAILABLE = True
except ImportError:
    PYVISTA_AVAILABLE = False
    pv = None

logger = logging.getLogger(__name__) # <<<<<<<<<<<<<< ADD THIS DEFINITION

@pytest.fixture(scope="function")
def simple_gbo_config_and_output():
    # Create a temporary directory for this test module's outputs
    test_module_output_dir = "temp_test_vascular_growth_output"
    if os.path.exists(test_module_output_dir): # Clean up from previous runs
        shutil.rmtree(test_module_output_dir)
    os.makedirs(test_module_output_dir, exist_ok=True)
    
    # Simplified config for a small test run
    config = {
        "paths": {
            "output_dir": test_module_output_dir 
        },
        "simulation": {
            "simulation_name": "gbo_simple_test",
            "random_seed": 42,
            "log_level": "DEBUG", # Use DEBUG to see detailed logs from growth
             "units": {"length": "mm", "pressure": "Pa", "flow_rate": "mm^3/s"}
        },
        "tissue_properties": {
            "metabolic_rates": {"gm": 0.01, "wm": 0.003}, # Simplified rates
            "permeability": {"gm": 1.0e-7, "wm": 5.0e-8}
        },
        "vascular_properties": {
            "blood_viscosity": constants.DEFAULT_BLOOD_VISCOSITY,
            "murray_law_exponent": 3.0,
            "initial_terminal_flow": 1e-5, # Slightly larger initial flow for small domain
            "min_radius": 0.005, # mm
            "k_murray_scaling_factor": 0.5, # s^(1/3)
            "min_segment_length": 0.05, # mm
        },
        "gbo_growth": {
            "max_iterations": 5, # Run for very few iterations
            "energy_coefficient_C_met_vessel_wall": 1.0e-5, # (Units: 10^-9 W / mm^3 or consistent)
            "initial_territory_radius": 0.2, # mm, for Ω_init
            "frontier_search_radius_factor": 3.0, # Smaller factor for small domain
            "frontier_search_radius_fixed": 0.5,  # mm
            "max_voxels_for_Rip": 50, # Small Rip
            "branch_radius_increase_threshold": 1.1, # More sensitive branching
            "max_flow_single_terminal": 0.0001, # Lower threshold for branching
            "target_domain_perfusion_fraction": 0.90, # Lower target for short test
            "bifurcation_candidate_points": 5, # Fewer candidates for speed
            "min_iterations_before_no_growth_stop": 2,
            "stop_criteria": {"max_radius_factor_measured": 1.0} # Not relevant for seed test
        },
        "tumor_angiogenesis": {"enabled": False},
        "perfusion_solver": {"enabled": False},
        "visualization": {
            "save_intermediate_steps": True, # Save steps for inspection
            "intermediate_step_interval": 1
        }
    }
    # Create a unique output dir for this specific test function run
    run_output_dir = utils.create_output_directory(test_module_output_dir, "run_simple_gbo", timestamp=False)

    yield config, run_output_dir # Provide config and specific run output_dir

    # Teardown: Optionally remove the test_module_output_dir after all tests in module are done
    # shutil.rmtree(test_module_output_dir) # Comment out if you want to inspect outputs

@pytest.fixture
def minimal_tissue_data():
    shape = (10, 10, 10) # Small 3D volume
    affine = np.eye(4) 
    affine[0,0] = affine[1,1] = affine[2,2] = 0.1 # Voxel size 0.1 mm (total volume 1mm^3)
    
    domain_mask = np.zeros(shape, dtype=bool)
    domain_mask[2:8, 2:8, 2:8] = True # A central block of tissue
    
    # Define GM and WM regions within the domain_mask
    gm_mask = np.zeros(shape, dtype=bool)
    gm_mask[2:5, 2:8, 2:8] = True # Lower part is GM
    gm_mask = gm_mask & domain_mask # Ensure it's within domain

    wm_mask = np.zeros(shape, dtype=bool)
    wm_mask[5:8, 2:8, 2:8] = True # Upper part is WM
    wm_mask = wm_mask & domain_mask

    metabolic_demand_map_qmet = np.zeros(shape, dtype=np.float32)
    # Use simplified config rates for this test
    metabolic_demand_map_qmet[gm_mask] = 0.01 # q_met for GM (e.g., 1/s)
    metabolic_demand_map_qmet[wm_mask] = 0.003 # q_met for WM
    
    voxel_volume = utils.get_voxel_volume_from_affine(affine)

    # Get flat coordinates and indices for voxels *within domain_mask*
    voxel_indices_3d_domain = np.array(np.where(domain_mask)).T
    world_coords_domain_flat = utils.voxel_to_world(voxel_indices_3d_domain, affine)

    tissue = {
        'shape': shape,
        'affine': affine,
        'voxel_volume': voxel_volume,
        'domain_mask': domain_mask,
        'GM': gm_mask, # Provide GM/WM for seeding logic if used
        'WM': wm_mask,
        # metabolic_demand_map should store q_met (rate per unit volume), 
        # actual demand (q_met * dV) is calculated where needed
        'metabolic_demand_map': metabolic_demand_map_qmet, 
        'world_coords_flat': world_coords_domain_flat, # Coords of voxels IN domain_mask
        'voxel_indices_flat': voxel_indices_3d_domain  # 3D indices of voxels IN domain_mask
    }
    return tissue

def test_vascular_growth_single_seed(simple_gbo_config_and_output, minimal_tissue_data, caplog):
    """
    Test GBO growth starting from a single seed point in a minimal tissue.
    """
    config, output_dir = simple_gbo_config_and_output
    tissue_data = minimal_tissue_data

    # Set log level for more detail during this test if not globally set
    caplog.set_level(logging.DEBUG, logger="src.vascular_growth")
    caplog.set_level(logging.DEBUG, logger="src.energy_model")

    # Ensure RNG seed for deterministic test
    utils.set_rng_seed(config_manager.get_param(config, "simulation.random_seed", 42))

    initial_graph = None # Start with no measured arteries for this test

    # --- Run the growth simulation ---
    final_graph = vascular_growth.grow_healthy_vasculature(
        config=config,
        tissue_data=tissue_data,
        initial_graph=initial_graph,
        output_dir=output_dir
    )
    
     # Determine the actual last iteration for which files would be saved
    # In tests/test_vascular_growth.py

        # ... (after final_graph = vascular_growth.grow_healthy_vasculature(...)) ...

    max_iterations_config = config['gbo_growth']['max_iterations']
        # Default to assuming it ran all iterations
    expected_save_file_iter_num = max_iterations_config 
        
    found_stop_message = False
    for record in caplog.records:
        # logger.debug(f"Test log scan: {record.levelname} - {record.message}") # Uncomment for extreme debug
        if record.levelname == "INFO":
            if "GBO Stopping after iteration" in record.message:
                match = re.search(r"GBO Stopping after iteration (\d+):", record.message)
                if match:
                    expected_save_file_iter_num = int(match.group(1))
                    logger.info(f"Found stop message: '{record.message}'. Expecting files for iter_{expected_save_file_iter_num}.")
                    found_stop_message = True
                    break 
            elif "GBO Stopping:" in record.message and not found_stop_message: # Fallback for other stop messages
                # This fallback is less precise; it will take the iter # of the GBO Iteration log line
                # that appeared *before* this generic stop message.
                # This might be off by one if the generic stop happens after all processing for an iter.
                logger.warning(f"Found generic stop message: '{record.message}'. Attempting to find last processed iteration.")
                # Search backwards for the last "--- GBO Iteration X / Y ---"
                temp_iter_num = 1 
                current_record_index = caplog.records.index(record)
                for i_rec in range(current_record_index - 1, -1, -1):
                    if "--- GBO Iteration" in caplog.records[i_rec].message:
                        match_iter = re.search(r"--- GBO Iteration (\d+)", caplog.records[i_rec].message)
                        if match_iter:
                            temp_iter_num = int(match_iter.group(1))
                            break
                expected_save_file_iter_num = temp_iter_num
                logger.info(f"Fallback: Expecting files for iter_{expected_save_file_iter_num} based on generic stop.")
                found_stop_message = True # Consider it found for overriding the default
                break

    if not found_stop_message:
        logger.info(f"No 'GBO Stopping' message found or parsed. Defaulting to check for iter_{max_iterations_config} files.")
        # expected_save_file_iter_num remains max_iterations_config

    last_iter_perfused_mask_path = os.path.join(output_dir, f"perfused_mask_iter_{expected_save_file_iter_num}.nii.gz")
    logger.info(f"FINAL CHECK for saved mask: {last_iter_perfused_mask_path}")
    assert os.path.exists(last_iter_perfused_mask_path), \
        f"Expected {os.path.basename(last_iter_perfused_mask_path)} was not saved. Check logs for save errors or actual stop iteration."

    last_iter_graph_path = os.path.join(output_dir, f"gbo_graph_iter_{expected_save_file_iter_num}.vtp")
    assert os.path.exists(last_iter_graph_path), \
        f"Expected {os.path.basename(last_iter_graph_path)} was not saved."

    # --- Assertions if files exist ---
    assert final_graph is not None, "Growth function returned None"
    num_initial_nodes = 0 
    assert final_graph.number_of_nodes() > num_initial_nodes, \
        f"Graph did not grow. Started with {num_initial_nodes} nodes, ended with {final_graph.number_of_nodes()}."

    if os.path.exists(last_iter_perfused_mask_path):
        perf_mask_data, _, _ = io_utils.load_nifti_image(last_iter_perfused_mask_path)
        assert perf_mask_data is not None
        min_expected_perf_count = 10 
        if final_graph.number_of_nodes() > 1: 
            assert np.sum(perf_mask_data) > min_expected_perf_count, \
                f"Very few voxels perfused. Perfused count: {np.sum(perf_mask_data)}"
    else: # If the mask file wasn't found, this part of the test can't run.
            # The assertion above would have already failed.
        pass

    logger.info(f"Test `test_vascular_growth_single_seed` completed. Final graph: {final_graph.number_of_nodes()} nodes, {final_graph.number_of_edges()} edges.")
    logger.info(f"Outputs saved in: {output_dir}")

@pytest.fixture(scope="module") # Use module scope if fixture is complex and shared
def larger_synthetic_tissue_and_config():
    test_module_output_dir = "temp_test_larger_growth_output"
    if os.path.exists(test_module_output_dir):
        shutil.rmtree(test_module_output_dir)
    os.makedirs(test_module_output_dir, exist_ok=True)

    # Config for a slightly larger run
    config = {
        "paths": {"output_dir": test_module_output_dir},
        "simulation": {
            "simulation_name": "gbo_larger_test", "random_seed": 42, "log_level": "INFO", # INFO to reduce log spam
            "units": {"length": "mm", "pressure": "Pa", "flow_rate": "mm^3/s"}
        },
        "tissue_properties": {"metabolic_rates": {"gm": 0.016, "wm": 0.005}, "permeability": {}},
        "vascular_properties": {
            "blood_viscosity": constants.DEFAULT_BLOOD_VISCOSITY, "murray_law_exponent": 2.7,
            "initial_terminal_flow": 1e-4, "min_radius": 0.005, "k_murray_scaling_factor": 0.5,
            "min_segment_length": 0.1, "max_segment_length": 1.0, # Smaller max seg length
        },
        "gbo_growth": {
            "max_iterations": 7, # More iterations
            "energy_coefficient_C_met_vessel_wall": 1.0e-5,
            "initial_territory_radius": 0.3, # Larger initial claim for larger domain
            "frontier_search_radius_factor": 3.0,
            "frontier_search_radius_fixed": 0.4, 
            "max_voxels_for_Rip": 50,
            "branch_radius_increase_threshold": 1.15,
            "max_flow_single_terminal": 0.001, 
            "target_domain_perfusion_fraction": 0.80, # Aim for 80%
            "bifurcation_candidate_points": 10,
            "min_iterations_before_no_growth_stop": 5,
            "stop_criteria": {"max_radius_factor_measured": 1.0}
        },
        "visualization": {"save_intermediate_steps": True, "intermediate_step_interval": 3} # Save every 3 iters
    }
    run_output_dir = utils.create_output_directory(test_module_output_dir, "run_larger_gbo", timestamp=False)

    # Tissue data
    shape = (40, 40, 40) # Larger grid
    affine = np.eye(4)
    affine[0,0] = affine[1,1] = affine[2,2] = 0.1 # 0.1mm voxels (4mm x 4mm x 4mm cube)
    
    domain_mask = np.zeros(shape, dtype=bool)
    domain_mask[5:35, 5:35, 5:35] = True # Central active tissue block
    
    gm_mask = np.zeros(shape, dtype=bool) # e.g., a shell or specific region
    gm_mask[5:15, 5:35, 5:35] = True 
    gm_mask = gm_mask & domain_mask

    metabolic_demand_map_qmet = np.zeros(shape, dtype=np.float32)
    metabolic_demand_map_qmet[gm_mask] = config["tissue_properties"]["metabolic_rates"]["gm"]
    # Fill rest of domain with WM rate
    wm_fill_mask = domain_mask & (~gm_mask)
    metabolic_demand_map_qmet[wm_fill_mask] = config["tissue_properties"]["metabolic_rates"]["wm"]
    
    voxel_volume = utils.get_voxel_volume_from_affine(affine)
    voxel_indices_3d_domain = np.array(np.where(domain_mask)).T
    world_coords_domain_flat = utils.voxel_to_world(voxel_indices_3d_domain, affine)

    tissue = {
        'shape': shape, 'affine': affine, 'voxel_volume': voxel_volume,
        'domain_mask': domain_mask, 'GM': gm_mask, 'WM': wm_fill_mask,
        'metabolic_demand_map': metabolic_demand_map_qmet, 
        'world_coords_flat': world_coords_domain_flat,
        'voxel_indices_flat': voxel_indices_3d_domain
    }

    # Initial "measured" artery - a simple line segment
    initial_artery_graph = data_structures.create_empty_vascular_graph()
    # Root point (e.g., edge of domain)
    # Voxel (0, 20, 20) -> world coords
    p_root_vox = np.array([0, shape[1]//2, shape[2]//2]) 
    p_root_world = utils.voxel_to_world(p_root_vox.reshape(1,-1), affine)[0]
    # Terminal point (e.g., just inside the domain_mask)
    p_term_vox = np.array([6, shape[1]//2, shape[2]//2]) # Inside domain_mask[5:35,...]
    p_term_world = utils.voxel_to_world(p_term_vox.reshape(1,-1), affine)[0]

    data_structures.add_node_to_graph(initial_artery_graph, "m_root_0", pos=p_root_world, radius=0.3, type='measured_root')
    data_structures.add_node_to_graph(initial_artery_graph, "m_term_0", pos=p_term_world, radius=0.25, type='measured_terminal')
    data_structures.add_edge_to_graph(initial_artery_graph, "m_root_0", "m_term_0", type='measured_segment')
    
    return config, run_output_dir, tissue, initial_artery_graph

def test_vascular_growth_larger_phantom_with_artery(larger_synthetic_tissue_and_config, caplog):
    config, output_dir, tissue_data, initial_graph = larger_synthetic_tissue_and_config
    
    caplog.set_level(logging.INFO) # Keep logs less verbose for this potentially longer test
    utils.set_rng_seed(config_manager.get_param(config, "simulation.random_seed", 42))

    logger.info(f"Starting larger phantom test. Output will be in: {output_dir}")
    logger.info(f"Initial graph has {initial_graph.number_of_nodes()} nodes, {initial_graph.number_of_edges()} edges.")
    logger.info(f"Tissue domain size: {tissue_data['shape']}, Voxel size: {np.diag(tissue_data['affine'])[:3]}mm")
    logger.info(f"Number of voxels in domain_mask: {np.sum(tissue_data['domain_mask'])}")


    final_graph = vascular_growth.grow_healthy_vasculature(
        config=config,
        tissue_data=tissue_data,
        initial_graph=initial_graph,
        output_dir=output_dir
    )

    assert final_graph is not None
    assert final_graph.number_of_nodes() > initial_graph.number_of_nodes(), "Graph did not grow from initial artery."

    # Check if some files were saved (at least the first intermediate or final)
    # This relies on the save logic in grow_healthy_vasculature correctly identifying when to save.
    # For this test, we are mostly interested in visual inspection of the output.
    # Let's just check if the output directory contains VTP files.
    vtp_files_found = [f for f in os.listdir(output_dir) if f.endswith(".vtp")]
    nii_files_found = [f for f in os.listdir(output_dir) if f.endswith(".nii.gz")]

    assert len(vtp_files_found) > 0, "No VTP graph files were saved."
    assert len(nii_files_found) > 0, "No NIfTI mask files were saved."
    
    logger.info(f"Larger phantom test completed. Final graph has {final_graph.number_of_nodes()} nodes, {final_graph.number_of_edges()} edges.")
    logger.info(f"Inspect outputs in: {output_dir}")
    
    # Call the main visualization function (which might produce a PyVista plot/screenshot)
    # Ensure visualization settings in config are appropriate (e.g., background color)
    if config_manager.get_param(config,"visualization.pyvista_enabled_for_test", True) and PYVISTA_AVAILABLE : # Add a flag to enable/disable for CI
        from src import visualization # Import here to use the updated one
        visualization.plot_vascular_tree_pyvista(
            final_graph, 
            title="Larger Phantom GBO Result",
            output_screenshot_path=os.path.join(output_dir, "larger_phantom_final_tree.png")
        )
    else:
        logger.info("Skipping PyVista plot in test_vascular_growth_larger_phantom_with_artery.")# tests/test_perfusion_solver.py
import pytest
import numpy as np
import networkx as nx
import logging

from src import perfusion_solver, data_structures, constants, config_manager # Added config_manager

@pytest.fixture
def basic_config_flow_test(): # Renamed for clarity
    """Provides a basic config for perfusion solver tests."""
    # Ensure all necessary keys that get_param might look for are present, even if using defaults
    return {
        "vascular_properties": {
            "blood_viscosity": 0.0035, 
            "min_radius": 0.001,
            # Add other vascular_properties if get_param in solver might access them with defaults
            "k_murray_scaling_factor": 0.5, # Example, not directly used by solver but good practice
            "murray_law_exponent": 3.0,
             "min_segment_length": 0.01,
             "max_segment_length": 2.0,
             "initial_terminal_flow": 1e-6,
        },
        "perfusion_solver": {
            "inlet_pressure": 10000.0 
        },
        "gbo_growth": { # Add this section even if empty, as get_param might traverse
             "energy_coefficient_C_met_vessel_wall": 1.0e-5,
        }
    }

def test_calculate_segment_resistance():
    viscosity = 0.0035
    expected_R = (8 * viscosity * 1.0) / (constants.PI * (0.1**4))
    assert np.isclose(perfusion_solver.calculate_segment_resistance(1.0, 0.1, viscosity), expected_R)
    assert perfusion_solver.calculate_segment_resistance(1.0, 0.0, viscosity) == np.inf
    assert perfusion_solver.calculate_segment_resistance(1.0, constants.EPSILON/10, viscosity) == np.inf
    assert perfusion_solver.calculate_segment_resistance(0.0, 0.1, viscosity) == 0.0

def test_solve_single_segment_pressure_bc(basic_config_flow_test):
    graph = data_structures.create_empty_vascular_graph()
    config = basic_config_flow_test
    viscosity = config_manager.get_param(config, "vascular_properties.blood_viscosity")
    
    P_in_val = 100.0
    P_out_val = 50.0 # Target outlet pressure
    L_val, R_val = 10.0, 0.5 

    data_structures.add_node_to_graph(graph, "n0", pos=np.array([0,0,0]), radius=R_val, type='measured_root')
    # For this test to work with current solver (flow sinks), n1 needs to be a flow sink.
    # We calculate the expected flow if P_out was 50, and set that as the sink.
    data_structures.add_node_to_graph(graph, "n1", pos=np.array([L_val,0,0]), radius=R_val, type='synthetic_terminal')
    
    # Edge radius should be that of the upstream node for consistency with how GBO might set it
    data_structures.add_edge_to_graph(graph, "n0", "n1", length=L_val, radius=R_val) 

    resistance = perfusion_solver.calculate_segment_resistance(L_val, R_val, viscosity)
    expected_flow = (P_in_val - P_out_val) / resistance
    
    graph.nodes['n1']['Q_flow'] = expected_flow # Set n1 as a flow sink with this value
    
    solved_graph = perfusion_solver.solve_1d_poiseuille_flow(graph, config, root_pressure_val=P_in_val)

    assert np.isclose(solved_graph.nodes['n0']['pressure'], P_in_val)
    # The pressure at n1 will be P_in - Q*R. If Q is set correctly, P_n1 should be P_out_val.
    assert np.isclose(solved_graph.nodes['n1']['pressure'], P_out_val, atol=1e-1) 
    assert np.isclose(solved_graph.edges[('n0','n1')]['flow_solver'], expected_flow, rtol=1e-3)


def test_solve_y_bifurcation_flow_sinks(basic_config_flow_test):
    graph = data_structures.create_empty_vascular_graph()
    config = basic_config_flow_test
    P_in = config_manager.get_param(config, "perfusion_solver.inlet_pressure")
    visc = config_manager.get_param(config, "vascular_properties.blood_viscosity")

    L01, R01 = 5.0, 0.3
    L1T1, R1T1 = 3.0, 0.2
    L1T2, R1T2 = 4.0, 0.25

    data_structures.add_node_to_graph(graph, "P0", pos=np.array([0,0,0]), radius=R01, type='measured_root') # CORRECTED
    data_structures.add_node_to_graph(graph, "P1", pos=np.array([L01,0,0]), radius=R01, type='synthetic_bifurcation') # CORRECTED
    data_structures.add_node_to_graph(graph, "T1", pos=np.array([L01+L1T1,1,0]), radius=R1T1, type='synthetic_terminal') # CORRECTED
    data_structures.add_node_to_graph(graph, "T2", pos=np.array([L01+L1T2,-1,0]), radius=R1T2, type='synthetic_terminal') # 

    # Edges store their own radius (typically parent's radius or average)
    data_structures.add_edge_to_graph(graph, "P0", "P1", length=L01, radius=R01)
    data_structures.add_edge_to_graph(graph, "P1", "T1", length=L1T1, radius=R1T1) # Child segment gets its own radius
    data_structures.add_edge_to_graph(graph, "P1", "T2", length=L1T2, radius=R1T2) # Child segment gets its own radius

    Q_out_T1 = 0.002 
    Q_out_T2 = 0.003 
    graph.nodes["T1"]['Q_flow'] = Q_out_T1
    graph.nodes["T2"]['Q_flow'] = Q_out_T2

    solved_graph = perfusion_solver.solve_1d_poiseuille_flow(graph, config)

    Res01 = perfusion_solver.calculate_segment_resistance(L01, R01, visc)
    Res1T1 = perfusion_solver.calculate_segment_resistance(L1T1, R1T1, visc)
    Res1T2 = perfusion_solver.calculate_segment_resistance(L1T2, R1T2, visc)

    exp_Q01 = Q_out_T1 + Q_out_T2
    exp_P_P1 = P_in - exp_Q01 * Res01
    exp_P_T1 = exp_P_P1 - Q_out_T1 * Res1T1
    exp_P_T2 = exp_P_P1 - Q_out_T2 * Res1T2

    assert np.isclose(solved_graph.nodes['P0']['pressure'], P_in, rtol=1e-3)
    assert np.isclose(solved_graph.nodes['P1']['pressure'], exp_P_P1, rtol=1e-3)
    assert np.isclose(solved_graph.nodes['T1']['pressure'], exp_P_T1, rtol=1e-3)
    assert np.isclose(solved_graph.nodes['T2']['pressure'], exp_P_T2, rtol=1e-3)

    assert np.isclose(solved_graph.edges[('P0','P1')]['flow_solver'], exp_Q01, rtol=1e-3)
    assert np.isclose(solved_graph.edges[('P1','T1')]['flow_solver'], Q_out_T1, rtol=1e-3)
    assert np.isclose(solved_graph.edges[('P1','T2')]['flow_solver'], Q_out_T2, rtol=1e-3)

def test_solve_disconnected_component_no_bc(basic_config_flow_test, caplog): # Add caplog
    """Test that solver handles a component not connected to a pressure BC."""
    graph = data_structures.create_empty_vascular_graph()
    config = basic_config_flow_test

    # Component 1 (with BC)
    data_structures.add_node_to_graph(graph, "R0", pos=np.array([0,0,0]), radius=0.1, type='measured_root')
    data_structures.add_node_to_graph(graph, "T0", pos=np.array([1,0,0]), radius=0.1, type='synthetic_terminal', Q_flow=0.001)
    data_structures.add_edge_to_graph(graph, "R0", "T0", length=1.0, radius=0.1)

    # Component 2 (floating, no BC)
    data_structures.add_node_to_graph(graph, "F1", pos=np.array([10,0,0]), radius=0.1, type='synthetic_bifurcation')
    data_structures.add_node_to_graph(graph, "F2", pos=np.array([11,0,0]), radius=0.1, type='synthetic_terminal', Q_flow=0.0005)
    data_structures.add_edge_to_graph(graph, "F1", "F2", length=1.0, radius=0.1)

    # Capture logs to verify the singularity message
    with caplog.at_level(logging.ERROR, logger="src.perfusion_solver"):
        solved_graph = perfusion_solver.solve_1d_poiseuille_flow(graph, config)

    # Check if the singularity error was logged
    assert any("Matrix A is singular or rank-deficient" in record.message for record in caplog.records), \
        "Expected singularity error was not logged."

    # When matrix is singular, current solver sets all pressures and flows to NaN
    assert np.isnan(solved_graph.nodes['R0']['pressure'])
    assert np.isnan(solved_graph.nodes['T0']['pressure'])
    assert np.isnan(solved_graph.nodes['F1']['pressure'])
    assert np.isnan(solved_graph.nodes['F2']['pressure'])
    
    assert np.isnan(solved_graph.edges[('R0','T0')]['flow_solver'])
    assert np.isnan(solved_graph.edges[('F1','F2')]['flow_solver'])

    def test_solve_two_disconnected_y_junctions(basic_config_flow_test, caplog):
    
    # tests/test_io_utils.py
import pytest
import nibabel as nib
import numpy as np
import pyvista as pv
import networkx as nx
import os
import yaml
import shutil # For cleaning up
from src import io_utils, data_structures # For graph creation helpers
from src import constants # For default values

@pytest.fixture(scope="module") # Use module scope for tmp_path to reduce overhead
def test_output_dir(tmp_path_factory):
    # Create a single temporary directory for all tests in this module
    tdir = tmp_path_factory.mktemp("io_test_data")
    return tdir

# --- NIfTI I/O Tests ---
def test_nifti_save_load_roundtrip(test_output_dir):
    filepath = test_output_dir / "test.nii.gz"
    shape = (5, 5, 5)
    affine = np.diag([0.5, 0.5, 0.5, 1.0]) # 0.5mm isotropic
    data = np.random.rand(*shape).astype(np.float32)
    
    io_utils.save_nifti_image(data, affine, str(filepath))
    assert os.path.exists(filepath)
    
    loaded_data, loaded_affine, _ = io_utils.load_nifti_image(str(filepath))
    assert loaded_data is not None
    assert np.allclose(data, loaded_data)
    assert np.allclose(affine, loaded_affine)


def test_nifti_save_load_boolean(test_output_dir):
    filepath = test_output_dir / "test_bool.nii.gz"
    shape = (5, 5, 5)
    affine = np.eye(4)
    original_bool_data = np.random.choice([True, False], size=shape)

    # In save_nifti_image, boolean data is cast to uint8
    io_utils.save_nifti_image(original_bool_data, affine, str(filepath))
    
    # load_nifti_image casts to float32
    loaded_data_float, _, _ = io_utils.load_nifti_image(str(filepath)) 

    assert loaded_data_float is not None
    assert loaded_data_float.dtype == np.float32 # Expect float32 after loading

    # Check if the values are preserved (0.0 for False, 1.0 for True)
    # Cast original boolean data to float for comparison
    expected_float_values = original_bool_data.astype(np.float32)
    assert np.all(loaded_data_float == expected_float_values)

def test_load_nifti_non_existent(caplog):
    data, affine, header = io_utils.load_nifti_image("non_existent_file.nii.gz")
    assert data is None
    assert affine is None
    assert header is None
    assert "NIfTI file not found" in caplog.text

# --- VTP Arterial Centerlines I/O Tests ---
@pytest.fixture
def sample_vtp_file(test_output_dir):
    filepath = test_output_dir / "sample_arteries.vtp"
    points = np.array([[0,0,0], [1,1,0], [2,0,0]], dtype=float)
    lines = np.array([2, 0, 1, 2, 1, 2]) # Connect point 0-1, then 1-2
    poly = pv.PolyData(points, lines=lines)
    poly.point_data['radius'] = np.array([0.5, 0.4, 0.3])
    poly.save(str(filepath))
    return str(filepath)

def test_load_arterial_centerlines_vtp_valid(sample_vtp_file):
    poly_data = io_utils.load_arterial_centerlines_vtp(sample_vtp_file)
    assert poly_data is not None
    assert poly_data.n_points == 3
    assert 'radius' in poly_data.point_data
    assert np.allclose(poly_data.point_data['radius'], [0.5, 0.4, 0.3])

def test_load_arterial_centerlines_vtp_no_radius(test_output_dir, caplog):
    filepath_no_radius = test_output_dir / "no_radius.vtp"
    points = np.array([[0,0,0], [1,0,0]], dtype=float)
    lines = np.array([2,0,1])
    poly = pv.PolyData(points, lines=lines)
    poly.save(str(filepath_no_radius))
    
    poly_data = io_utils.load_arterial_centerlines_vtp(str(filepath_no_radius))
    assert poly_data is not None
    assert 'radius' not in poly_data.point_data
    assert "does not contain 'radius' point data" in caplog.text

def test_load_arterial_centerlines_vtp_non_existent(caplog):
    poly_data = io_utils.load_arterial_centerlines_vtp("non_existent.vtp")
    assert poly_data is None
    assert "Arterial centerline file not found" in caplog.text

# --- TXT Arterial Centerlines I/O Tests ---
@pytest.fixture
def sample_txt_file(test_output_dir):
    filepath = test_output_dir / "sample_arteries.txt"
    content = (
        "# Test TXT file\n"
        "0 0 0 0.5\n"  # x y z radius
        "1 1 0 0.4\n"
        "2 0 0      \n"  # x y z (use default radius)
        "3 1 1 0.2\n"
    )
    with open(filepath, "w") as f:
        f.write(content)
    return str(filepath)

def test_load_arterial_centerlines_txt_valid(sample_txt_file):
    poly_data = io_utils.load_arterial_centerlines_txt(sample_txt_file, radius_default=0.1)
    assert poly_data is not None
    assert poly_data.n_points == 4
    assert 'radius' in poly_data.point_data
    expected_radii = np.array([0.5, 0.4, 0.1, 0.2])
    assert np.allclose(poly_data.point_data['radius'], expected_radii)
    # Check lines (assumes single polyline connection)
    assert poly_data.n_cells == 3 # 3 segments for 4 points

def test_load_arterial_centerlines_txt_non_existent(caplog):
    poly_data = io_utils.load_arterial_centerlines_txt("non_existent.txt")
    assert poly_data is None
    assert "Arterial centerline TXT file not found" in caplog.text

# --- Vascular Tree (NetworkX to VTP) Save/Load Test ---
@pytest.fixture
def sample_nx_graph():
    graph = data_structures.create_empty_vascular_graph()
    data_structures.add_node_to_graph(graph, "n0", pos=np.array([0.,0.,0.]), radius=0.5, pressure=100.0, type="root")
    data_structures.add_node_to_graph(graph, "n1", pos=np.array([1.,0.,0.]), radius=0.4, pressure=90.0, type="segment")
    data_structures.add_node_to_graph(graph, "n2", pos=np.array([1.,1.,0.]), radius=0.3, pressure=80.0, type="terminal")
    
    data_structures.add_edge_to_graph(graph, "n0", "n1", flow=10.0, type="segment_edge")
    data_structures.add_edge_to_graph(graph, "n1", "n2", flow=8.0, type="segment_edge")
    return graph

def test_save_vascular_tree_vtp_valid(sample_nx_graph, test_output_dir):
    filepath = test_output_dir / "vascular_tree_output.vtp"
    io_utils.save_vascular_tree_vtp(sample_nx_graph, str(filepath))
    assert os.path.exists(filepath)

    # Load back with PyVista and verify
    loaded_poly = pv.read(filepath)
    assert loaded_poly.n_points == 3
    assert 'radius' in loaded_poly.point_data
    assert 'pressure' in loaded_poly.point_data
    assert np.allclose(loaded_poly.point_data['radius'], [0.5, 0.4, 0.3]) # Order dependent on node iteration
    
    # Check flow on cells (edges)
    assert loaded_poly.n_cells == 2 # number of edges
    assert 'flow' in loaded_poly.cell_data
    # The order of flows in cell_data depends on the order edges were processed.
    # For a robust check, one might need to map cells back to graph edges if order isn't guaranteed.
    # For now, check if values exist and are correct, assuming a small graph maintains order.
    # This might be brittle. A better check might involve querying specific cells if possible.
    # For now, let's check if the set of flows is correct.
    assert set(np.round(loaded_poly.cell_data['flow'], 5)) == {10.0, 8.0}


def test_save_vascular_tree_vtp_empty_graph(test_output_dir, caplog):
    filepath = test_output_dir / "empty_tree.vtp"
    empty_graph = data_structures.create_empty_vascular_graph()
    io_utils.save_vascular_tree_vtp(empty_graph, str(filepath))
    
    assert os.path.exists(filepath)
    assert "Saved an empty VTP file" in caplog.text # Check for our specific log message
    
    loaded_poly = pv.read(filepath)
    assert loaded_poly.n_points == 0
    assert loaded_poly.n_cells == 0

# --- Simulation Parameters Save Test ---
def test_save_simulation_parameters(test_output_dir):
    filepath = test_output_dir / "sim_params_test.yaml"
    test_config = {"param_A": 123, "group_B": {"sub_param_C": "value_test"}}
    
    io_utils.save_simulation_parameters(test_config, str(filepath))
    assert os.path.exists(filepath)
    
    with open(filepath, 'r') as f:
        loaded_params = yaml.safe_load(f)
    
    assert loaded_params == test_config# tests/test_config_manager.py
import pytest
import yaml
import os
from src import config_manager
import logging

# Fixture to create a temporary valid config file
@pytest.fixture
def temp_valid_config_file(tmp_path):
    config_data = {
        "paths": {"output_dir": "test_output"},
        "simulation": {"random_seed": 123, "log_level": "DEBUG"},
        "nested_params": {"level1": {"level2": "value"}}
    }
    config_file = tmp_path / "valid_config.yaml"
    with open(config_file, 'w') as f:
        yaml.dump(config_data, f)
    return str(config_file), config_data

# Fixture to create a temporary malformed config file
@pytest.fixture
def temp_malformed_config_file(tmp_path):
    config_file = tmp_path / "malformed_config.yaml"
    with open(config_file, 'w') as f:
        f.write("paths: {output_dir: test_output\nlog_level: INFO") # Malformed YAML
    return str(config_file)

def test_load_config_valid(temp_valid_config_file):
    config_path, expected_data = temp_valid_config_file
    config = config_manager.load_config(config_path)
    assert config == expected_data
    assert config_manager.get_param(config, "simulation.random_seed") == 123

def test_load_config_non_existent():
    with pytest.raises(FileNotFoundError):
        config_manager.load_config("non_existent_config.yaml")

def test_load_config_malformed(temp_malformed_config_file):
    config_path = temp_malformed_config_file
    with pytest.raises(yaml.YAMLError):
        config_manager.load_config(config_path)

def test_get_param_existing(temp_valid_config_file):
    config_path, config_data = temp_valid_config_file
    config = config_manager.load_config(config_path)
    
    assert config_manager.get_param(config, "paths.output_dir") == "test_output"
    assert config_manager.get_param(config, "simulation.random_seed") == 123
    assert config_manager.get_param(config, "nested_params.level1.level2") == "value"

def test_get_param_non_existent(temp_valid_config_file):
    config_path, _ = temp_valid_config_file
    config = config_manager.load_config(config_path)

    assert config_manager.get_param(config, "non.existent.key") is None
    assert config_manager.get_param(config, "non.existent.key", "default_val") == "default_val"
    assert config_manager.get_param(config, "simulation.non_existent", 999) == 999

def test_create_default_config_new_file(tmp_path):
    default_config_path = tmp_path / "default_config.yaml"
    assert not os.path.exists(default_config_path)
    
    config_manager.create_default_config(str(default_config_path))
    assert os.path.exists(default_config_path)
    
    # Try loading it to ensure it's valid YAML
    try:
        loaded_default_config = config_manager.load_config(str(default_config_path))
        assert "paths" in loaded_default_config # Check a known top-level key
        assert "output_dir" in loaded_default_config["paths"]
    except Exception as e:
        pytest.fail(f"Loading default config failed: {e}")


def test_create_default_config_existing_file(tmp_path, caplog):
    default_config_path = tmp_path / "existing_default_config.yaml"
    # Create a dummy file first
    with open(default_config_path, "w") as f:
        f.write("some: content")
    
    # Temporarily set the logging level for the relevant logger for this test
    # This ensures INFO messages are captured.
    # You can also set this globally in pytest.ini or pyproject.toml if needed for many tests.
    with caplog.at_level(logging.INFO, logger="src.config_manager"): # Specify the logger name
        config_manager.create_default_config(str(default_config_path))
    
    # Check that the file was not overwritten and a log message was produced
    with open(default_config_path, "r") as f:
        content = f.read()
        assert content == "some: content"
    
    print(f"Captured log text: {caplog.text}") # For debugging if it still fails
    assert f"Configuration file already exists: {str(default_config_path)}" in caplog.text# src/perfusion_solver.py
from __future__ import annotations
import numpy as np
import networkx as nx
import logging
from typing import Dict, Tuple, Optional, List

from src import constants, config_manager, utils

logger = logging.getLogger(__name__)

def calculate_segment_resistance(length: float, radius: float, viscosity: float) -> float:
    """Calculates hydraulic resistance of a cylindrical segment using Poiseuille's law.
    R = (8 * mu * L) / (pi * r^4)
    """
    if radius < constants.EPSILON: # Avoid division by zero or extremely small radius
        return np.inf
    if length < constants.EPSILON: # Segment with negligible length
        # Return a very small resistance to ensure conductance is high but finite
        return constants.EPSILON
    return (8.0 * viscosity * length) / (constants.PI * (radius**4))

def solve_1d_poiseuille_flow(
    graph: nx.DiGraph,
    config: dict,
    root_pressure_val: Optional[float] = None,
    terminal_flows_val: Optional[Dict[str, float]] = None
) -> nx.DiGraph:
    """Solves for nodal pressures and segmental flows in a vascular graph.

    Modifies the input graph in-place by adding/updating:
    - 'pressure' attribute to nodes.
    - 'flow_solver' attribute to edges.

    Assumptions on input graph:
    - Nodes:
        - Must have a unique ID.
        - Pressure inlet nodes must have `is_flow_root=True`.
        - Flow sink nodes (terminals) must have a 'Q_flow' attribute (target demand/outflow)
          and be identifiable (e.g., `type='synthetic_terminal'` and `graph.out_degree(node_id) == 0`,
          or an explicit `is_flow_terminal=True`).
    - Edges:
        - Must have 'length' and 'radius' attributes. The 'radius' attribute on an edge
          typically represents the radius of the upstream segment/node.
    """
    if graph.number_of_nodes() == 0:
        logger.warning("Flow solver: Graph is empty. Nothing to solve.")
        return graph

    logger.info(f"Starting 1D Poiseuille flow solution for graph with {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges.")

    viscosity = config_manager.get_param(config, "vascular_properties.blood_viscosity", constants.DEFAULT_BLOOD_VISCOSITY)
    if root_pressure_val is None:
        inlet_pressure = config_manager.get_param(config, "perfusion_solver.inlet_pressure", 10000.0) # Pa
    else:
        inlet_pressure = root_pressure_val

    node_list = list(graph.nodes())
    node_to_idx = {node_id: i for i, node_id in enumerate(node_list)}
    num_nodes = len(node_list)

    # Initialize graph attributes in case of early exit or errors
    for node_id_init in node_list:
        graph.nodes[node_id_init]['pressure'] = np.nan
    for u_init, v_init, data_init in graph.edges(data=True):
        data_init['flow_solver'] = np.nan

    if num_nodes == 0: # Should have been caught, but as a safeguard
        return graph

    A_matrix = np.zeros((num_nodes, num_nodes), dtype=float)
    B_vector = np.zeros(num_nodes, dtype=float)
    num_defined_pressure_bcs = 0
    num_defined_flow_sinks = 0

    logger.debug("--- Flow Solver: Assembling System Matrix A and Vector B ---")
    for i, node_id_i in enumerate(node_list):
        node_i_data = graph.nodes[node_id_i]

        # --- Set up equation for node i ---
        if node_i_data.get('is_flow_root', False):
            # Pressure Boundary Condition: P_i = inlet_pressure
            A_matrix[i, :] = 0.0
            A_matrix[i, i] = 1.0
            B_vector[i] = inlet_pressure
            num_defined_pressure_bcs += 1
            logger.debug(f"  BC Set (Pressure): Node {node_id_i} (type: {node_i_data.get('type')}) is flow_root. Row {i}: A[{i},{i}]=1, B[{i}]={inlet_pressure:.2f}")
            continue # Move to the next node

        # Flow Conservation Equation for non-root nodes: sum(G_ij * (P_j - P_i)) = Q_source_sink_i
        # Rearranged: sum(G_ij * P_j) - (sum(G_ij)) * P_i = Q_source_sink_i
        # So, A[i,i] = -sum(G_ij) and A[i,j] = G_ij.
        # Or, (sum(G_ij)) * P_i - sum(G_ij * P_j) = -Q_source_sink_i
        # So, A[i,i] = sum_over_j(G_ij) and A[i,j] = -G_ij
        # The current implementation uses the second form for A_matrix terms.

        sum_conductances_at_i = 0.0
        # Consider all physical neighbors (connected by an edge, regardless of direction in DiGraph)
        physical_neighbors = set(graph.predecessors(node_id_i)) | set(graph.successors(node_id_i))
        current_row_terms_log = []

        for neighbor_id_j in physical_neighbors:
            if neighbor_id_j not in node_to_idx:
                logger.error(f"  Error: Neighbor {neighbor_id_j} of {node_id_i} not in node_to_idx map. Skipping.")
                continue
            j = node_to_idx[neighbor_id_j]

            edge_data = None
            if graph.has_edge(node_id_i, neighbor_id_j):
                edge_data = graph.edges[node_id_i, neighbor_id_j]
            elif graph.has_edge(neighbor_id_j, node_id_i):
                edge_data = graph.edges[neighbor_id_j, node_id_i]

            if edge_data:
                length = edge_data.get('length', 0.0)
                # Radius for resistance calculation is taken from the edge attribute
                radius = edge_data.get('radius', constants.MIN_VESSEL_RADIUS_MM)
                radius = max(radius, constants.MIN_VESSEL_RADIUS_MM * 0.01) # Ensure tiny positive
                if length < constants.EPSILON:
                    length = constants.EPSILON # Use tiny length for zero-length segments

                resistance = calculate_segment_resistance(length, radius, viscosity)
                conductance = 0.0
                if resistance < np.inf and resistance > constants.EPSILON:
                    conductance = 1.0 / resistance
                elif resistance <= constants.EPSILON: # Effectively zero resistance
                    conductance = 1.0 / (constants.EPSILON * 10) # Very high conductance
                    logger.debug(f"  Edge involving {node_id_i}-{neighbor_id_j}: R~0, using high G={conductance:.1e}")

                if conductance > 0:
                    A_matrix[i, j] -= conductance # Off-diagonal term: -G_ij
                    sum_conductances_at_i += conductance
                    current_row_terms_log.append(f"G_({node_id_i}-{neighbor_id_j})={conductance:.2e} (to P_{neighbor_id_j})")
            else:
                 logger.warning(f"  No edge data found between physically connected {node_id_i} and {neighbor_id_j}.")

        A_matrix[i, i] = sum_conductances_at_i # Diagonal term: sum_over_j(G_ij)

        # Handle flow sinks (outflow boundary condition)
        # A terminal for flow sink is a node with no outgoing segments AND it's not a pressure root.
        is_graph_terminal_for_flow = (graph.out_degree(node_id_i) == 0 and not node_i_data.get('is_flow_root', False))

        if (node_i_data.get('type') == 'synthetic_terminal' and is_graph_terminal_for_flow) or \
           node_i_data.get('is_flow_terminal', False): # Explicitly marked as a flow terminal
            outflow_demand = 0.0
            if terminal_flows_val and node_id_i in terminal_flows_val: # Prioritize explicitly passed terminal flows
                outflow_demand = terminal_flows_val[node_id_i]
            elif 'Q_flow' in node_i_data: # Fallback to Q_flow attribute on the node
                outflow_demand = node_i_data['Q_flow']
            else:
                logger.warning(f"  Terminal node {node_id_i} (type: {node_i_data.get('type')}) has no Q_flow or provided outflow. Assuming zero demand.")

            B_vector[i] -= outflow_demand # Flow leaving node i is -Q on RHS (sum(G_ij*P_j) - sum(G_ij)*P_i = -Q_out)
            num_defined_flow_sinks +=1
            logger.debug(f"  BC Set (Flow Sink): Node {node_id_i}. B[{i}] -= {outflow_demand:.2e} (Total B[{i}]={B_vector[i]:.2e})")
        elif B_vector[i] == 0.0 and not is_graph_terminal_for_flow and not node_i_data.get('is_flow_root', False): # Internal node with no explicit source/sink
            logger.debug(f"  Matrix Row {i} ({node_id_i}, type: {node_i_data.get('type')}): Internal node. B[{i}]=0.")
        
        logger.debug(f"  Matrix Row {i} ({node_id_i}, type: {node_i_data.get('type')}): Diag A[{i},{i}]={A_matrix[i,i]:.2e}. Off-diag terms: {', '.join(current_row_terms_log) if current_row_terms_log else 'None'}. B[{i}]={B_vector[i]:.2e}")


    logger.debug(f"--- Flow Solver: System Matrix A (shape {A_matrix.shape}):\n{A_matrix if num_nodes < 10 else 'Too large to print'}")
    logger.debug(f"--- Flow Solver: System Vector B (shape {B_vector.shape}):\n{B_vector if num_nodes < 10 else 'Too large to print'}")
    logger.info(f"Flow Solver: Total defined pressure BCs (is_flow_root): {num_defined_pressure_bcs}")
    logger.info(f"Flow Solver: Total defined flow sinks (terminals with Q_flow): {num_defined_flow_sinks}")

    if num_defined_pressure_bcs == 0 and num_nodes > 0:
        logger.error("Flow solver: No pressure boundary conditions (is_flow_root=True) defined. Cannot solve. Pressures and flows will be NaN.")
        return graph # Graph attributes already initialized to NaN

    node_pressures_vec = np.full(num_nodes, np.nan) # Default to NaN

    try:
        rank_A = 0
        if num_nodes > 0:
            rank_A = np.linalg.matrix_rank(A_matrix)
        logger.debug(f"Flow Solver: Matrix A Rank = {rank_A} for {num_nodes} nodes.")

        if num_nodes > 0 and rank_A < num_nodes:
            logger.error(f"Flow solver: Matrix A is singular or rank-deficient (rank {rank_A} for {num_nodes} nodes). Cannot solve. "
                         "Check for disconnected graph components not attached to a pressure BC, or insufficient/inconsistent outflow conditions. "
                         "Pressures and flows will be NaN.")
            # Graph attributes already initialized to NaN
        else:
            node_pressures_vec = np.linalg.solve(A_matrix, B_vector)
            logger.info("Flow solver: Successfully solved for nodal pressures.")
            # logger.debug(f"Solved Node Pressures Vector:\n{node_pressures_vec}")

    except np.linalg.LinAlgError as e:
        logger.error(f"Flow solver: Linear algebra error during pressure solution: {e}", exc_info=False)
        # Log condition number only if matrix is likely well-formed enough for it
        if num_nodes > 0 and rank_A == num_nodes and not np.all(A_matrix == 0):
             logger.error(f"Matrix A Condition Number: {np.linalg.cond(A_matrix)}")
        # Graph attributes already initialized to NaN

    # Assign solved pressures to graph nodes
    for i, node_id in enumerate(node_list):
        graph.nodes[node_id]['pressure'] = node_pressures_vec[i] # Will be NaN if solve failed or matrix was singular

    # Calculate flows on edges based on solved pressures
    for u_id, v_id, data in graph.edges(data=True):
        P_u = graph.nodes[u_id]['pressure']
        P_v = graph.nodes[v_id]['pressure']

        if np.isnan(P_u) or np.isnan(P_v): # If pressures couldn't be solved, flow is also unknown
            data['flow_solver'] = np.nan
            continue

        length = data.get('length', 0.0)
        radius = data.get('radius', constants.MIN_VESSEL_RADIUS_MM)
        radius = max(radius, constants.MIN_VESSEL_RADIUS_MM * 0.01)
        if length < constants.EPSILON:
            length = constants.EPSILON

        resistance = calculate_segment_resistance(length, radius, viscosity)
        flow_on_edge = 0.0
        if resistance < np.inf and resistance > constants.EPSILON:
            flow_on_edge = (P_u - P_v) / resistance
        elif resistance <= constants.EPSILON: # Near zero resistance
            # If pressures are different, flow could be very large.
            # This case implies P_u should be very close to P_v.
            if not np.isclose(P_u, P_v, atol=1e-3): # Allow small tolerance for numerical error
                 logger.warning(f"Edge {u_id}->{v_id} has R~0 ({resistance:.1e}) but P_diff {(P_u-P_v):.2e}. Flow may be very large/ill-defined.")
                 # Use a very high conductance to estimate flow, but this situation is often problematic.
                 flow_on_edge = (P_u - P_v) / (constants.EPSILON * 10)
            # else, if P_u is close to P_v, flow is near zero, which is fine.
        elif resistance == np.inf: # Infinite resistance, zero flow
            flow_on_edge = 0.0


        data['flow_solver'] = flow_on_edge
        if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f"Edge {u_id}->{v_id}: P_u={P_u:.2f}, P_v={P_v:.2f}, L={length:.3f}, R_edge={radius:.4f}, Res={resistance:.2e}, Q_solver={flow_on_edge:.2e}")

    logger.info("Flow solver: Finished annotating graph with pressures and flows.")
    return graph# src/config_manager.py
import yaml
import os
from typing import Any, Dict
import logging

logger = logging.getLogger(__name__)

def load_config(config_path: str) -> Dict[str, Any]:
    """
    Loads a YAML configuration file.

    Args:
        config_path (str): Path to the YAML configuration file.

    Returns:
        Dict[str, Any]: A dictionary containing the configuration parameters.
    
    Raises:
        FileNotFoundError: If the config file is not found.
        yaml.YAMLError: If there's an error parsing the YAML file.
    """
    if not os.path.exists(config_path):
        logger.error(f"Configuration file not found: {config_path}")
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        logger.info(f"Successfully loaded configuration from: {config_path}")
        return config
    except yaml.YAMLError as e:
        logger.error(f"Error parsing YAML configuration file {config_path}: {e}")
        raise

def get_param(config: Dict[str, Any], key_path: str, default: Any = None) -> Any:
    """
    Retrieves a parameter from the config dictionary using a dot-separated key path.
    Example: get_param(config, "simulation.gbo.max_iterations")

    Args:
        config (Dict[str, Any]): The configuration dictionary.
        key_path (str): Dot-separated path to the key (e.g., "parent.child.key").
        default (Any, optional): Default value to return if key is not found. Defaults to None.

    Returns:
        Any: The parameter value or the default value.
    """
    keys = key_path.split('.')
    value = config
    try:
        for key in keys:
            value = value[key]
        return value
    except (KeyError, TypeError):
        logger.warning(f"Parameter '{key_path}' not found in config. Using default: {default}")
        return default

def create_default_config(config_path: str = "config.yaml"):
    """
    Creates a default configuration file if it doesn't exist.
    """
    from src import constants # To access default values

    default_config_content = {
        "paths": {
            "output_dir": "output/simulation_results",
            "wm_nifti": "data/sample_brain_wm.nii.gz",
            "gm_nifti": "data/sample_brain_gm.nii.gz",
            "csf_nifti": "data/sample_brain_csf.nii.gz", # Optional
            "tumor_nifti": "data/sample_tumor.nii.gz", # Optional
            "arterial_centerlines": "data/sample_arteries.vtp" # or .txt
        },
        "simulation": {
            "random_seed": 42,
            "log_level": "INFO", # DEBUG, INFO, WARNING, ERROR
            "units": { # Define units used for consistency, e.g. 'mm' for length
                "length": "mm",
                "pressure": "Pa", # Pascal
                "flow_rate": "mm^3/s"
            }
        },
        "tissue_properties": {
            "metabolic_rates": { # in 1/s (ml_blood / s / ml_tissue)
                "gm": constants.Q_MET_GM_PER_ML,
                "wm": constants.Q_MET_WM_PER_ML,
                "tumor_rim": constants.Q_MET_TUMOR_RIM_PER_ML,
                "tumor_core": constants.Q_MET_TUMOR_CORE_PER_ML,
                "csf": constants.Q_MET_CSF_PER_ML
            },
            "permeability": { # in mm^2 (if length unit is mm)
                "gm": constants.DEFAULT_TISSUE_PERMEABILITY_GM,
                "wm": constants.DEFAULT_TISSUE_PERMEABILITY_WM,
                # Tumor permeability can be higher
                "tumor": 5e-7 # mm^2
            }
        },
        "vascular_properties": {
            "blood_viscosity": constants.DEFAULT_BLOOD_VISCOSITY, # Pa.s
            "murray_law_exponent": constants.MURRAY_LAW_EXPONENT,
            "initial_terminal_flow": constants.INITIAL_TERMINAL_FLOW_Q, # mm^3/s (if units are mm)
            "min_segment_length": 0.1, # mm
            "max_segment_length": 2.0, # mm
            "min_radius": 0.005 # mm (e.g. 5 microns)
        },
        "gbo_growth": {
            "max_iterations": 100,
            "energy_coefficient_C_met": constants.DEFAULT_C_MET_VESSEL_WALL, # W/m^3 or equivalent in chosen units
            "target_perfusion_level": 0.9, # Target fraction of demand to be met
            "branching_threshold_radius": 0.2, # mm (example: branch if terminal radius exceeds this)
            "bifurcation_angle_search_steps": 5, # Number of angles to test
            "bifurcation_length_factor": 0.5, # New segments are this factor of parent segment length (heuristic)
            "stop_criteria": {
                "max_radius_factor_measured": 1.0 # Stop if synth. radius = 1.0 * measured_terminal_radius
            }
        },
        "tumor_angiogenesis": {
            "enabled": True,
            "sprouting_vessel_min_radius": 0.1, # mm
            "growth_bias_strength": 0.5, # For biased random walk
            "max_tumor_vessels": 500,
            "segment_length_mean_tumor": 0.3, # mm
            "segment_length_std_tumor": 0.1, # mm
            "tortuosity_factor": 1.5 # How much more tortuous tumor vessels are
        },
        "perfusion_solver": {
            "enabled": True,
            "inlet_pressure": 10000, # Pa (approx 75 mmHg)
            "terminal_outlet_pressure": 2000, # Pa (approx 15 mmHg, for network solver if applicable)
            "coupling_beta": constants.DEFAULT_COUPLING_BETA, # mm^3 / (s*Pa)
            "use_flow_sinks_for_terminals": True, # Example, if we add options later
            "max_iterations_coupling": 20,
            "convergence_tolerance_coupling": 1e-4,
            "darcy_solver_max_iter": 1000,
            "darcy_solver_tolerance": 1e-6
        },
        "visualization":{
            "save_intermediate_steps": False,
            "plot_slice_axis": "axial", # axial, sagittal, coronal
            "plot_slice_index": -1 # -1 for middle slice
        }
    }
    if not os.path.exists(config_path):
        with open(config_path, 'w') as f:
            yaml.dump(default_config_content, f, sort_keys=False, indent=4)
        logger.info(f"Created default configuration file: {config_path}")
    else:
        logger.info(f"Configuration file already exists: {config_path}")

if __name__ == '__main__':
    # Example usage:
    logging.basicConfig(level=logging.INFO)
    
    # Create a dummy config.yaml if it doesn't exist in the script's directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    dummy_config_path = os.path.join(script_dir, "..", "config.yaml") # Assumes script is in src/
    
    if not os.path.exists(dummy_config_path):
        os.makedirs(os.path.dirname(dummy_config_path), exist_ok=True)
        create_default_config(dummy_config_path)

    try:
        config = load_config(dummy_config_path)
        print("Config loaded successfully.")
        print("Random seed:", get_param(config, "simulation.random_seed", 0))
        print("GM metabolic rate:", get_param(config, "tissue_properties.metabolic_rates.gm"))
        print("Non-existent param:", get_param(config, "foo.bar.baz", "default_val"))
        
        # Test output directory creation from config
        output_dir = get_param(config, "paths.output_dir", "output/default_sim")
        os.makedirs(output_dir, exist_ok=True)
        print(f"Ensured output directory exists: {output_dir}")

    except Exception as e:
        print(f"Error in example usage: {e}")# src/vascular_growth.py
from __future__ import annotations # Must be first line for postponed evaluation of annotations

import numpy as np
import networkx as nx
import logging
import os
from scipy.spatial import KDTree
from typing import Tuple, List, Dict, Set, Optional

from src import utils, data_structures, constants, config_manager
from src import energy_model
from src import perfusion_solver
from src import io_utils # Added missing import

logger = logging.getLogger(__name__)

class GBOIterationData:
    def __init__(self, terminal_id: str, pos: np.ndarray, radius: float, flow: float,
                 is_synthetic: bool = True, original_measured_radius: Optional[float] = None,
                 parent_id: Optional[str] = None, parent_measured_terminal_id: Optional[str] = None):
        self.id: str = terminal_id
        self.pos: np.ndarray = np.array(pos, dtype=float)
        self.radius: float = float(radius)
        self.flow: float = float(flow)
        self.parent_id: Optional[str] = parent_id
        self.parent_measured_terminal_id: Optional[str] = parent_measured_terminal_id
        self.original_measured_terminal_radius: Optional[float] = original_measured_radius
        self.length_from_parent: float = 0.0
        self.is_synthetic: bool = is_synthetic
        self.stop_growth: bool = False
        self.current_territory_voxel_indices_flat: List[int] = []
        self.current_territory_demand: float = 0.0


def initialize_perfused_territory_and_terminals(
    config: dict,
    initial_graph: Optional[nx.DiGraph], # Parsed VTP graph from main.py
    tissue_data: dict # Contains various masks and affine
) -> Tuple[np.ndarray, List[GBOIterationData], int, nx.DiGraph]:
    logger.info("Initializing perfused territory and active GBO terminals...")
    perfused_tissue_mask = np.zeros(tissue_data['shape'], dtype=bool)
    active_terminals: List[GBOIterationData] = []

    initial_synthetic_radius_default = config_manager.get_param(config, "vascular_properties.min_radius", 0.005)
    k_murray_factor = config_manager.get_param(config, "vascular_properties.k_murray_scaling_factor", 0.5)
    murray_exponent = config_manager.get_param(config, "vascular_properties.murray_law_exponent", 3.0)
    default_initial_flow = config_manager.get_param(config, "vascular_properties.initial_terminal_flow", constants.INITIAL_TERMINAL_FLOW_Q)

    next_synthetic_node_id = 0

    gbo_graph = initial_graph.copy() if initial_graph and initial_graph.number_of_nodes() > 0 else data_structures.create_empty_vascular_graph()
    if initial_graph is None or initial_graph.number_of_nodes() == 0:
        logger.info("No initial VTP graph provided or it's empty. GBO will rely on config seeds or fallback.")

    # Mark VTP roots from input graph as flow roots for the solver
    if initial_graph:
        for node_id, data in initial_graph.nodes(data=True):
            if data.get('type') == 'measured_root':
                if gbo_graph.has_node(node_id):
                    gbo_graph.nodes[node_id]['is_flow_root'] = True
                    gbo_graph.nodes[node_id]['Q_flow'] = 0.0 # Pressure BC
                    logger.info(f"Marked VTP input node {node_id} (type: measured_root) as is_flow_root=True in gbo_graph.")

    processed_from_vtp_terminals = False
    if initial_graph:
        vtp_terminals_in_anatomical_domain = []
        # This is the GM+WM mask, where GBO growth is allowed
        gbo_growth_domain_mask = tissue_data.get('gbo_growth_domain_mask') 
        affine = tissue_data.get('affine')

        if gbo_growth_domain_mask is None or affine is None:
            logger.error("GBO growth domain mask or affine missing in tissue_data. Cannot reliably sprout from VTP terminals.")
        else:
            for node_id, data in initial_graph.nodes(data=True):
                # In main.py, VTP terminals within GM/WM/CSF were typed as 'measured_terminal_in_anatomical_domain'
                if data.get('type') == 'measured_terminal_in_anatomical_domain':
                    # Now, further check if this terminal is within the GBO growth domain (GM/WM)
                    pos_world = data['pos']
                    pos_vox_int = np.round(utils.world_to_voxel(pos_world, affine)).astype(int)

                    if utils.is_voxel_in_bounds(pos_vox_int, gbo_growth_domain_mask.shape) and \
                       gbo_growth_domain_mask[tuple(pos_vox_int)]:
                        vtp_terminals_in_anatomical_domain.append(node_id)
                    else:
                        logger.info(f"VTP terminal {node_id} (in anatomical domain) is outside GBO growth domain (GM/WM). Not sprouting GBO from it.")
                        # Optionally change its type in gbo_graph if it won't be a sprouting point
                        if gbo_graph.has_node(node_id):
                            gbo_graph.nodes[node_id]['type'] = 'measured_terminal_non_parenchymal' # e.g. ends in CSF

        if vtp_terminals_in_anatomical_domain:
            logger.info(f"Found {len(vtp_terminals_in_anatomical_domain)} VTP terminals within GBO growth domain (GM/WM) to sprout from.")
            for measured_terminal_id in vtp_terminals_in_anatomical_domain:
                measured_data = initial_graph.nodes[measured_terminal_id] # Data from original VTP graph
                measured_pos = np.array(measured_data['pos'], dtype=float)
                original_measured_radius = measured_data.get('radius', initial_synthetic_radius_default) 

                gbo_sprout_id = f"s_{next_synthetic_node_id}"; next_synthetic_node_id += 1

                term_gbo_data_obj = GBOIterationData(
                    terminal_id=gbo_sprout_id, pos=measured_pos,
                    radius=initial_synthetic_radius_default, flow=default_initial_flow,
                    original_measured_radius=original_measured_radius,
                    parent_id=measured_terminal_id,
                    parent_measured_terminal_id=measured_terminal_id
                )
                active_terminals.append(term_gbo_data_obj)

                node_attrs_for_s_node = vars(term_gbo_data_obj).copy()
                node_attrs_for_s_node['type'] = 'synthetic_terminal'
                node_attrs_for_s_node['is_flow_root'] = False
                node_attrs_for_s_node['Q_flow'] = term_gbo_data_obj.flow # Initial demand for solver
                node_attrs_for_s_node.pop('current_territory_voxel_indices_flat', None)
                node_attrs_for_s_node.pop('current_territory_demand', None)
                data_structures.add_node_to_graph(gbo_graph, gbo_sprout_id, **node_attrs_for_s_node)

                if gbo_graph.has_node(measured_terminal_id):
                     gbo_graph.nodes[measured_terminal_id]['type'] = 'measured_to_synthetic_junction'
                     if 'Q_flow' in gbo_graph.nodes[measured_terminal_id]: # Remove if it was typed as sink by main.py
                         del gbo_graph.nodes[measured_terminal_id]['Q_flow'] 
                
                edge_length_vtp_to_gbo = config_manager.get_param(config, "gbo_growth.vtp_sprout_connection_length", constants.EPSILON)
                data_structures.add_edge_to_graph(
                    gbo_graph, measured_terminal_id, gbo_sprout_id, 
                    length=edge_length_vtp_to_gbo, 
                    radius=initial_synthetic_radius_default, # Edge takes radius of the new GBO sprout
                    type='synthetic_sprout_from_measured'
                )
                logger.info(f"GBO Init: Sprouted synthetic_terminal '{gbo_sprout_id}' from VTP node '{measured_terminal_id}' "
                            f"(VTP R={original_measured_radius:.4f}, Sprout R_init={initial_synthetic_radius_default:.4f}).")
                processed_from_vtp_terminals = True
        
        if not processed_from_vtp_terminals and initial_graph.number_of_nodes() > 0 :
             if any(data.get('type') == 'measured_terminal_in_anatomical_domain' for _, data in initial_graph.nodes(data=True)):
                logger.warning("VTP terminals were found in anatomical domain, but none were within GBO growth domain (GM/WM). Checking config seeds.")
             else: # No 'measured_terminal_in_anatomical_domain' types were found at all
                logger.warning("Initial graph (VTP) provided but no 'measured_terminal_in_anatomical_domain' nodes found/processed by GBO init. Checking config seeds.")


    if not processed_from_vtp_terminals:
        seed_points_config = config_manager.get_param(config, "gbo_growth.seed_points", [])
        if seed_points_config and isinstance(seed_points_config, list):
            logger.info(f"No VTP terminals processed for GBO, or none valid. Using {len(seed_points_config)} GBO seed points from configuration.")
            for seed_info in seed_points_config:
                seed_id_base = seed_info.get('id', f"cfg_seed_{next_synthetic_node_id}")
                next_synthetic_node_id +=1

                seed_pos = np.array(seed_info.get('position'), dtype=float)
                seed_initial_radius = float(seed_info.get('initial_radius', initial_synthetic_radius_default))
                # Flow for config seeds is for radius calc, actual Q_flow on node for solver is 0 if is_flow_root
                seed_flow_for_radius = (seed_initial_radius / k_murray_factor) ** murray_exponent \
                                    if seed_initial_radius > constants.EPSILON and k_murray_factor > constants.EPSILON \
                                    else default_initial_flow
                term_gbo_data_obj = GBOIterationData(
                    terminal_id=seed_id_base, pos=seed_pos, radius=seed_initial_radius, flow=seed_flow_for_radius
                )
                active_terminals.append(term_gbo_data_obj)

                node_attrs_seed = vars(term_gbo_data_obj).copy()
                node_attrs_seed['is_flow_root'] = True 
                node_attrs_seed['type'] = 'synthetic_root_terminal'
                node_attrs_seed['Q_flow'] = 0.0 # Pressure BC for solver
                node_attrs_seed.pop('current_territory_voxel_indices_flat', None)
                node_attrs_seed.pop('current_territory_demand', None)
                data_structures.add_node_to_graph(gbo_graph, term_gbo_data_obj.id, **node_attrs_seed)
                logger.info(f"Initialized GBO seed terminal from config: {term_gbo_data_obj.id} at {np.round(seed_pos,2)} "
                            f"with R={seed_initial_radius:.4f}. Marked as is_flow_root.")
        elif not processed_from_vtp_terminals: # Only log if no VTP terminals AND no config seeds
             logger.info("No VTP terminals processed and no GBO seed points found in configuration. Checking fallback.")

    if not active_terminals: # Fallback seed
        logger.warning("No GBO starting points from VTP or config seeds. Attempting one fallback seed.")
        # Ensure 'domain_mask' for fallback is gbo_growth_domain_mask (GM/WM)
        fallback_domain_mask = tissue_data.get('gbo_growth_domain_mask', tissue_data.get('domain_mask'))
        if fallback_domain_mask is not None and np.any(fallback_domain_mask):
            seed_point_world = utils.get_random_point_in_mask(fallback_domain_mask, tissue_data['affine'])
            if seed_point_world is not None:
                fallback_id = f"s_fallback_{next_synthetic_node_id}"; next_synthetic_node_id +=1
                term_gbo_data_obj = GBOIterationData(fallback_id, seed_point_world, initial_synthetic_radius_default, default_initial_flow)
                active_terminals.append(term_gbo_data_obj)
                node_attrs_fallback = vars(term_gbo_data_obj).copy()
                node_attrs_fallback['is_flow_root'] = True; node_attrs_fallback['type'] = 'synthetic_root_terminal'
                node_attrs_fallback['Q_flow'] = 0.0 # Pressure BC
                node_attrs_fallback.pop('current_territory_voxel_indices_flat', None); node_attrs_fallback.pop('current_territory_demand', None)
                data_structures.add_node_to_graph(gbo_graph, fallback_id, **node_attrs_fallback)
                logger.info(f"Initialized fallback GBO seed terminal {fallback_id} at {np.round(seed_point_world,2)}.")
            else: logger.error("Cannot find a valid random seed point within GBO growth domain for fallback.")
        else: logger.error("No GBO growth domain_mask available for fallback seeding.")

    if not active_terminals:
        logger.error("CRITICAL: No GBO terminals to initialize growth. Aborting GBO.")
        return perfused_tissue_mask, [], next_synthetic_node_id, gbo_graph

    # Initial Territory Assignment uses 'world_coords_flat' derived from 'gbo_growth_domain_mask'
    if tissue_data.get('world_coords_flat') is None or tissue_data['world_coords_flat'].size == 0:
        logger.error("tissue_data['world_coords_flat'] (from GBO growth domain) is empty. Cannot initialize GBO territories.")
        for term_data in active_terminals: term_data.stop_growth = True
        return perfused_tissue_mask, active_terminals, next_synthetic_node_id, gbo_graph
        
    kdtree_gbo_domain_voxels = KDTree(tissue_data['world_coords_flat']) # KDTree of GM/WM voxels
    initial_territory_radius_search = config_manager.get_param(config, "gbo_growth.initial_territory_radius", 0.2) 

    for term_gbo_obj in active_terminals: # These are GBOIterationData objects (s_XXX)
        # Query against GM/WM kdtree
        nearby_flat_indices_in_gbo_domain = kdtree_gbo_domain_voxels.query_ball_point(term_gbo_obj.pos, r=initial_territory_radius_search)
        actual_demand_init = 0.0
        voxels_for_term_init_flat: List[int] = [] # These will be indices into tissue_data['world_coords_flat']

        if nearby_flat_indices_in_gbo_domain: # Check if any nearby voxels were found
            for local_kdtree_idx in nearby_flat_indices_in_gbo_domain:
                # The index from query_ball_point is an index into the points used to build kdtree_gbo_domain_voxels
                # which are tissue_data['world_coords_flat']. So, local_kdtree_idx IS the global_flat_idx.
                global_flat_idx = local_kdtree_idx
                
                v_3d_idx_tuple = tuple(tissue_data['voxel_indices_flat'][global_flat_idx]) # Get 3D index from global flat index
                
                if not perfused_tissue_mask[v_3d_idx_tuple]: 
                    perfused_tissue_mask[v_3d_idx_tuple] = True
                    actual_demand_init += tissue_data['metabolic_demand_map'][v_3d_idx_tuple] 
                    voxels_for_term_init_flat.append(global_flat_idx)
        
        term_gbo_obj.current_territory_voxel_indices_flat = voxels_for_term_init_flat
        term_gbo_obj.current_territory_demand = actual_demand_init
        
        if actual_demand_init > constants.EPSILON:
            term_gbo_obj.flow = actual_demand_init
            new_r = k_murray_factor * (term_gbo_obj.flow ** (1.0 / murray_exponent))
            term_gbo_obj.radius = max(initial_synthetic_radius_default, new_r) 
        else: 
            term_gbo_obj.flow = default_initial_flow 
            term_gbo_obj.radius = initial_synthetic_radius_default

        if gbo_graph.has_node(term_gbo_obj.id):
            gbo_node_data = gbo_graph.nodes[term_gbo_obj.id]
            if gbo_node_data.get('is_flow_root'): gbo_node_data['Q_flow'] = 0.0
            else: gbo_node_data['Q_flow'] = term_gbo_obj.flow
            gbo_node_data['radius'] = term_gbo_obj.radius
            
            if term_gbo_obj.current_territory_voxel_indices_flat:
                valid_indices_centroid = [idx for idx in term_gbo_obj.current_territory_voxel_indices_flat 
                                          if idx < tissue_data['world_coords_flat'].shape[0]]
                if valid_indices_centroid:
                    initial_coords = tissue_data['world_coords_flat'][valid_indices_centroid]
                    if initial_coords.shape[0] > 0:
                        new_pos_centroid = np.mean(initial_coords, axis=0)
                        if utils.distance_squared(term_gbo_obj.pos, new_pos_centroid) > constants.EPSILON**2 : # Only move if significant
                            old_pos_log = term_gbo_obj.pos.copy()
                            term_gbo_obj.pos = new_pos_centroid 
                            gbo_node_data['pos'] = new_pos_centroid 
                            logger.debug(f"GBO Terminal {term_gbo_obj.id} (Ω_init): Moved from {np.round(old_pos_log,3)} to centroid {np.round(new_pos_centroid,3)}.")
                            if term_gbo_obj.parent_id and gbo_graph.has_edge(term_gbo_obj.parent_id, term_gbo_obj.id):
                                parent_pos = gbo_graph.nodes[term_gbo_obj.parent_id]['pos']
                                new_len = utils.distance(parent_pos, term_gbo_obj.pos)
                                gbo_graph.edges[term_gbo_obj.parent_id, term_gbo_obj.id]['length'] = new_len
                                term_gbo_obj.length_from_parent = new_len
        else:
            logger.error(f"GBO terminal {term_gbo_obj.id} from GBOIterationData not found in gbo_graph during territory init.")

        logger.debug(f"GBO Terminal {term_gbo_obj.id} (Ω_init final): Pos={np.round(term_gbo_obj.pos,3)}, Claimed {len(voxels_for_term_init_flat)} voxels, "
                     f"Demand={term_gbo_obj.current_territory_demand:.2e}, Target Flow={term_gbo_obj.flow:.2e}, Radius={term_gbo_obj.radius:.4f}")

    logger.info(f"GBO Initialization complete. Perfused {np.sum(perfused_tissue_mask)} initial voxels within GBO domain. "
                f"{len(active_terminals)} active GBO terminals.")
    return perfused_tissue_mask, active_terminals, next_synthetic_node_id, gbo_graph


def find_growth_frontier_voxels(
    terminal_gbo_data: GBOIterationData,
    kdtree_unperfused_domain_voxels: Optional[KDTree],
    unperfused_global_flat_indices: np.ndarray,
    tissue_data: dict,
    config: dict
) -> np.ndarray:
    logger.debug(f"Terminal {terminal_gbo_data.id}: Entering find_growth_frontier_voxels. "
                 f"Pos: {np.round(terminal_gbo_data.pos,3)}, Radius: {terminal_gbo_data.radius:.4f}")

    if kdtree_unperfused_domain_voxels is None or kdtree_unperfused_domain_voxels.n == 0:
        logger.debug(f"Terminal {terminal_gbo_data.id}: KDTree of unperfused voxels is empty or None. No frontier.")
        return np.array([], dtype=int)

    radius_factor = config_manager.get_param(config, "gbo_growth.frontier_search_radius_factor", 3.0)
    fixed_radius = config_manager.get_param(config, "gbo_growth.frontier_search_radius_fixed", 0.25)
    voxel_dim = tissue_data['voxel_volume']**(1/3.0)
    search_r = max(radius_factor * terminal_gbo_data.radius, fixed_radius, voxel_dim * 1.5)

    logger.debug(f"Terminal {terminal_gbo_data.id}: Searching for frontier with effective radius {search_r:.3f}mm. "
                 f"(voxel_dim ~{voxel_dim:.3f}, term_radius_component ~{radius_factor * terminal_gbo_data.radius:.3f})")

    try:
        local_indices_in_kdtree = kdtree_unperfused_domain_voxels.query_ball_point(terminal_gbo_data.pos, r=search_r)
    except Exception as e:
        logger.error(f"Terminal {terminal_gbo_data.id}: KDTree query_ball_point failed: {e}", exc_info=True)
        return np.array([], dtype=int)

    logger.debug(f"Terminal {terminal_gbo_data.id}: KDTree query_ball_point found {len(local_indices_in_kdtree)} local indices "
                 f"within {search_r:.3f}mm.")

    if not local_indices_in_kdtree: return np.array([], dtype=int)
    if unperfused_global_flat_indices.shape[0] == 0:
        logger.warning(f"Terminal {terminal_gbo_data.id}: unperfused_global_flat_indices is empty.")
        return np.array([], dtype=int)

    valid_kdtree_indices = [idx for idx in local_indices_in_kdtree if idx < len(unperfused_global_flat_indices)]
    if len(valid_kdtree_indices) != len(local_indices_in_kdtree):
        logger.warning(f"Terminal {terminal_gbo_data.id}: Some KDTree indices were out of bounds for unperfused_global_flat_indices. "
                       f"Original: {len(local_indices_in_kdtree)}, Valid: {len(valid_kdtree_indices)}")
    if not valid_kdtree_indices: return np.array([], dtype=int)

    frontier_voxels_global_flat_indices_initial = unperfused_global_flat_indices[valid_kdtree_indices]
    logger.debug(f"Terminal {terminal_gbo_data.id}: Initial frontier (after mapping) "
                 f"contains {len(frontier_voxels_global_flat_indices_initial)} voxels.")

    max_voxels_in_Rip = config_manager.get_param(config, "gbo_growth.max_voxels_for_Rip", 50)
    final_frontier_voxels_global_flat_indices = frontier_voxels_global_flat_indices_initial

    if len(frontier_voxels_global_flat_indices_initial) > max_voxels_in_Rip:
        logger.debug(f"Terminal {terminal_gbo_data.id}: Initial frontier ({len(frontier_voxels_global_flat_indices_initial)}) "
                     f"> max_voxels_for_Rip ({max_voxels_in_Rip}). Selecting closest.")
        try:
            k_val = min(max_voxels_in_Rip, kdtree_unperfused_domain_voxels.n)
            if k_val > 0 :
                _, local_indices_k_closest = kdtree_unperfused_domain_voxels.query(terminal_gbo_data.pos, k=k_val)
                if isinstance(local_indices_k_closest, (int, np.integer)):
                    local_indices_k_closest = np.array([local_indices_k_closest])

                if len(local_indices_k_closest) > 0:
                    valid_k_closest_indices = [idx for idx in local_indices_k_closest if idx < len(unperfused_global_flat_indices)]
                    if len(valid_k_closest_indices) != len(local_indices_k_closest):
                         logger.warning(f"Terminal {terminal_gbo_data.id}: Some k-closest KDTree indices out of bounds.")
                    if valid_k_closest_indices:
                        final_frontier_voxels_global_flat_indices = unperfused_global_flat_indices[valid_k_closest_indices]
                    else: final_frontier_voxels_global_flat_indices = np.array([], dtype=int)
                else: final_frontier_voxels_global_flat_indices = np.array([], dtype=int)
            else: final_frontier_voxels_global_flat_indices = np.array([], dtype=int)
        except Exception as e_kquery:
            logger.error(f"Terminal {terminal_gbo_data.id}: KDTree k-closest query failed: {e_kquery}. Using initial ball query if valid.", exc_info=True)
            if np.any(np.array(valid_kdtree_indices) >= len(unperfused_global_flat_indices)):
                 final_frontier_voxels_global_flat_indices = np.array([], dtype=int)
        logger.debug(f"Terminal {terminal_gbo_data.id}: Frontier limited to {len(final_frontier_voxels_global_flat_indices)} voxels.")

    logger.info(f"Terminal {terminal_gbo_data.id} identified {len(final_frontier_voxels_global_flat_indices)} "
                 f"final frontier voxels (Ri,p).")
    return final_frontier_voxels_global_flat_indices


# In src/vascular_growth.py

import numpy as np
import networkx as nx
import logging
import os
from scipy.spatial import KDTree
from typing import Tuple, List, Dict, Optional 

from src import utils, data_structures, constants, config_manager
from src import energy_model
from src import perfusion_solver
from src import io_utils # For saving intermediate VTPs/NIfTIs

# Assuming GBOIterationData class and initialize_perfused_territory_and_terminals,
# find_growth_frontier_voxels are defined above this function in the same file or imported.

# If GBOIterationData is not defined elsewhere in this file, it should be:
# class GBOIterationData:
#     def __init__(self, terminal_id: str, pos: np.ndarray, radius: float, flow: float,
#                  is_synthetic: bool = True, original_measured_radius: Optional[float] = None,
#                  parent_id: Optional[str] = None, parent_measured_terminal_id: Optional[str] = None):
#         self.id: str = terminal_id
#         self.pos: np.ndarray = np.array(pos, dtype=float)
#         self.radius: float = float(radius)
#         self.flow: float = float(flow)
#         self.parent_id: Optional[str] = parent_id
#         self.parent_measured_terminal_id: Optional[str] = parent_measured_terminal_id
#         self.original_measured_terminal_radius: Optional[float] = original_measured_radius
#         self.length_from_parent: float = 0.0
#         self.is_synthetic: bool = is_synthetic
#         self.stop_growth: bool = False
#         self.current_territory_voxel_indices_flat: List[int] = []
#         self.current_territory_demand: float = 0.0

# Assume initialize_perfused_territory_and_terminals and find_growth_frontier_voxels
# are correctly defined in this file (as provided in previous responses).

logger = logging.getLogger(__name__)


def grow_healthy_vasculature(config: dict,
                             tissue_data: dict,
                             initial_graph: Optional[nx.DiGraph],
                             output_dir: str) -> Optional[nx.DiGraph]:
    logger.info("Starting GBO healthy vascular growth (Tissue-Led Model v2 with Flow Solver Integration)...")

    perfused_tissue_mask, current_active_terminals, next_node_id, gbo_graph = \
        initialize_perfused_territory_and_terminals(config, initial_graph, tissue_data)

    if not current_active_terminals:
        logger.error("GBO Aborted: No active GBO terminals after initialization.")
        return gbo_graph

    # --- Load parameters used throughout the main loop ---
    max_iterations = config_manager.get_param(config, "gbo_growth.max_iterations", 100)
    min_radius = config_manager.get_param(config, "vascular_properties.min_radius", constants.MIN_VESSEL_RADIUS_MM)
    k_murray = config_manager.get_param(config, "vascular_properties.k_murray_scaling_factor", 0.5)
    murray_exp = config_manager.get_param(config, "vascular_properties.murray_law_exponent", 3.0)
    branch_radius_factor_thresh = config_manager.get_param(config, "gbo_growth.branch_radius_increase_threshold", 1.1)
    max_flow_single_term = config_manager.get_param(config, "gbo_growth.max_flow_single_terminal", 0.005)
    min_iters_no_growth_stop = config_manager.get_param(config, "gbo_growth.min_iterations_before_no_growth_stop", 10)
    min_demand_rip_bif_factor = config_manager.get_param(config, "gbo_growth.min_frontier_demand_factor_for_bifurcation", 0.3)
    default_initial_flow = config_manager.get_param(config, "vascular_properties.initial_terminal_flow", constants.INITIAL_TERMINAL_FLOW_Q)
    flow_solver_interval = config_manager.get_param(config, "gbo_growth.flow_solver_interval", 1)
    max_segment_length_gbo = config_manager.get_param(config, "vascular_properties.max_segment_length", 2.0)


    total_voxels_in_domain = 0
    # Use healthy domain_mask for GBO total voxels count
    if tissue_data.get('domain_mask') is not None and np.any(tissue_data['domain_mask']):
        total_voxels_in_domain = np.sum(tissue_data['domain_mask'])
    
    if total_voxels_in_domain == 0:
        logger.warning("Healthy GBO: Domain mask for healthy tissue is empty. Growth might be limited or rely on fallback.")

    map_3d_to_flat_idx = -np.ones(tissue_data['shape'], dtype=np.int64)
    if tissue_data.get('voxel_indices_flat') is not None and tissue_data['voxel_indices_flat'].size > 0:
        valid_indices = tissue_data['voxel_indices_flat']
        valid_mask = (valid_indices[:,0] < tissue_data['shape'][0]) & \
                     (valid_indices[:,1] < tissue_data['shape'][1]) & \
                     (valid_indices[:,2] < tissue_data['shape'][2]) & \
                     (valid_indices[:,0] >= 0) & \
                     (valid_indices[:,1] >= 0) & \
                     (valid_indices[:,2] >= 0)
        valid_indices = valid_indices[valid_mask]
        if valid_indices.size > 0: # Check after filtering
            map_3d_to_flat_idx[valid_indices[:,0], valid_indices[:,1], valid_indices[:,2]] = np.arange(valid_indices.shape[0])

    # --- Main Iteration Loop ---
    for iteration in range(max_iterations):
        logger.info(f"--- GBO Iteration {iteration + 1} / {max_iterations} ---")

        terminals_for_growth_attempt = [t for t in current_active_terminals if not t.stop_growth]
        if not terminals_for_growth_attempt:
            logger.info("GBO: No active GBO terminals for growth at start of iteration.")
            break

        current_perfused_count = np.sum(perfused_tissue_mask)
        if total_voxels_in_domain > 0:
            perf_percentage = (current_perfused_count / total_voxels_in_domain) * 100
            logger.info(f"Active GBO terminals: {len(terminals_for_growth_attempt)}. Perfused voxels: {current_perfused_count}/{total_voxels_in_domain} ({perf_percentage:.1f}%)")
        else:
            logger.info(f"Active GBO terminals: {len(terminals_for_growth_attempt)}. Perfused voxels: {current_perfused_count} (Total domain voxels is 0).")

        # --- Build KDTree of Unperfused Voxels (within healthy GBO domain) ---
        unperfused_mask_3d = tissue_data.get('domain_mask', np.zeros(tissue_data['shape'], dtype=bool)) & (~perfused_tissue_mask)
        unperfused_voxels_3d_indices = np.array(np.where(unperfused_mask_3d)).T
        kdtree_unperfused: Optional[KDTree] = None
        # unperfused_kdtree_global_flat_indices maps indices from kdtree_unperfused back to global flat indices
        unperfused_kdtree_global_flat_indices: np.ndarray = np.array([], dtype=int)

        if unperfused_voxels_3d_indices.shape[0] > 0:
            unperfused_voxels_world_coords_for_kdt_build = utils.voxel_to_world(unperfused_voxels_3d_indices, tissue_data['affine'])
            
            # Get the global flat indices corresponding to these unperfused 3D voxel indices
            # These are indices into tissue_data['voxel_indices_flat'] / tissue_data['world_coords_flat']
            temp_flat_indices = map_3d_to_flat_idx[unperfused_voxels_3d_indices[:,0],
                                                   unperfused_voxels_3d_indices[:,1],
                                                   unperfused_voxels_3d_indices[:,2]]
            valid_for_kdtree_mask = (temp_flat_indices != -1)
            
            if np.any(valid_for_kdtree_mask):
                unperfused_kdtree_global_flat_indices = temp_flat_indices[valid_for_kdtree_mask]
                unperfused_voxels_world_coords_for_kdt_build = unperfused_voxels_world_coords_for_kdt_build[valid_for_kdtree_mask]
                if unperfused_voxels_world_coords_for_kdt_build.shape[0] > 0:
                    kdtree_unperfused = KDTree(unperfused_voxels_world_coords_for_kdt_build)
            else:
                logger.debug("No unperfused voxels mapped to valid flat indices for KDTree.")
        
        if kdtree_unperfused is None or kdtree_unperfused.n == 0:
            logger.info("GBO: No unperfused domain voxels for KDTree this iteration.")

        next_iter_terminals_manager: List[GBOIterationData] = []
        newly_perfused_in_iter_mask = np.zeros_like(perfused_tissue_mask)

        # --- Growth/Branching/Extension Phase for each GBO terminal ---
        for term_p_gbo_data in terminals_for_growth_attempt:
            logger.debug(f"Processing GBO terminal {term_p_gbo_data.id}. Pos: {np.round(term_p_gbo_data.pos,3)}, "
                         f"R: {term_p_gbo_data.radius:.4f}, Current Q_target: {term_p_gbo_data.flow:.2e}")

            # find_growth_frontier_voxels returns indices LOCAL to the kdtree_unperfused
            local_indices_in_kdt = find_growth_frontier_voxels(
                term_p_gbo_data, kdtree_unperfused, 
                np.arange(kdtree_unperfused.n if kdtree_unperfused else 0), # Pass local indices 0..N-1
                tissue_data, config
            )

            if kdtree_unperfused is None or kdtree_unperfused.n == 0 or local_indices_in_kdt.size == 0:
                logger.debug(f"Terminal {term_p_gbo_data.id} found no frontier voxels.")
                next_iter_terminals_manager.append(term_p_gbo_data)
                continue
            
            # Map these local KDTree indices back to the global flat indices
            current_frontier_global_flat_indices = unperfused_kdtree_global_flat_indices[local_indices_in_kdt]
            unique_frontier_global_flat_indices = np.unique(current_frontier_global_flat_indices) # Ensure unique

            if unique_frontier_global_flat_indices.size == 0:
                next_iter_terminals_manager.append(term_p_gbo_data)
                continue

            demand_map_3d_indices_frontier = tissue_data['voxel_indices_flat'][unique_frontier_global_flat_indices]
            demand_of_frontier_voxels = tissue_data['metabolic_demand_map'][
                demand_map_3d_indices_frontier[:,0], demand_map_3d_indices_frontier[:,1],
                demand_map_3d_indices_frontier[:,2]] # Demand map already has * dV
            demand_Rip = np.sum(demand_of_frontier_voxels)

            if demand_Rip < constants.EPSILON:
                next_iter_terminals_manager.append(term_p_gbo_data)
                continue
            
            potential_total_flow_if_extended = term_p_gbo_data.flow + demand_Rip
            potential_radius_if_extended = k_murray * (potential_total_flow_if_extended ** (1.0 / murray_exp))
            attempt_branching = False
            if term_p_gbo_data.radius > constants.EPSILON and \
               potential_radius_if_extended > term_p_gbo_data.radius * branch_radius_factor_thresh : attempt_branching = True
            if potential_total_flow_if_extended > max_flow_single_term: attempt_branching = True
            if demand_Rip > term_p_gbo_data.flow * min_demand_rip_bif_factor and term_p_gbo_data.flow > constants.EPSILON: attempt_branching = True

            if attempt_branching and unique_frontier_global_flat_indices.size > 0:
                logger.debug(f"Terminal {term_p_gbo_data.id} evaluating branching. New frontier demand: {demand_Rip:.2e}.")
                old_territory_indices_flat = np.array(term_p_gbo_data.current_territory_voxel_indices_flat, dtype=int)
                
                combined_territory_indices_flat = np.unique(np.concatenate((old_territory_indices_flat, unique_frontier_global_flat_indices))) \
                                             if old_territory_indices_flat.size > 0 else unique_frontier_global_flat_indices

                if len(combined_territory_indices_flat) < 2:
                    logger.debug(f"Combined territory for {term_p_gbo_data.id} too small. Attempting extension.")
                    attempt_branching = False
                else:
                    bifurcation_result = energy_model.find_optimal_bifurcation_for_combined_territory(
                        term_p_gbo_data, combined_territory_indices_flat, tissue_data, config, k_murray, murray_exp
                    )
                    if bifurcation_result:
                        c1_pos, c1_rad, c1_total_flow, c2_pos, c2_rad, c2_total_flow, _ = bifurcation_result
                        new_parent_total_flow = c1_total_flow + c2_total_flow
                        new_parent_radius = max(min_radius, k_murray * (new_parent_total_flow ** (1.0 / murray_exp)))
                        
                        parent_node_gbo_graph_data = gbo_graph.nodes[term_p_gbo_data.id]
                        parent_is_flow_root = parent_node_gbo_graph_data.get('is_flow_root', False)
                        parent_node_gbo_graph_data.update(
                            type='synthetic_bifurcation', radius=new_parent_radius,
                            Q_flow=new_parent_total_flow if not parent_is_flow_root else 0.0,
                            is_flow_root=parent_is_flow_root
                        )
                        logger.debug(f"Bifurcating node {term_p_gbo_data.id}: is_flow_root remains {parent_is_flow_root}")

                        for _, (child_pos, child_rad, child_flow) in enumerate([(c1_pos, c1_rad, c1_total_flow), (c2_pos, c2_rad, c2_total_flow)]):
                            child_id = f"s_{next_node_id}"; next_node_id += 1
                            child_gbo_obj = GBOIterationData(
                                child_id, child_pos, child_rad, child_flow,
                                original_measured_radius=term_p_gbo_data.original_measured_terminal_radius,
                                parent_id=term_p_gbo_data.id, parent_measured_terminal_id=term_p_gbo_data.parent_measured_terminal_id
                            )
                            child_gbo_obj.length_from_parent = utils.distance(parent_node_gbo_graph_data['pos'], child_pos)
                            next_iter_terminals_manager.append(child_gbo_obj)

                            child_attrs = vars(child_gbo_obj).copy()
                            child_attrs['type'] = 'synthetic_terminal'; child_attrs['is_flow_root'] = False
                            child_attrs['Q_flow'] = child_gbo_obj.flow
                            child_attrs.pop('current_territory_voxel_indices_flat', None); child_attrs.pop('current_territory_demand', None)
                            data_structures.add_node_to_graph(gbo_graph, child_id, **child_attrs)
                            data_structures.add_edge_to_graph(gbo_graph, term_p_gbo_data.id, child_id,
                                                              length=child_gbo_obj.length_from_parent, radius=new_parent_radius,
                                                              type='synthetic_segment')
                        
                        for v_idx_flat in unique_frontier_global_flat_indices:
                            v_3d_idx = tuple(tissue_data['voxel_indices_flat'][v_idx_flat])
                            newly_perfused_in_iter_mask[v_3d_idx] = True
                        # Parent GBOIterationData is done, children are in next_iter_terminals_manager
                        term_p_gbo_data.stop_growth = True # Mark parent GBO object as stopped
                        logger.info(f"Terminal {term_p_gbo_data.id} branched. Children target flows: {c1_total_flow:.2e}, {c2_total_flow:.2e}.")
                    else: attempt_branching = False 
            
            if not attempt_branching: # Extend
                old_pos_ext = term_p_gbo_data.pos.copy()
                logger.debug(f"Terminal {term_p_gbo_data.id} extending for Ri,p (demand {demand_Rip:.2e}).")
                term_p_gbo_data.flow += demand_Rip 
                term_p_gbo_data.radius = max(min_radius, k_murray * (term_p_gbo_data.flow ** (1.0 / murray_exp)))
                
                if unique_frontier_global_flat_indices.size > 0:
                    current_territory_coords_list = []
                    if term_p_gbo_data.current_territory_voxel_indices_flat:
                         valid_curr_idx_ext = [idx for idx in term_p_gbo_data.current_territory_voxel_indices_flat if idx < tissue_data['world_coords_flat'].shape[0]]
                         if valid_curr_idx_ext: current_territory_coords_list.append(tissue_data['world_coords_flat'][valid_curr_idx_ext])
                    
                    newly_acquired_coords = tissue_data['world_coords_flat'][unique_frontier_global_flat_indices]
                    current_territory_coords_list.append(newly_acquired_coords)
                    
                    all_supplied_coords = np.vstack(current_territory_coords_list)
                    if all_supplied_coords.shape[0] > 0:
                        new_target_pos = np.mean(all_supplied_coords, axis=0)
                        extension_vector = new_target_pos - old_pos_ext
                        extension_length = np.linalg.norm(extension_vector)

                        if extension_length > constants.EPSILON:
                            move_dist = min(extension_length, max_segment_length_gbo)
                            term_p_gbo_data.pos = old_pos_ext + extension_vector * (move_dist / extension_length)
                            if term_p_gbo_data.parent_id and gbo_graph.has_edge(term_p_gbo_data.parent_id, term_p_gbo_data.id):
                                parent_pos = gbo_graph.nodes[term_p_gbo_data.parent_id]['pos']
                                new_len = utils.distance(parent_pos, term_p_gbo_data.pos)
                                gbo_graph.edges[term_p_gbo_data.parent_id, term_p_gbo_data.id]['length'] = new_len
                                if gbo_graph.has_node(term_p_gbo_data.parent_id):
                                    gbo_graph.edges[term_p_gbo_data.parent_id, term_p_gbo_data.id]['radius'] = gbo_graph.nodes[term_p_gbo_data.parent_id]['radius']
                                term_p_gbo_data.length_from_parent = new_len
                
                if gbo_graph.has_node(term_p_gbo_data.id):
                    gbo_graph.nodes[term_p_gbo_data.id].update(pos=term_p_gbo_data.pos, 
                                                               Q_flow=term_p_gbo_data.flow if not gbo_graph.nodes[term_p_gbo_data.id].get('is_flow_root') else 0.0, 
                                                               radius=term_p_gbo_data.radius)
                
                for v_idx_flat in unique_frontier_global_flat_indices:
                    v_3d_idx = tuple(tissue_data['voxel_indices_flat'][v_idx_flat])
                    newly_perfused_in_iter_mask[v_3d_idx] = True
                term_p_gbo_data.current_territory_voxel_indices_flat.extend(list(unique_frontier_global_flat_indices))
                next_iter_terminals_manager.append(term_p_gbo_data)
        # --- End of loop for terminals_for_growth_attempt ---

        current_active_terminals = next_iter_terminals_manager # Update active terminals list
        perfused_tissue_mask = perfused_tissue_mask | newly_perfused_in_iter_mask
        num_newly_perfused_this_iter = np.sum(newly_perfused_in_iter_mask)
        logger.info(f"Perfused {num_newly_perfused_this_iter} new voxels in this GBO iteration's growth/branching phase.")
        
        # --- Global Adaptation Phase ---
        if current_active_terminals and np.any(perfused_tissue_mask):
            live_terminals_for_adaptation = [t for t in current_active_terminals if not t.stop_growth]
            if live_terminals_for_adaptation:
                # 1. Voronoi Refinement
                perfused_3d_indices_vor = np.array(np.where(perfused_tissue_mask)).T
                if perfused_3d_indices_vor.shape[0] > 0:
                    perfused_global_flat_indices_vor = map_3d_to_flat_idx[perfused_3d_indices_vor[:,0], perfused_3d_indices_vor[:,1], perfused_3d_indices_vor[:,2]]
                    valid_flat_mask_for_perf_vor = perfused_global_flat_indices_vor != -1
                    perfused_global_flat_indices_vor = perfused_global_flat_indices_vor[valid_flat_mask_for_perf_vor]
                    
                    if perfused_global_flat_indices_vor.size > 0 :
                        perfused_world_coords_for_voronoi = tissue_data['world_coords_flat'][perfused_global_flat_indices_vor]
                        term_positions_vor = np.array([t.pos for t in live_terminals_for_adaptation])
                        term_flows_capacity_vor = np.array([t.radius**murray_exp if t.radius > constants.EPSILON else default_initial_flow for t in live_terminals_for_adaptation]) # Weight by capacity (r^gamma)
                        
                        assigned_local_term_indices = np.full(perfused_world_coords_for_voronoi.shape[0], -1, dtype=int)
                        for i_pvox, p_vox_wc in enumerate(perfused_world_coords_for_voronoi):
                            distances_sq = np.sum((term_positions_vor - p_vox_wc)**2, axis=1)
                            weighted_distances = distances_sq / (term_flows_capacity_vor + constants.EPSILON) 
                            assigned_local_term_indices[i_pvox] = np.argmin(weighted_distances)
                        
                        for t_data_vor in live_terminals_for_adaptation: t_data_vor.current_territory_voxel_indices_flat, t_data_vor.current_territory_demand = [], 0.0
                        for i_pvox, local_term_idx in enumerate(assigned_local_term_indices):
                            if local_term_idx != -1 and local_term_idx < len(live_terminals_for_adaptation):
                                term_obj_vor = live_terminals_for_adaptation[local_term_idx]
                                global_flat_v_idx_vor = perfused_global_flat_indices_vor[i_pvox]
                                term_obj_vor.current_territory_voxel_indices_flat.append(global_flat_v_idx_vor)
                                v_3d_idx_for_demand_vor = tuple(tissue_data['voxel_indices_flat'][global_flat_v_idx_vor])
                                term_obj_vor.current_territory_demand += tissue_data['metabolic_demand_map'][v_3d_idx_for_demand_vor]
                        
                        for t_data_vor in live_terminals_for_adaptation: 
                            t_data_vor.flow = t_data_vor.current_territory_demand if t_data_vor.current_territory_demand > constants.EPSILON else default_initial_flow
                            if not ((iteration + 1) % flow_solver_interval == 0 or iteration == max_iterations - 1):
                                new_r_vor = k_murray * (t_data_vor.flow ** (1.0 / murray_exp))
                                t_data_vor.radius = max(min_radius, new_r_vor)
                                if gbo_graph.has_node(t_data_vor.id): 
                                    node_to_update = gbo_graph.nodes[t_data_vor.id]
                                    node_to_update['radius'] = t_data_vor.radius
                                    if not node_to_update.get('is_flow_root'):
                                        node_to_update['Q_flow'] = t_data_vor.flow
                            logger.debug(f"Term {t_data_vor.id} (Voronoi Refined): Target Q_demand={t_data_vor.flow:.2e}, Current R_after_voronoi_adapt={t_data_vor.radius:.4f}")
                    logger.info("Completed Voronoi refinement for active GBO terminals.")

                # 2. Solve Network Flow & Global Radius Adaptation
                run_flow_solver_this_iteration = ((iteration + 1) % flow_solver_interval == 0) or \
                                                 (iteration == max_iterations - 1 and iteration > 0)

                if run_flow_solver_this_iteration and gbo_graph.number_of_nodes() > 0 :
                    logger.info(f"Running 1D network flow solver for GBO iteration {iteration + 1}...")
                    for term_obj_flow_set in live_terminals_for_adaptation: # Update demands for sinks
                        if gbo_graph.has_node(term_obj_flow_set.id):
                            node_data_fs = gbo_graph.nodes[term_obj_flow_set.id]
                            if not node_data_fs.get('is_flow_root', False):
                                node_data_fs['Q_flow'] = term_obj_flow_set.flow
                            else: # Roots are pressure BCs, their Q_flow for B_vector is 0
                                node_data_fs['Q_flow'] = 0.0
                    
                    temp_graph_for_solver = gbo_graph.copy()
                    gbo_graph_with_flow = perfusion_solver.solve_1d_poiseuille_flow(temp_graph_for_solver, config, None, None) # Use Q_flow on nodes
                    
                    if gbo_graph_with_flow:
                        gbo_graph = gbo_graph_with_flow
                        logger.info("Flow solution obtained. Starting global radius adaptation for GBO tree...")
                        # Adapt radii of synthetic GBO nodes (and potentially measured junctions if desired)
                        nodes_to_adapt_gbo = [n for n, data_gbo_adapt in gbo_graph.nodes(data=True)
                                              if data_gbo_adapt.get('type','').startswith('synthetic_') or \
                                                 data_gbo_adapt.get('type') == 'measured_to_synthetic_junction'] # Adapt junctions too

                        for node_id_adapt in nodes_to_adapt_gbo:
                            node_data_adapt = gbo_graph.nodes[node_id_adapt]
                            actual_node_flow = 0.0
                            original_radius_before_adapt = node_data_adapt.get('radius', min_radius)
                            node_type_for_adapt = node_data_adapt.get('type')

                            is_sink_node = gbo_graph.out_degree(node_id_adapt) == 0 and gbo_graph.in_degree(node_id_adapt) > 0
                            is_source_like_node = gbo_graph.out_degree(node_id_adapt) > 0 # Roots, Bifs

                            if is_sink_node: # e.g. synthetic_terminal
                                for _, _, edge_data_in in gbo_graph.in_edges(node_id_adapt, data=True):
                                    solved_in_flow = edge_data_in.get('flow_solver')
                                    if solved_in_flow is not None and np.isfinite(solved_in_flow):
                                        actual_node_flow += abs(solved_in_flow) # Sum of magnitudes of incoming flows
                            elif is_source_like_node: # e.g. synthetic_bifurcation, synthetic_root_terminal, measured_to_synthetic_junction
                                for _, _, edge_data_out in gbo_graph.out_edges(node_id_adapt, data=True):
                                    solved_out_flow = edge_data_out.get('flow_solver')
                                    if solved_out_flow is not None and np.isfinite(solved_out_flow):
                                        actual_node_flow += abs(solved_out_flow) # Sum of magnitudes of outgoing flows
                            
                            if node_data_adapt.get('is_flow_root'): # Don't adapt radius of fixed pressure roots based on this flow sum
                                logger.debug(f"GlobalAdapt GBO: Node {node_id_adapt} is flow_root. Actual flow sum (for info): {actual_node_flow:.2e}. Radius not adapted by flow.")
                                continue # Skip radius adaptation for roots based on summed child flows

                            if abs(actual_node_flow) > constants.EPSILON:
                                new_radius_adapted = k_murray * (abs(actual_node_flow) ** (1.0 / murray_exp))
                                new_radius_adapted = max(min_radius, new_radius_adapted)
                                if not np.isclose(original_radius_before_adapt, new_radius_adapted, rtol=1e-2, atol=1e-5):
                                    logger.info(f"GlobalAdapt GBO: Node {node_id_adapt} ({node_type_for_adapt}) R: {original_radius_before_adapt:.4f} -> {new_radius_adapted:.4f} (Q_actual_sum={actual_node_flow:.2e})")
                                node_data_adapt['radius'] = new_radius_adapted
                            elif original_radius_before_adapt > min_radius + constants.EPSILON :
                                logger.info(f"GlobalAdapt GBO: Node {node_id_adapt} ({node_type_for_adapt}) Q_actual_sum near zero. Shrinking R from {original_radius_before_adapt:.4f} to {min_radius:.4f}.")
                                node_data_adapt['radius'] = min_radius
                        
                        # Sync radii from gbo_graph back to GBOIterationData objects
                        for term_obj_sync in live_terminals_for_adaptation: # These are GBOIterationData
                            if gbo_graph.has_node(term_obj_sync.id): # s_XXX nodes
                                term_obj_sync.radius = gbo_graph.nodes[term_obj_sync.id]['radius']
                                # Also update GBOIterationData flow if it was derived from solved edge flows (e.g. for terminals)
                                # For now, term_obj_sync.flow is primarily driven by Voronoi demand.
                        logger.info("Global radius adaptation for GBO tree complete.")
                    else: logger.error("Flow solver did not return a graph. Skipping GBO radius adaptation.")
                else: logger.info(f"Skipping GBO flow solver and global radius adaptation for iteration {iteration + 1}.")

        # --- Update Stop Flags (includes the new criterion) ---
        logger.info("Updating stop flags for GBO terminals...")
        active_terminals_still_growing = 0
        for term_data_stop_check in current_active_terminals:
            if term_data_stop_check.stop_growth:
                if gbo_graph.has_node(term_data_stop_check.id): gbo_graph.nodes[term_data_stop_check.id]['stop_growth'] = True
                continue

            if term_data_stop_check.radius < (min_radius + constants.EPSILON) : 
                term_data_stop_check.stop_growth = True
                logger.info(f"Terminal {term_data_stop_check.id} stopped: minRadius (R={term_data_stop_check.radius:.4f} <= {min_radius:.4f})")
            
            if not term_data_stop_check.stop_growth and term_data_stop_check.original_measured_terminal_radius is not None:
                stop_radius_factor_measured = config_manager.get_param(config, "gbo_growth.stop_criteria.radius_match_factor_measured", 0.95)
                stop_radius_factor_measured = np.clip(stop_radius_factor_measured, 0.1, 2.0)
                target_stop_radius = term_data_stop_check.original_measured_terminal_radius * stop_radius_factor_measured
                if term_data_stop_check.radius >= target_stop_radius:
                    term_data_stop_check.stop_growth = True
                    logger.info(f"Terminal {term_data_stop_check.id} (from VTP {term_data_stop_check.parent_measured_terminal_id}) "
                                f"stopped: GBO R {term_data_stop_check.radius:.4f} reached target {target_stop_radius:.4f} "
                                f"(orig_measured_R={term_data_stop_check.original_measured_terminal_radius:.4f}, factor={stop_radius_factor_measured:.2f}).")
            
            if not term_data_stop_check.stop_growth and \
               (not term_data_stop_check.current_territory_voxel_indices_flat and \
                term_data_stop_check.current_territory_demand < constants.EPSILON):
                term_data_stop_check.stop_growth = True
                logger.info(f"Terminal {term_data_stop_check.id} stopped: no territory/demand.")

            if gbo_graph.has_node(term_data_stop_check.id):
                gbo_graph.nodes[term_data_stop_check.id]['stop_growth'] = term_data_stop_check.stop_growth
            
            if not term_data_stop_check.stop_growth: 
                active_terminals_still_growing += 1
        
        logger.info(f"End of GBO iteration {iteration + 1}: {active_terminals_still_growing} GBO terminals still active.")

        # --- Intermediate Save & Global Stop Conditions ---
        stop_due_to_target_perfusion = False
        if total_voxels_in_domain > 0:
             stop_due_to_target_perfusion = (np.sum(perfused_tissue_mask) >= total_voxels_in_domain * 
                                        config_manager.get_param(config, "gbo_growth.target_domain_perfusion_fraction", 0.99))
        stop_due_to_no_active_terminals = (active_terminals_still_growing == 0 and iteration > 0)
        stop_due_to_no_new_growth = (num_newly_perfused_this_iter == 0 and iteration >= min_iters_no_growth_stop)

        if config_manager.get_param(config, "visualization.save_intermediate_steps", False):
            interval = config_manager.get_param(config, "visualization.intermediate_step_interval", 1)
            if ((iteration + 1) % interval == 0) or (iteration == max_iterations - 1) or \
               stop_due_to_no_new_growth or stop_due_to_target_perfusion or stop_due_to_no_active_terminals:
                logger.info(f"Saving intermediate GBO results for iteration {iteration + 1}...")
                io_utils.save_vascular_tree_vtp(gbo_graph, os.path.join(output_dir, f"gbo_graph_iter_{iteration+1}.vtp"))
                if np.any(perfused_tissue_mask):
                    io_utils.save_nifti_image(perfused_tissue_mask.astype(np.uint8), tissue_data['affine'],
                                              os.path.join(output_dir, f"gbo_perfused_mask_iter_{iteration+1}.nii.gz"))

        if stop_due_to_target_perfusion: logger.info(f"GBO Stopping after iteration {iteration + 1}: Target perfusion reached."); break
        if stop_due_to_no_active_terminals: logger.info(f"GBO Stopping after iteration {iteration + 1}: No active GBO terminals."); break
        if stop_due_to_no_new_growth: logger.info(f"GBO Stopping after iteration {iteration + 1}: No new growth for specified iters."); break
            
    logger.info(f"GBO healthy vascular growth finished. Final GBO tree component(s) added to graph. Total graph: {gbo_graph.number_of_nodes()} nodes, {gbo_graph.number_of_edges()} edges.")
    return gbo_graph# src/constants.py
import numpy as np

# Physical Constants
PI = np.pi
# Blood viscosity (e.g., in Pa.s). Typical human blood viscosity is 3-4 cP (0.003-0.004 Pa.s).
# Ensure consistency with pressure (Pa) and length (m or mm) units.
# If length in mm, pressure in Pa, Q in mm^3/s, then viscosity in Pa.s.
# Example: 0.0035 Pa.s
DEFAULT_BLOOD_VISCOSITY = 3.5e-3  # Pa.s

# Default metabolic rates
# These are example values. Units should be carefully considered for flow calculation.
# Let's assume units of: ml_blood / s / ml_tissue (volume flow rate per unit volume of tissue)
# Conversion from common literature values (e.g., ml_blood / min / 100g_tissue):
# Assume tissue density ~ 1 g/ml. So 100g_tissue ~ 100ml_tissue.
# Example: GM 60 ml/min/100g = 60 ml/min/100ml = 1 ml/min/ml_tissue = (1/60) ml/s/ml_tissue ~= 0.0167 s^-1
Q_MET_GM_PER_ML = 0.0167  # 1/s (equivalent to ml_blood / s / ml_tissue)
Q_MET_WM_PER_ML = 0.0056  # 1/s (approx 20-25 ml/min/100g)
Q_MET_TUMOR_RIM_PER_ML = 0.0334 # 1/s (e.g., 2x GM)
Q_MET_TUMOR_CORE_PER_ML = 0.0083 # 1/s (can be lower due to necrosis)
Q_MET_CSF_PER_ML = 0.0 # No metabolic demand for CSF

# GBO Parameters
MURRAY_LAW_EXPONENT = 3.0 # For r_parent^gamma = sum(r_child_i^gamma)
# C_met for metabolic maintenance cost: E_metabolic = C_met * PI * r^2 * L
# Units must be consistent with E_flow. E_flow is in Joules if Q is m^3/s, P in Pa, L in m, r in m.
# E_flow = (Pressure_drop * Q) * time_implicit = ( (8 * mu * L * Q) / (PI * r^4) ) * Q
# So E_flow has units of Power (Energy/time). For the sum, it's more like total power dissipation.
# E_metabolic should also be in units of Power.
# C_met * PI * r^2 * L => C_met units = Power / Length^3 = Power / Volume
# (e.g., W/m^3). This represents volumetric metabolic power density of vessel wall.
DEFAULT_C_MET_VESSEL_WALL = 1.0e5 # W/m^3 (Placeholder value, needs calibration from literature)

# Initial small flow for new terminals to seed Voronoi calculation (e.g., mm^3/s or m^3/s)
# Must be > 0. Let's use mm^3/s if coordinates are in mm.
INITIAL_TERMINAL_FLOW_Q = 1e-6 # mm^3/s (if using mm)

# Perfusion Model Parameters
# Tissue permeability K (e.g., in mm^2 or m^2). This is for Darcy's Law: v = -(K/mu) * grad(P)
# For brain tissue, K is very low. Values can range from 10^-12 to 10^-18 m^2.
# Let's use mm, so if K is in mm^2:
DEFAULT_TISSUE_PERMEABILITY_GM = 1e-7 # mm^2 (Placeholder, highly dependent on tissue type)
DEFAULT_TISSUE_PERMEABILITY_WM = 5e-8  # mm^2 (Placeholder)
# Coupling coefficient beta for Q_terminal = beta * (P_vessel_terminal - P_tissue_at_terminal)
# Units: (mm^3/s) / Pa if P is in Pa and Q in mm^3/s.
DEFAULT_COUPLING_BETA = 1e-7 # mm^3 / (s * Pa) (Placeholder)

# Simulation control
DEFAULT_VOXEL_SIZE_MM = 1.0 # Default isotropic voxel size in mm, if not from NIfTI

# Small epsilon for numerical stability
EPSILON = 1e-9


MIN_VESSEL_RADIUS_MM = 0.005 # example, 5 microns in mm
INITIAL_TERMINAL_FLOW_Q = 1e-6 # mm^3/s# src/energy_model.py
import numpy as np
import networkx as nx # Not directly used in functions yet, but good for context if needed later
import logging
from typing import Tuple, List, Optional # For type hinting

# Attempt to import sklearn for KMeans, but make it optional
try:
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    KMeans = None # Placeholder if not available

from src import constants, config_manager, utils

logger = logging.getLogger(__name__)

def calculate_segment_flow_energy(length: float, radius: float, flow: float, viscosity: float) -> float:
    """ 
    Calculates viscous energy dissipation (power) for a vessel segment.
    E_flow = (8 * mu * L * Q^2) / (pi * r^4) 
    Units: If mu (Pa.s), L (m), Q (m^3/s), r (m), then E_flow is in Watts (J/s).
           If mu (Pa.s), L (mm), Q (mm^3/s), r (mm):
           Pa.s * mm * (mm^3/s)^2 / mm^4 = (N/m^2).s * (1e-3 m) * (1e-9 m^3/s)^2 / (1e-12 m^4)
                                       = N.s/m^2 * 1e-3 m * 1e-18 m^6/s^2 / 1e-12 m^4
                                       = 1e-9 N.m/s = 1e-9 W.
           To get milliWatts (mW), multiply by 1000: 1e-6 mW.
           Users must ensure consistency or apply conversion factors.
    """
    if radius < constants.EPSILON: # Avoid division by zero
        # If there's flow through a zero-radius vessel, cost is infinite
        return np.inf if abs(flow) > constants.EPSILON else 0.0
    if abs(flow) < constants.EPSILON: # If flow is effectively zero, energy dissipation is zero
        return 0.0
    
    # For extremely small radii with non-zero flow, energy can become excessively large.
    # This is physically plausible (high resistance) but can cause numerical issues.
    if radius < 1e-7 and abs(flow) > constants.EPSILON: # e.g., radius < 0.1 micron
        logger.debug(f"Very small radius ({radius:.2e}) with non-zero flow ({flow:.2e}). Flow energy may be extreme.")

    return (8.0 * viscosity * length * (flow**2)) / (constants.PI * (radius**4))

def calculate_segment_metabolic_energy(length: float, radius: float, c_met_coeff: float) -> float:
    """ 
    Calculates metabolic maintenance cost (power) for a vessel segment.
    E_metabolic = C_met_coeff * pi * r^2 * L 
    Units: If C_met_coeff (W/m^3), r (m), L (m), then E_metabolic is in Watts.
           If C_met_coeff (mW/mm^3), r (mm), L (mm), then E_metabolic is in milliWatts.
           The config value for c_met_coeff should be in units consistent with E_flow.
    """
    if radius < constants.EPSILON or length < constants.EPSILON:
        return 0.0
    return c_met_coeff * constants.PI * (radius**2) * length

def calculate_bifurcation_loss(
    parent_pos: np.ndarray, # Position of the bifurcation point itself
    child1_pos: np.ndarray, child1_radius: float, child1_flow: float,
    child2_pos: np.ndarray, child2_radius: float, child2_flow: float,
    config: dict
) -> float:
    """
    Calculates the total loss (E_flow + E_metabolic) for the two new child segments
    originating from parent_pos. The flows are specific to these new child segments.
    """
    viscosity = config_manager.get_param(config, "vascular_properties.blood_viscosity", constants.DEFAULT_BLOOD_VISCOSITY)
    c_met = config_manager.get_param(config, "gbo_growth.energy_coefficient_C_met_vessel_wall", constants.DEFAULT_C_MET_VESSEL_WALL)

    l_c1 = utils.distance(parent_pos, child1_pos)
    l_c2 = utils.distance(parent_pos, child2_pos)

    # Calculate energy for child 1 segment
    if l_c1 < constants.EPSILON or child1_radius < constants.EPSILON:
        e_flow_c1 = np.inf if abs(child1_flow) > constants.EPSILON else 0.0
        e_met_c1 = 0.0
    else:
        e_flow_c1 = calculate_segment_flow_energy(l_c1, child1_radius, child1_flow, viscosity)
        e_met_c1 = calculate_segment_metabolic_energy(l_c1, child1_radius, c_met)

    # Calculate energy for child 2 segment
    if l_c2 < constants.EPSILON or child2_radius < constants.EPSILON:
        e_flow_c2 = np.inf if abs(child2_flow) > constants.EPSILON else 0.0
        e_met_c2 = 0.0
    else:
        e_flow_c2 = calculate_segment_flow_energy(l_c2, child2_radius, child2_flow, viscosity)
        e_met_c2 = calculate_segment_metabolic_energy(l_c2, child2_radius, c_met)
    
    total_loss = e_flow_c1 + e_met_c1 + e_flow_c2 + e_met_c2
    
    # logger.debug(f"Bifurcation candidate: L1={l_c1:.2f}, R1={child1_radius:.4f}, Q1={child1_flow:.2e} -> E_f1={e_flow_c1:.2e}, E_m1={e_met_c1:.2e}")
    # logger.debug(f"                     L2={l_c2:.2f}, R2={child2_radius:.4f}, Q2={child2_flow:.2e} -> E_f2={e_flow_c2:.2e}, E_m2={e_met_c2:.2e}")
    # logger.debug(f"                     Total Loss = {total_loss:.3e}")
    return total_loss



def find_optimal_bifurcation_for_combined_territory(
    parent_terminal_gbo_data: object, # GBOIterationData for the terminal that will bifurcate
    # Combined territory: parent's old territory + new frontier region
    combined_territory_voxel_indices_flat: np.ndarray, 
    tissue_data: dict,
    config: dict,
    k_murray_factor: float,
    murray_exponent: float
) -> Optional[Tuple[np.ndarray, float, float, np.ndarray, float, float, float]]:
    """
    Searches for an optimal bifurcation for the parent_terminal to supply a 
    COMBINED territory (its old territory + a new growth region).
    The children C1 and C2 will share the total demand of this combined_territory.
    
    Args:
        parent_terminal_gbo_data: GBOIterationData of the branching terminal.
        combined_territory_voxel_indices_flat: Flat indices of ALL voxels 
                                               (old territory + new frontier) to be supplied.
        tissue_data: Full tissue data dict.
        config: Simulation config.
        k_murray_factor, murray_exponent: For radius calculation.

    Returns:
        Tuple (child1_pos, child1_radius, child1_total_flow, 
               child2_pos, child2_radius, child2_total_flow, min_loss_for_new_segments) 
        or None if no suitable bifurcation found.
        The child flows are their respective total target flows.
    """
    parent_id = parent_terminal_gbo_data.id
    parent_pos = parent_terminal_gbo_data.pos # This is the bifurcation point
    logger.debug(f"Finding optimal bifurcation for {parent_id} at {np.round(parent_pos,3)} to supply combined "
                 f"territory of {len(combined_territory_voxel_indices_flat)} voxels.")

    if len(combined_territory_voxel_indices_flat) == 0:
        logger.debug(f"Terminal {parent_id}: Combined target territory is empty. No bifurcation.")
        return None

    # Get world coordinates and demand for ALL voxels in the combined territory
    combined_voxels_world_coords = tissue_data['world_coords_flat'][combined_territory_voxel_indices_flat]
    
    demand_map_3d_indices = tissue_data['voxel_indices_flat'][combined_territory_voxel_indices_flat]
    demand_per_combined_voxel_qmet = tissue_data['metabolic_demand_map'][
        demand_map_3d_indices[:,0],
        demand_map_3d_indices[:,1],
        demand_map_3d_indices[:,2]
    ]
    demand_of_combined_voxels_flow = demand_per_combined_voxel_qmet * tissue_data['voxel_volume']
    total_demand_of_combined_territory = np.sum(demand_of_combined_voxels_flow)

    if total_demand_of_combined_territory < constants.EPSILON:
        logger.debug(f"Terminal {parent_id}: Total demand in combined territory is negligible. No bifurcation.")
        return None

    num_candidate_location_sets = config_manager.get_param(config, "gbo_growth.bifurcation_candidate_points", 10)
    min_seg_len = config_manager.get_param(config, "vascular_properties.min_segment_length", 0.1)
    min_radius = config_manager.get_param(config, "vascular_properties.min_radius", constants.MIN_VESSEL_RADIUS_MM)

    best_bifurcation_params = None
    min_loss_found = np.inf

    if len(combined_voxels_world_coords) < 2:
        logger.debug(f"Terminal {parent_id}: Combined territory too small ({len(combined_voxels_world_coords)} voxels) "
                     "for meaningful bifurcation. Consider extension.")
        return None # Bifurcation needs to split demand between two children

    # Candidate child locations should be within or near the combined_territory
    # (KMeans or random sampling on combined_voxels_world_coords)
    for i in range(num_candidate_location_sets):
        c1_pos_candidate, c2_pos_candidate = None, None
        # --- 1. Generate candidate child locations (c1_pos, c2_pos) based on combined_territory ---
        # (Using KMeans as before, but on combined_voxels_world_coords)
        if SKLEARN_AVAILABLE and KMeans is not None:
            try:
                n_clust = min(2, len(combined_voxels_world_coords))
                if n_clust < 2: # Should be caught by len(combined_voxels_world_coords) < 2 above
                    continue 
                kmeans = KMeans(n_clusters=n_clust, random_state=i, n_init='auto').fit(combined_voxels_world_coords)
                c1_pos_candidate = kmeans.cluster_centers_[0]
                c2_pos_candidate = kmeans.cluster_centers_[1]
            except Exception as e_km:
                logger.warning(f"KMeans failed for combined territory (iter {i}): {e_km}. Fallback.")
                indices = np.random.choice(len(combined_voxels_world_coords), 2, replace=False)
                c1_pos_candidate = combined_voxels_world_coords[indices[0]]
                c2_pos_candidate = combined_voxels_world_coords[indices[1]]
        else:
            if i == 0 and not SKLEARN_AVAILABLE : logger.warning("Sklearn KMeans not available for child placement.")
            indices = np.random.choice(len(combined_voxels_world_coords), 2, replace=False)
            c1_pos_candidate = combined_voxels_world_coords[indices[0]]
            c2_pos_candidate = combined_voxels_world_coords[indices[1]]

        if utils.distance(parent_pos, c1_pos_candidate) < min_seg_len or \
           utils.distance(parent_pos, c2_pos_candidate) < min_seg_len or \
           utils.distance(c1_pos_candidate, c2_pos_candidate) < min_seg_len:
            continue

        # --- 2. Assign ALL voxels in combined_territory to c1 or c2 ---
        # This determines Q_C1_total and Q_C2_total for the candidate children.
        q_c1_total_candidate = 0.0
        q_c2_total_candidate = 0.0
        
        for idx_in_combined, voxel_wc in enumerate(combined_voxels_world_coords):
            dist_sq_to_c1 = utils.distance_squared(voxel_wc, c1_pos_candidate)
            dist_sq_to_c2 = utils.distance_squared(voxel_wc, c2_pos_candidate)
            if dist_sq_to_c1 <= dist_sq_to_c2:
                q_c1_total_candidate += demand_of_combined_voxels_flow[idx_in_combined]
            else:
                q_c2_total_candidate += demand_of_combined_voxels_flow[idx_in_combined]
        
        if q_c1_total_candidate < constants.EPSILON or q_c2_total_candidate < constants.EPSILON:
            # This means one child would get (almost) no flow from the entire combined territory.
            # This might be a poor bifurcation unless the other child takes nearly all.
            # Forcing both to have substantial flow might be too restrictive.
            # Let it proceed if total demand is met.
            if abs(q_c1_total_candidate + q_c2_total_candidate - total_demand_of_combined_territory) > constants.EPSILON * total_demand_of_combined_territory:
                 logger.warning(f"Demand conservation issue in child assignment: sum_child_Q={q_c1_total_candidate+q_c2_total_candidate:.2e}, total_demand={total_demand_of_combined_territory:.2e}")
                 continue # Skip if total demand not conserved by split

        # --- 3. Calculate radii for c1, c2 based on their TOTAL flows ---
        r_c1_candidate = k_murray_factor * (q_c1_total_candidate ** (1.0 / murray_exponent)) if q_c1_total_candidate > constants.EPSILON else min_radius
        r_c2_candidate = k_murray_factor * (q_c2_total_candidate ** (1.0 / murray_exponent)) if q_c2_total_candidate > constants.EPSILON else min_radius
        r_c1_candidate = max(min_radius, r_c1_candidate)
        r_c2_candidate = max(min_radius, r_c2_candidate)

        # --- 4. Calculate loss for this candidate bifurcation (for the two new child segments) ---
        # Flows used here are q_c1_total_candidate and q_c2_total_candidate
        current_loss = calculate_bifurcation_loss(
            parent_pos, # Bifurcation point
            c1_pos_candidate, r_c1_candidate, q_c1_total_candidate,
            c2_pos_candidate, r_c2_candidate, q_c2_total_candidate,
            config
        )

        if current_loss < min_loss_found:
            min_loss_found = current_loss
            best_bifurcation_params = (c1_pos_candidate.copy(), r_c1_candidate, q_c1_total_candidate,
                                       c2_pos_candidate.copy(), r_c2_candidate, q_c2_total_candidate,
                                       min_loss_found)

    if best_bifurcation_params:
        logger.info(f"Optimal bifurcation for {parent_id} (supplying combined territory) chosen (Loss {best_bifurcation_params[6]:.3e}): "
                    f"C1 (R={best_bifurcation_params[1]:.4f}, Q_total={best_bifurcation_params[2]:.2e}), "
                    f"C2 (R={best_bifurcation_params[4]:.4f}, Q_total={best_bifurcation_params[5]:.2e})")
        return best_bifurcation_params
    else:
        logger.debug(f"No suitable bifurcation found for terminal {parent_id} to supply combined territory.")
        return None

# The __main__ test block in energy_model.py would need to be updated to call this new function
# and provide mock data for a "combined_territory".


def find_optimal_bifurcation_for_new_region(
    parent_terminal_gbo_data: object, 
    new_growth_region_voxel_indices_flat: np.ndarray, 
    tissue_data: dict,
    config: dict,
    k_murray_factor: float,
    murray_exponent: float
) -> Optional[Tuple[np.ndarray, float, float, np.ndarray, float, float, float]]:
    """
    Searches for an optimal bifurcation for the parent_terminal to supply a *new growth region (Ri,p)*.
    (Full implementation as previously provided)
    """
    parent_id = parent_terminal_gbo_data.id
    parent_pos = parent_terminal_gbo_data.pos
    logger.debug(f"Finding optimal bifurcation for terminal {parent_id} at {parent_pos} to supply a new "
                 f"growth region of {len(new_growth_region_voxel_indices_flat)} voxels.")

    if len(new_growth_region_voxel_indices_flat) == 0:
        logger.debug(f"Terminal {parent_id}: New growth region is empty. No bifurcation.")
        return None

    new_growth_voxels_world_coords = tissue_data['world_coords_flat'][new_growth_region_voxel_indices_flat]
    
    demand_map_3d_indices = tissue_data['voxel_indices_flat'][new_growth_region_voxel_indices_flat]
    demand_per_new_growth_voxel = tissue_data['metabolic_demand_map'][
        demand_map_3d_indices[:,0],
        demand_map_3d_indices[:,1],
        demand_map_3d_indices[:,2]
    ] # This is q_met per voxel, not q_met * dV yet
    
    # Multiply by voxel volume to get demand (flow rate)
    demand_of_new_growth_voxels = demand_per_new_growth_voxel * tissue_data['voxel_volume']
    total_demand_of_new_region = np.sum(demand_of_new_growth_voxels)

    if total_demand_of_new_region < constants.EPSILON:
        logger.debug(f"Terminal {parent_id}: Total demand in the new growth region is negligible. No bifurcation.")
        return None

    num_candidate_location_sets = config_manager.get_param(config, "gbo_growth.bifurcation_candidate_points", 10)
    min_seg_len = config_manager.get_param(config, "vascular_properties.min_segment_length", 0.1)
    min_radius = config_manager.get_param(config, "vascular_properties.min_radius", constants.MIN_VESSEL_RADIUS_MM)

    best_bifurcation_params = None
    min_loss_found = np.inf

    if len(new_growth_voxels_world_coords) < 2 and len(new_growth_voxels_world_coords) > 0: # Handle single voxel new region
         logger.debug(f"New growth region for {parent_id} has only {len(new_growth_voxels_world_coords)} voxel(s). Treating as extension.")
         # This case should ideally be handled by "extension" logic in vascular_growth.py
         # For now, find_optimal_bifurcation will attempt to make one child supply it.
    elif len(new_growth_voxels_world_coords) == 0: # Should be caught earlier
        return None


    for i in range(num_candidate_location_sets):
        c1_pos_candidate, c2_pos_candidate = None, None
        if len(new_growth_voxels_world_coords) >= 2:
            if SKLEARN_AVAILABLE and KMeans is not None:
                try:
                    # Ensure n_clusters is not more than n_samples
                    n_clust = min(2, len(new_growth_voxels_world_coords))
                    if n_clust < 2 : # Not enough for two distinct clusters
                        idx = np.random.choice(len(new_growth_voxels_world_coords), 1)[0]
                        c1_pos_candidate = new_growth_voxels_world_coords[idx]
                        # Create a dummy c2 for calculation, it will get ~0 flow from new region
                        c2_pos_candidate = c1_pos_candidate + utils.normalize_vector(np.random.rand(3)-0.5) * min_seg_len
                    else:
                        kmeans = KMeans(n_clusters=n_clust, random_state=i, n_init='auto').fit(new_growth_voxels_world_coords)
                        c1_pos_candidate = kmeans.cluster_centers_[0]
                        c2_pos_candidate = kmeans.cluster_centers_[1] if n_clust == 2 else kmeans.cluster_centers_[0] + utils.normalize_vector(np.random.rand(3)-0.5) * min_seg_len

                except Exception as e_km: # Catch any Kmeans error
                    logger.warning(f"KMeans clustering failed for bifurcation candidates (iter {i}): {e_km}. Falling back to random points.")
                    # Fallback to random points from the new growth region
                    indices = np.random.choice(len(new_growth_voxels_world_coords), 2, replace=len(new_growth_voxels_world_coords) < 2)
                    c1_pos_candidate = new_growth_voxels_world_coords[indices[0]]
                    c2_pos_candidate = new_growth_voxels_world_coords[indices[1]]

            else: # SKLEARN_AVAILABLE is False or KMeans is None (ImportError)
                if i == 0 : logger.warning("Scikit-learn not found. Using random points for bifurcation candidates.")
                indices = np.random.choice(len(new_growth_voxels_world_coords), 2, replace=len(new_growth_voxels_world_coords) < 2)
                c1_pos_candidate = new_growth_voxels_world_coords[indices[0]]
                c2_pos_candidate = new_growth_voxels_world_coords[indices[1]]
        
        elif len(new_growth_voxels_world_coords) == 1:
            c1_pos_candidate = new_growth_voxels_world_coords[0]
            c2_pos_candidate = c1_pos_candidate + utils.normalize_vector(np.random.rand(3)-0.5) * min_seg_len # Dummy c2

        if c1_pos_candidate is None or c2_pos_candidate is None : continue # Should not happen with fallbacks

        if utils.distance(parent_pos, c1_pos_candidate) < min_seg_len or \
           utils.distance(parent_pos, c2_pos_candidate) < min_seg_len or \
           (utils.distance(c1_pos_candidate, c2_pos_candidate) < min_seg_len and not np.allclose(c1_pos_candidate, c2_pos_candidate)):
            continue

        q_c1_candidate_from_new = 0.0
        q_c2_candidate_from_new = 0.0
        
        for idx_in_new_region, voxel_wc in enumerate(new_growth_voxels_world_coords):
            dist_sq_to_c1 = utils.distance_squared(voxel_wc, c1_pos_candidate)
            dist_sq_to_c2 = utils.distance_squared(voxel_wc, c2_pos_candidate)
            if dist_sq_to_c1 <= dist_sq_to_c2:
                q_c1_candidate_from_new += demand_of_new_growth_voxels[idx_in_new_region]
            else:
                q_c2_candidate_from_new += demand_of_new_growth_voxels[idx_in_new_region]
        
        # Ensure flow is not split into practically zero for both if there's demand
        if (q_c1_candidate_from_new < constants.EPSILON and q_c2_candidate_from_new < constants.EPSILON and total_demand_of_new_region > constants.EPSILON):
            # This might happen if c1_pos and c2_pos are poorly chosen relative to demand distribution.
            # For example, if both are far from all demand points.
            # Or if total_demand_of_new_region is tiny.
            # logger.debug(f"Candidate pair {i} results in zero flow for both children from new region. Skipping.")
            continue

        r_c1_candidate = k_murray_factor * (q_c1_candidate_from_new ** (1.0 / murray_exponent)) if q_c1_candidate_from_new > constants.EPSILON else min_radius
        r_c2_candidate = k_murray_factor * (q_c2_candidate_from_new ** (1.0 / murray_exponent)) if q_c2_candidate_from_new > constants.EPSILON else min_radius
        r_c1_candidate = max(min_radius, r_c1_candidate)
        r_c2_candidate = max(min_radius, r_c2_candidate)

        current_loss = calculate_bifurcation_loss(
            parent_pos,
            c1_pos_candidate, r_c1_candidate, q_c1_candidate_from_new,
            c2_pos_candidate, r_c2_candidate, q_c2_candidate_from_new,
            config
        )

        if current_loss < min_loss_found:
            min_loss_found = current_loss
            best_bifurcation_params = (c1_pos_candidate.copy(), r_c1_candidate, q_c1_candidate_from_new,
                                       c2_pos_candidate.copy(), r_c2_candidate, q_c2_candidate_from_new,
                                       min_loss_found)

    if best_bifurcation_params:
        logger.info(f"Optimal bifurcation for {parent_id} to supply new region chosen (Loss {best_bifurcation_params[6]:.3e}): "
                    f"C1 (R={best_bifurcation_params[1]:.4f}, Q_new={best_bifurcation_params[2]:.2e}), "
                    f"C2 (R={best_bifurcation_params[4]:.4f}, Q_new={best_bifurcation_params[5]:.2e})")
        return best_bifurcation_params
    else:
        logger.debug(f"No suitable (lower loss or valid) bifurcation found for terminal {parent_id} to supply new region.")
        return None

if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG) # Changed to DEBUG for more verbose test output
    
    # Mock config
    test_config = {
        "vascular_properties": {
            "blood_viscosity": 0.0035, 
            "min_segment_length": 0.01, 
            "min_radius": 0.005, 
            "k_murray_scaling_factor": 0.5, 
            "murray_law_exponent": 3.0
        },
        "gbo_growth": {
            # C_met unit: e.g. mW/mm^3 (if E_flow target unit is mW)
            # If E_flow from calculate_segment_flow_energy is in 10^-9 W (nanoWatts) when inputs are mm, Pa.s, mm^3/s
            # And we want E_met to be in same units: C_met * mm^2 * mm = C_met * mm^3
            # So C_met should be in (10^-9 W) / mm^3.
            # If paper uses C_met for W/m^3, and we use mm:
            # C_met_paper (W/m^3) * (1m/1000mm)^3 = C_met_paper * 1e-9 (W/mm^3)
            # Let's use a value that makes E_met somewhat comparable to E_flow for typical values.
            # Example from before: L=1, R=0.1, Q=0.01 -> E_flow ~ 1.1e-7 (in 10^-9 W units, so 1.1e-16 W actual)
            # E_met = C_met * pi * (0.1)^2 * 1. If C_met = 1e-5 (in 10^-9W/mm^3 units), E_met ~ 3e-7.
            "energy_coefficient_C_met_vessel_wall": 1.0e-5, # (Units: 10^-9 W / mm^3 or equivalent)
            "bifurcation_candidate_points": 50, # Increased for better testing
        }
    }
    
    # Ensure constants are properly defined or mocked for standalone execution
    class MockConstants:
        PI = np.pi
        EPSILON = 1e-10 # Slightly smaller epsilon
        DEFAULT_BLOOD_VISCOSITY = 0.0035
        # This default C_MET_VESSEL_WALL in constants.py is likely in W/m^3.
        # The config value above is what's used by the functions.
        DEFAULT_C_MET_VESSEL_WALL = 1.0e5 
        MIN_VESSEL_RADIUS_MM = 0.005
    
    # Overwrite constants only if they are not the actual imported ones (e.g. running file directly)
    if 'constants' not in globals() or not hasattr(constants, 'MIN_VESSEL_RADIUS_MM'):
        constants = MockConstants()
        print("Using MockConstants for standalone test.")


    # Test calculate_segment_flow_energy
    # Using L (mm), R (mm), Q (mm^3/s), mu (Pa.s)
    # Expected output units: 10^-9 W (nanoWatts) or Pa.mm^3/s
    l, r_test, q_test, mu_test = 1.0, 0.1, 0.01, test_config["vascular_properties"]["blood_viscosity"]
    e_flow = calculate_segment_flow_energy(l, r_test, q_test, mu_test)
    
    # c_met_val from config is assumed to be in (10^-9 W)/mm^3 to match E_flow units
    c_met_val = test_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"] 
    e_met = calculate_segment_metabolic_energy(l, r_test, c_met_val)
    logger.info(f"Test segment: L={l}mm, R={r_test}mm, Q={q_test}mm^3/s, mu={mu_test}Pa.s")
    logger.info(f"Calculated E_flow = {e_flow:.3e} (expected units: Pa.mm^3/s or 10^-9 W)")
    logger.info(f"Calculated E_met (with C_met={c_met_val:.1e}) = {e_met:.3e} (expected units: same as E_flow)")
    # Expected E_flow = (8 * 0.0035 * 1 * 0.01^2) / (pi * 0.1^4) = (2.8e-6) / (pi * 1e-4) approx 2.8e-6 / 3.14e-4 = 0.0089 Pa.mm^3/s
    # My previous manual calc was off. Let's recheck:
    # (8 * 0.0035 * 1.0 * (0.01**2)) / (np.pi * (0.1**4)) = 0.008912676...
    # E_met = 1e-5 * np.pi * (0.1**2) * 1.0 = 3.14159e-7
    # These values seem reasonable relative to each other if C_met is chosen appropriately.

    # Mock parent terminal for find_optimal_bifurcation_for_new_region
    class MockGBOIterationData: # Simplified for this test
        def __init__(self, id, pos, radius, flow): # Removed territory_demand as it's not used by this func
            self.id = id
            self.pos = np.array(pos)
            self.radius = radius
            self.flow = flow # This is flow to *existing* territory, not used by find_optimal_bifurcation_for_new_region

    parent_term = MockGBOIterationData(
        id="p_test_0", 
        pos=np.array([0.,0.,0.]), 
        radius=0.2, 
        flow=0.00 # Flow to existing territory, not directly used for optimizing *new* region supply
    )

    # Mock tissue_data for the new growth region
    num_new_growth_voxels = 50
    # These are flat indices relative to the global tissue_data arrays
    # For the test, let's assume these are the first 'num_new_growth_voxels' in a hypothetical global list
    mock_new_growth_indices_flat = np.arange(num_new_growth_voxels) 
    
    # World coords for these specific new growth voxels
    # Place them in a cluster, e.g., around [1,1,0]
    mock_new_growth_world_coords = np.random.rand(num_new_growth_voxels, 3) * 0.5 + np.array([1.0, 1.0, 0.0])
    
    # For tissue_data, we need the *full* set of domain voxels
    # For this test, we can make tissue_data['world_coords_flat'] just be these new growth voxels
    # And correspondingly for voxel_indices_flat and metabolic_demand_map
    
    mock_tissue_voxel_indices_3d = np.zeros((num_new_growth_voxels, 3), dtype=int)
    for i in range(num_new_growth_voxels): # Dummy 3D indices
        mock_tissue_voxel_indices_3d[i] = [i // 10, i % 10, 0] 

    # Demand for these new growth voxels (q_met, not q_met * dV yet)
    mock_demand_q_met_for_new_voxels = np.random.uniform(low=0.01, high=0.02, size=num_new_growth_voxels) # 1/s (q_met)
    
    # Assume a voxel volume for calculating total demand from q_met
    mock_voxel_vol = 0.001 # mm^3 (e.g., 0.1mm x 0.1mm x 0.1mm)

    # Create the metabolic_demand_map (3D) that would contain these values
    # For simplicity, make it just large enough for our dummy 3D indices
    max_indices = np.max(mock_tissue_voxel_indices_3d, axis=0)
    mock_full_metabolic_demand_map_3d = np.zeros((max_indices[0]+1, max_indices[1]+1, max_indices[2]+1))
    mock_full_metabolic_demand_map_3d[mock_tissue_voxel_indices_3d[:,0],
                                      mock_tissue_voxel_indices_3d[:,1],
                                      mock_tissue_voxel_indices_3d[:,2]] = mock_demand_q_met_for_new_voxels
    
    mock_tissue_data = {
        'world_coords_flat': mock_new_growth_world_coords, # Only contains the new growth voxels for this test
        'voxel_indices_flat': mock_tissue_voxel_indices_3d, # Corresponding 3D indices
        'metabolic_demand_map': mock_full_metabolic_demand_map_3d, # Full 3D q_met map
        'voxel_volume': mock_voxel_vol
    }
    
    bifurcation_result = find_optimal_bifurcation_for_combined_territory(
        parent_term,
        mock_new_growth_indices_flat, # These are indices into the arrays in mock_tissue_data
        mock_tissue_data,
        test_config,
        k_murray_factor=test_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=test_config["vascular_properties"]["murray_law_exponent"]
    )

    if bifurcation_result:
        c1p, c1r, c1q_new, c2p, c2r, c2q_new, loss = bifurcation_result
        logger.info(f"Optimal Bifurcation Found for {parent_term.id} to supply new region:")
        logger.info(f"  Child 1: Pos={np.round(c1p,3)}, Radius={c1r:.4f}, Flow_new={c1q_new:.3e}")
        logger.info(f"  Child 2: Pos={np.round(c2p,3)}, Radius={c2r:.4f}, Flow_new={c2q_new:.3e}")
        logger.info(f"  Minimized Loss (for new segments): {loss:.3e}")
        # Verify total new flow captured
        total_new_demand_calc = np.sum(mock_demand_q_met_for_new_voxels * mock_voxel_vol)
        logger.info(f"  Sum of child flows from new region: {(c1q_new + c2q_new):.3e}")
        logger.info(f"  Total demand of new region: {total_new_demand_calc:.3e}")
        assert np.isclose(c1q_new + c2q_new, total_new_demand_calc), "Child flows do not sum to total new demand"
    else:
        logger.info(f"No optimal bifurcation found for {parent_term.id} in this test.")# src/visualization.py
from __future__ import annotations # Must be first line

import logging
import os
import numpy as np
import networkx as nx
from typing import Optional, Dict, List, Tuple
import pandas as pd

try:
    import pyvista as pv
    PYVISTA_AVAILABLE = True
except ImportError:
    PYVISTA_AVAILABLE = False
    pv = None

try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    plt = None

from src import io_utils, config_manager, utils, constants

logger = logging.getLogger(__name__)

# --- Plotting helper for distributions ---
def _plot_histogram(data: List[float], title: str, xlabel: str, output_path: str, bins: int = 30, density: bool = False):
    if not MATPLOTLIB_AVAILABLE:
        logger.warning(f"Matplotlib not available. Skipping histogram plot: {title}")
        return
    if not data:
        logger.warning(f"No data to plot for histogram: {title}")
        return
    valid_data = [x for x in data if np.isfinite(x)]
    if not valid_data:
        logger.warning(f"No finite data to plot for histogram (all NaN/Inf): {title}")
        return

    plt.figure(figsize=(8, 6))
    plt.hist(valid_data, bins=bins, color='skyblue', edgecolor='black', density=density)
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel("Frequency" if not density else "Density")
    plt.grid(axis='y', alpha=0.75)
    try:
        plt.savefig(output_path)
        logger.info(f"Saved histogram '{title}' to {output_path}")
    except Exception as e:
        logger.error(f"Error saving histogram '{title}' to {output_path}: {e}")
    finally:
        plt.close()

# --- Functions for Quantitative Analysis ---
def analyze_radii_distribution(graph: nx.DiGraph, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Radii analysis: Graph is empty or None.")
        return
    radii = [data['radius'] for _, data in graph.nodes(data=True)
             if 'radius' in data and data.get('is_synthetic', True) and np.isfinite(data['radius'])]
    if not radii: logger.info("No synthetic nodes with valid radii found for distribution analysis."); return
    df_radii = pd.DataFrame(radii, columns=['radius_mm'])
    csv_path = os.path.join(output_dir, f"{filename_prefix}radii_data.csv")
    df_radii.to_csv(csv_path, index=False); logger.info(f"Saved raw radii data to {csv_path}")
    plot_path = os.path.join(output_dir, f"{filename_prefix}radii_distribution.png")
    _plot_histogram(radii, "Distribution of Vessel Radii (Synthetic Nodes)", "Radius (mm)", plot_path)

def analyze_segment_lengths(graph: nx.DiGraph, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_edges() == 0:
        logger.warning("Segment length analysis: Graph is empty or has no edges.")
        return
    lengths = []
    for u, v, data in graph.edges(data=True):
        if u in graph.nodes and v in graph.nodes and 'length' in data and \
           (graph.nodes[u].get('is_synthetic', True) or graph.nodes[v].get('is_synthetic', True)) and \
           np.isfinite(data['length']):
            lengths.append(data['length'])

    if not lengths: logger.info("No synthetic segments with valid lengths found for distribution analysis."); return
    df_lengths = pd.DataFrame(lengths, columns=['length_mm'])
    csv_path = os.path.join(output_dir, f"{filename_prefix}segment_lengths_data.csv")
    df_lengths.to_csv(csv_path, index=False); logger.info(f"Saved raw segment length data to {csv_path}")
    plot_path = os.path.join(output_dir, f"{filename_prefix}segment_lengths_distribution.png")
    _plot_histogram(lengths, "Distribution of Segment Lengths", "Length (mm)", plot_path)


def analyze_bifurcation_geometry(graph: nx.DiGraph, output_dir: str, murray_exponent: float = 3.0, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Bifurcation geometry analysis: Graph is empty or None.")
        return

    murray_parent_powers: List[float] = []
    murray_children_sum_powers: List[float] = []
    area_ratios_alpha: List[float] = []
    daughter_asymmetry_ratios: List[float] = []
    branching_angles_c1_c2: List[float] = []
    bifurcation_data_for_csv: List[Dict] = []

    logger.info(f"--- Starting Bifurcation Geometry Analysis (Murray Exp: {murray_exponent}) ---")
    bifurcation_nodes_processed = 0

    for node_id, data in graph.nodes(data=True):
        node_type = data.get('type', '')
        is_potential_bifurcation = (node_type == 'synthetic_bifurcation') or \
                                   (node_type == 'synthetic_root_terminal' and data.get('is_flow_root', False))

        if is_potential_bifurcation and graph.out_degree(node_id) == 2:
            bifurcation_nodes_processed += 1
            parent_pos = data.get('pos')
            children_ids = list(graph.successors(node_id))

            r_p_node = data.get('radius')
            q_p_node = data.get('Q_flow') 

            if len(children_ids) != 2 or \
               children_ids[0] not in graph.nodes or \
               children_ids[1] not in graph.nodes:
                logger.debug(f"Bif. Geom. Test: Node {node_id} has invalid children setup. Skipping.")
                continue

            data_c1 = graph.nodes[children_ids[0]]
            data_c2 = graph.nodes[children_ids[1]]
            child1_pos = data_c1.get('pos')
            child2_pos = data_c2.get('pos')
            r_c1_node = data_c1.get('radius')
            r_c2_node = data_c2.get('radius')
            q_c1_node = data_c1.get('Q_flow')
            q_c2_node = data_c2.get('Q_flow')

            if not (r_p_node and np.isfinite(r_p_node) and r_p_node >= constants.EPSILON and
                    r_c1_node and np.isfinite(r_c1_node) and r_c1_node >= constants.EPSILON and
                    r_c2_node and np.isfinite(r_c2_node) and r_c2_node >= constants.EPSILON):
                logger.debug(f"Bif. Geom. Test: Node {node_id} or children have invalid radii. Skipping.")
                continue

            p_power = r_p_node**murray_exponent
            c_sum_power = r_c1_node**murray_exponent + r_c2_node**murray_exponent
            murray_parent_powers.append(p_power)
            murray_children_sum_powers.append(c_sum_power)

            alpha = (r_c1_node**2 + r_c2_node**2) / (r_p_node**2)
            area_ratios_alpha.append(alpha)

            asymmetry_ratio = min(r_c1_node, r_c2_node) / max(r_c1_node, r_c2_node) if max(r_c1_node, r_c2_node) > constants.EPSILON else 1.0
            daughter_asymmetry_ratios.append(asymmetry_ratio)
            
            angle_deg_val = np.nan # Initialize for CSV
            if parent_pos is not None and child1_pos is not None and child2_pos is not None:
                vec1 = child1_pos - parent_pos
                vec2 = child2_pos - parent_pos
                norm_vec1 = np.linalg.norm(vec1)
                norm_vec2 = np.linalg.norm(vec2)
                if norm_vec1 > constants.EPSILON and norm_vec2 > constants.EPSILON:
                    cosine_angle = np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)
                    angle_rad = np.arccos(np.clip(cosine_angle, -1.0, 1.0))
                    angle_deg_val = np.degrees(angle_rad)
                    if np.isfinite(angle_deg_val):
                        branching_angles_c1_c2.append(angle_deg_val)

            bifurcation_data_for_csv.append({
                'b_id': node_id,
                f'rP^{murray_exponent:.1f}': p_power, f'sum_rC^{murray_exponent:.1f}': c_sum_power,
                'area_ratio_alpha': alpha, 'daughter_asymmetry_ratio': asymmetry_ratio,
                'angle_deg': angle_deg_val,
                'rP': r_p_node, 'rC1': r_c1_node, 'rC2': r_c2_node,
                'qP_graph': q_p_node, 'qC1_graph': q_c1_node, 'qC2_graph': q_c2_node
            })

    logger.info(f"Bifurcation Geometry Analysis: Processed {bifurcation_nodes_processed} potential bifurcation nodes.")

    if bifurcation_data_for_csv:
        df_bif_geom = pd.DataFrame(bifurcation_data_for_csv)
        csv_path = os.path.join(output_dir, f"{filename_prefix}bifurcation_geometry_data.csv")
        df_bif_geom.to_csv(csv_path, index=False)
        logger.info(f"Saved bifurcation geometry data to {csv_path}")
    else:
        logger.info("No valid bifurcation data collected for CSV. Skipping plots.")
        return

    if MATPLOTLIB_AVAILABLE and murray_parent_powers:
        plt.figure(figsize=(7, 7))
        plt.scatter(murray_children_sum_powers, murray_parent_powers, alpha=0.6, edgecolors='k', s=40, label="Bifurcations")
        max_val_plot = 0.0
        valid_parent_powers = [p for p in murray_parent_powers if np.isfinite(p)]
        valid_children_sum_powers = [c for c in murray_children_sum_powers if np.isfinite(c)]
        if valid_parent_powers and valid_children_sum_powers:
             max_val_plot = max(max(valid_parent_powers, default=0.0), max(valid_children_sum_powers, default=0.0)) * 1.1
        if max_val_plot < constants.EPSILON : max_val_plot = 1.0

        plt.plot([0, max_val_plot], [0, max_val_plot], 'r--', label=f'Ideal Murray (y=x, exp={murray_exponent:.1f})')
        plt.xlabel(f"Sum of Children Radii^{murray_exponent:.1f} (r$_1^{murray_exponent:.1f}$ + r$_2^{murray_exponent:.1f}$)")
        plt.ylabel(f"Parent Radius^{murray_exponent:.1f} (r$_0^{murray_exponent:.1f}$)")
        plt.title("Murray's Law Compliance Test")
        plt.legend()
        plt.grid(True)
        plt.xlim([0, max_val_plot]); plt.ylim([0, max_val_plot])
        plot_path = os.path.join(output_dir, f"{filename_prefix}murray_law_compliance.png")
        try: plt.savefig(plot_path); logger.info(f"Saved Murray's Law plot to {plot_path}");
        except Exception as e: logger.error(f"Error saving Murray's Law plot: {e}")
        finally: plt.close()
    else: logger.info("Skipping Murray's Law plot (Matplotlib unavailable or no data).")

    _plot_histogram(area_ratios_alpha, "Distribution of Bifurcation Area Ratios (α)", "Area Ratio α = (r₁²+r₂²)/r₀²",
                    os.path.join(output_dir, f"{filename_prefix}area_ratios_distribution.png"), bins=20)
    _plot_histogram(daughter_asymmetry_ratios, "Distribution of Daughter Radius Asymmetry Ratios", "Asymmetry Ratio min(r₁,r₂)/max(r₁,r₂)",
                    os.path.join(output_dir, f"{filename_prefix}daughter_asymmetry_distribution.png"), bins=20)
    _plot_histogram(branching_angles_c1_c2, "Distribution of Branching Angles (Child-Child)", "Angle (degrees)",
                    os.path.join(output_dir, f"{filename_prefix}branching_angles_distribution.png"), bins=18)


def analyze_degree_distribution(graph: nx.DiGraph, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Degree distribution analysis: Graph is empty or None.")
        return
    if not MATPLOTLIB_AVAILABLE:
        logger.warning("Matplotlib not available. Skipping degree distribution plots.")
        return

    degrees = [d for n, d in graph.degree()]
    in_degrees = [d for n, d in graph.in_degree()]
    out_degrees = [d for n, d in graph.out_degree()]

    df_degrees = pd.DataFrame({
        'node_id': list(graph.nodes()),
        'total_degree': degrees,
        'in_degree': in_degrees,
        'out_degree': out_degrees
    })
    csv_path = os.path.join(output_dir, f"{filename_prefix}degree_data.csv")
    df_degrees.to_csv(csv_path, index=False)
    logger.info(f"Saved node degree data to {csv_path}")

    max_bins_degree = 10 # Default max bins for degree plots
    if degrees: max_bins_degree = max(1, max(degrees))
    _plot_histogram(degrees, "Total Node Degree Distribution", "Degree",
                    os.path.join(output_dir, f"{filename_prefix}total_degree_distribution.png"),
                    bins=min(max_bins_degree, 50)) # Cap bins for very high degrees
    if in_degrees: max_bins_degree = max(1, max(in_degrees))
    _plot_histogram(in_degrees, "Node In-Degree Distribution", "In-Degree",
                    os.path.join(output_dir, f"{filename_prefix}in_degree_distribution.png"),
                    bins=min(max_bins_degree, 50))
    if out_degrees: max_bins_degree = max(1, max(out_degrees))
    _plot_histogram(out_degrees, "Node Out-Degree Distribution", "Out-Degree",
                    os.path.join(output_dir, f"{filename_prefix}out_degree_distribution.png"),
                    bins=min(max_bins_degree, 50))

def analyze_network_connectivity(graph: nx.DiGraph, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Network connectivity analysis: Graph is empty or None.")
        return
    undirected_graph = graph.to_undirected()
    num_components = nx.number_connected_components(undirected_graph)
    logger.info(f"Network Connectivity: Number of connected components (undirected) = {num_components}")
    if num_components > 1:
        logger.warning(f"Network has {num_components} disconnected components. Expected 1 for a fully connected structure from seeds.")

def analyze_volumetric_densities(graph: nx.DiGraph, tissue_data: dict, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Volumetric density analysis: Graph is empty or None.")
        return
    if 'domain_mask' not in tissue_data or 'voxel_volume' not in tissue_data:
        logger.warning("Volumetric density analysis: Missing 'domain_mask' or 'voxel_volume' in tissue_data.")
        return

    domain_mask = tissue_data['domain_mask']
    voxel_volume = tissue_data['voxel_volume'] 

    if domain_mask is None or voxel_volume <= 0:
        logger.warning("Volumetric density analysis: Invalid domain_mask or voxel_volume.")
        return

    domain_volume_mm3 = np.sum(domain_mask) * voxel_volume
    if domain_volume_mm3 < constants.EPSILON:
        logger.warning("Volumetric density analysis: Domain volume is zero. Cannot calculate densities.")
        return

    total_vessel_length_mm = sum(data['length'] for u, v, data in graph.edges(data=True) if 'length' in data and np.isfinite(data['length']))
    num_bifurcation_nodes = sum(1 for node_id, data in graph.nodes(data=True) if data.get('type') == 'synthetic_bifurcation')

    vessel_length_density_mm_per_mm3 = total_vessel_length_mm / domain_volume_mm3
    branchpoint_density_per_mm3 = num_bifurcation_nodes / domain_volume_mm3

    logger.info(f"--- Volumetric Densities (Domain Volume: {domain_volume_mm3:.2e} mm^3) ---")
    logger.info(f"  Total vessel length: {total_vessel_length_mm:.2e} mm")
    logger.info(f"  Vessel length density: {vessel_length_density_mm_per_mm3:.2e} mm/mm^3")
    logger.info(f"  Number of bifurcation points: {num_bifurcation_nodes}")
    logger.info(f"  Branchpoint density: {branchpoint_density_per_mm3:.2e} #/mm^3")

    density_data = {
        'domain_volume_mm3': domain_volume_mm3,
        'total_vessel_length_mm': total_vessel_length_mm,
        'vessel_length_density_mm_per_mm3': vessel_length_density_mm_per_mm3,
        'num_bifurcation_nodes': num_bifurcation_nodes,
        'branchpoint_density_per_mm3': branchpoint_density_per_mm3
    }
    df_density = pd.DataFrame([density_data])
    csv_path = os.path.join(output_dir, f"{filename_prefix}volumetric_density_data.csv")
    df_density.to_csv(csv_path, index=False)
    logger.info(f"Saved volumetric density data to {csv_path}")


def plot_vascular_tree_pyvista(
    graph: Optional[nx.DiGraph],
    title: str = "Vascular Tree",
    # ... (other existing parameters: background_color, cmap_radius, etc.)
    output_screenshot_path: Optional[str] = None,
    tissue_masks: Optional[Dict[str, Tuple[np.ndarray, np.ndarray]]] = None,
    gbo_seed_points_world: Optional[List[Tuple[np.ndarray, float, str]]] = None, # Renamed for clarity
    initial_tumor_seed_info: Optional[Dict] = None, # New: {'center_world': np.array, 'radius_world': float}
    # ... (other existing parameters: domain_outline_color, etc.)
    color_by_scalar: Optional[str] = 'radius', 
    scalar_bar_title_override: Optional[str] = None,
    custom_cmap: Optional[str] = None,
    config_for_viz_params: Optional[dict] = None # Pass config for viz specific params
    ):
    if not PYVISTA_AVAILABLE: logger.warning("PyVista not available. Skipping 3D PyVista plot."); return

    # Fetch viz params from config if provided, else use hardcoded defaults
    cfg = config_for_viz_params if config_for_viz_params else {}
    bg_color = config_manager.get_param(cfg, "visualization.pyvista_background_color", "white")
    default_cmap_radius = config_manager.get_param(cfg, "visualization.pyvista_cmap_radius", "viridis")
    seed_color = config_manager.get_param(cfg, "visualization.seed_point_color", "red")
    seed_radius_scale = config_manager.get_param(cfg, "visualization.seed_marker_radius_scale", 5.0)
    tumor_seed_color = config_manager.get_param(cfg, "visualization.tumor_seed_marker_color", "magenta")


    plotter = pv.Plotter(off_screen=output_screenshot_path is not None, window_size=[1200,900])
    plotter.background_color = bg_color
    plotter.add_title(title, font_size=16)

    spacing_for_markers = np.array([1.0, 1.0, 1.0]) # Default

    if tissue_masks:
        mask_colors_default = {
            "GM": "lightblue", "WM": "lightyellow", 
            "domain_mask": config_manager.get_param(cfg, "visualization.domain_mask_color", "lightgray"),
            "CSF": "lightcyan",
            "Tumor_Max_Extent": config_manager.get_param(cfg, "visualization.tumor_max_extent_mask_color", "salmon"),
            "Tumor": config_manager.get_param(cfg, "visualization.active_tumor_mask_color", "darkred") # For active tumor
        }
        mask_opacities_default = {
            "GM": 0.2, "WM": 0.2, 
            "domain_mask": config_manager.get_param(cfg, "visualization.domain_mask_opacity", 0.1),
            "CSF": 0.1,
            "Tumor_Max_Extent": 0.15,
            "Tumor": 0.3 # Active tumor slightly more opaque
        }

        for mask_name, mask_data_tuple in tissue_masks.items():
            if not isinstance(mask_data_tuple, tuple) or len(mask_data_tuple) != 2: continue
            mask_data, affine = mask_data_tuple
            if mask_data is None or not np.any(mask_data) or affine is None: continue

            logger.info(f"Plotting context mask '{mask_name}': Shape={mask_data.shape}, Sum={np.sum(mask_data)}")
            try:
                dims = np.array(mask_data.shape)
                current_spacing = np.abs(np.diag(affine)[:3])
                origin = affine[:3, 3]
                if np.all(current_spacing > constants.EPSILON): # Update if valid spacing
                    spacing_for_markers = current_spacing

                grid = pv.ImageData(dimensions=dims, spacing=current_spacing, origin=origin)
                grid.point_data[mask_name] = mask_data.flatten(order="F").astype(float)
                contour = grid.contour([0.5], scalars=mask_name, rng=[0,1])

                if contour.n_points > 0:
                    color = mask_colors_default.get(mask_name, "grey")
                    opacity = mask_opacities_default.get(mask_name, 0.1)
                    plotter.add_mesh(contour, color=color, opacity=opacity, style='surface')
                    logger.debug(f"Mask '{mask_name}' contour bounds: {contour.bounds}")
                    if mask_name == "domain_mask" and output_screenshot_path: # For initial setup plot mainly
                        debug_contour_path = os.path.join(os.path.dirname(output_screenshot_path), f"debug_plot_{mask_name}_contour.vtk")
                        try: contour.save(debug_contour_path)
                        except Exception as e_save: logger.error(f"Could not save debug contour {mask_name}: {e_save}")

                else: logger.warning(f"No contour generated for mask '{mask_name}'.")
            except Exception as e_mask: logger.error(f"Error plotting mask '{mask_name}': {e_mask}", exc_info=True)
    else:
        logger.info("No tissue masks provided for this plot.")

    # Plot GBO Seed points (if any)
    if gbo_seed_points_world:
        seed_color = config_manager.get_param(cfg, "visualization.seed_point_color", "red") # Assuming cfg is defined from config_for_viz_params
        seed_radius_scale = config_manager.get_param(cfg, "visualization.seed_marker_radius_scale", 2.0)
        for seed_pos, seed_initial_radius, seed_name in gbo_seed_points_world:
            marker_base_size = np.mean(spacing_for_markers) * 0.5
            visual_marker_radius = max(marker_base_size * seed_radius_scale, seed_initial_radius * 2.0, 0.05 * np.mean(spacing_for_markers))
            plotter.add_mesh(pv.Sphere(center=seed_pos, radius=visual_marker_radius), color=seed_color, opacity=0.9)
            plotter.add_point_labels(seed_pos + np.array([0,0,visual_marker_radius*2]), [seed_name], font_size=10, text_color=seed_color)


    # Plot Initial Tumor Seed Sphere (New)
    if initial_tumor_seed_info:
        center_w = initial_tumor_seed_info.get('center_world')
        radius_w = initial_tumor_seed_info.get('radius_world')
        if center_w is not None and radius_w is not None and radius_w > 0:
            logger.info(f"Plotting initial tumor seed sphere at {np.round(center_w,2)} with radius {radius_w:.3f}")
            plotter.add_mesh(pv.Sphere(center=center_w, radius=radius_w), color=tumor_seed_color, opacity=0.5)
            plotter.add_point_labels(center_w + np.array([0,0,radius_w*1.5]), ["Initial Tumor Seed"], font_size=10, text_color=tumor_seed_color)


    # Plot Vascular Graph (if any)
    # ... (The existing graph plotting logic from the previous "complete" visualization.py version) ...
    # ... This includes creating tree_mesh, handling point/cell data for scalars ...
    # ... Make sure to use `default_cmap_radius` from fetched config ...
    tree_mesh_bounds_logged = False
    if graph is not None and graph.number_of_nodes() > 0:
        points, lines, node_to_idx, idx_counter = [], [], {}, 0
        point_data_arrays: Dict[str, List[float]] = {'radius': [], 'pressure': []}
        edge_data_arrays: Dict[str, List[float]] = {'flow_solver': []}
        min_voxel_dim = np.min(spacing_for_markers) if np.any(spacing_for_markers > 0) else 0.01
        min_plot_radius = max(constants.MIN_VESSEL_RADIUS_MM * 0.1, min_voxel_dim * 0.05, 1e-4)

        for node_id, data in graph.nodes(data=True):
            if 'pos' in data and np.all(np.isfinite(data['pos'])):
                points.append(data['pos'])
                point_data_arrays['radius'].append(max(data.get('radius', min_plot_radius), min_plot_radius) if np.isfinite(data.get('radius', min_plot_radius)) else min_plot_radius)
                point_data_arrays['pressure'].append(data.get('pressure', np.nan))
                node_to_idx[node_id] = idx_counter; idx_counter += 1
        if not points: logger.warning("No valid nodes with positions in graph for tree plotting.")
        else:
            points_np = np.array(points)
            for u, v, data in graph.edges(data=True):
                if u in node_to_idx and v in node_to_idx:
                    lines.extend([2, node_to_idx[u], node_to_idx[v]])
                    edge_data_arrays['flow_solver'].append(data.get('flow_solver', np.nan))
            tree_mesh = pv.PolyData()
            if points_np.shape[0] > 0:
                tree_mesh.points = points_np
                for key, arr in point_data_arrays.items():
                    if arr and len(arr) == tree_mesh.n_points: tree_mesh.point_data[key] = np.array(arr)
                logger.info(f"Vascular tree mesh for plot: {tree_mesh.n_points} points. Bounds: {tree_mesh.bounds}")
                tree_mesh_bounds_logged = True
                if lines:
                    tree_mesh.lines = np.array(lines)
                    if tree_mesh.n_cells > 0:
                        for key, arr in edge_data_arrays.items():
                             if arr and len(arr) == tree_mesh.n_cells: tree_mesh.cell_data[key] = np.array(arr)
                        active_scalars_on_points = color_by_scalar in tree_mesh.point_data and np.any(np.isfinite(tree_mesh.point_data[color_by_scalar]))
                        active_scalars_on_cells = color_by_scalar in tree_mesh.cell_data and np.any(np.isfinite(tree_mesh.cell_data[color_by_scalar]))
                        sargs = {'title': scalar_bar_title_override if scalar_bar_title_override else color_by_scalar.replace("_"," ").title(), 'color':'black', 'vertical':True, 'position_y': 0.05, 'position_x': 0.85, 'height': 0.3, 'n_labels': 5}
                        current_cmap_actual = custom_cmap if custom_cmap else (default_cmap_radius if color_by_scalar == 'radius' else 'coolwarm')
                        preference = 'point' if active_scalars_on_points else ('cell' if active_scalars_on_cells else None)
                        if preference:
                            plotter.add_mesh(tree_mesh, scalars=color_by_scalar, line_width=3, cmap=current_cmap_actual, render_lines_as_tubes=True, scalar_bar_args=sargs, preference=preference)
                        else:
                            if 'radius' in tree_mesh.point_data and np.any(np.isfinite(tree_mesh.point_data['radius'])):
                                plotter.add_mesh(tree_mesh, scalars='radius', line_width=3, cmap=default_cmap_radius, render_lines_as_tubes=True, scalar_bar_args={'title': 'Radius (mm)', **sargs})
                            else: plotter.add_mesh(tree_mesh, color="darkgrey", line_width=2, render_lines_as_tubes=True)
                    # ... (Glyphing for points only if no lines was here, can be added back if needed) ...
    if not tree_mesh_bounds_logged: logger.info("No vascular graph provided or it was empty.")

    plotter.camera_position = 'iso'
    plotter.enable_parallel_projection()
    plotter.add_axes(interactive=True)
    if output_screenshot_path: plotter.show(auto_close=True, screenshot=output_screenshot_path)
    else: plotter.show()


# Modify generate_final_visualizations or add a new function for initial setup plot
def visualize_initial_setup(
    config: dict, output_dir: str, tissue_data: dict, 
    initial_arterial_graph: Optional[nx.DiGraph]
    ):
    logger.info("Generating visualization of initial simulation setup...")
    
    masks_to_plot: Dict[str, Tuple[np.ndarray, np.ndarray]] = {}
    mask_keys_to_plot = ['domain_mask', 'GM', 'WM', 'CSF', 'Tumor_Max_Extent']
    for key in mask_keys_to_plot:
        if tissue_data.get(key) is not None and np.any(tissue_data[key]) and tissue_data.get('affine') is not None:
            masks_to_plot[key] = (tissue_data[key], tissue_data['affine'])
            logger.info(f"Added '{key}' to initial setup plot.")

    gbo_seeds_viz_data: List[Tuple[np.ndarray, float, str]] = []
    if not initial_arterial_graph or initial_arterial_graph.number_of_nodes() == 0: # Only plot config seeds if no VTP graph
        config_gbo_seeds = config_manager.get_param(config, "gbo_growth.seed_points", [])
        if config_gbo_seeds and isinstance(config_gbo_seeds, list):
            for i, seed_info in enumerate(config_gbo_seeds):
                if isinstance(seed_info, dict) and 'position' in seed_info:
                    gbo_seeds_viz_data.append((np.array(seed_info['position']),
                                               float(seed_info.get('initial_radius',0.1)),
                                               seed_info.get('id',f"GBO_Seed_{i}")))

    initial_tumor_seed_plot_info: Optional[Dict] = None
    tumor_seed_config = config_manager.get_param(config, "tumor_angiogenesis.initial_tumor_seed", {})
    if tissue_data.get('Tumor_Max_Extent') is not None and np.any(tissue_data['Tumor_Max_Extent']):
        center_ijk = tumor_seed_config.get('center_voxel_ijk_relative_to_image_grid')
        radius_vox = tumor_seed_config.get('radius_voxels')
        if center_ijk and radius_vox and tissue_data.get('affine') is not None:
            center_world = utils.voxel_to_world(np.array(center_ijk), tissue_data['affine'])[0]
            # Approximate world radius - this is tricky as voxels can be anisotropic
            # For visualization, average voxel spacing * radius_vox is a decent estimate
            avg_voxel_spacing = np.mean(np.abs(np.diag(tissue_data['affine'])[:3]))
            radius_world = radius_vox * avg_voxel_spacing
            initial_tumor_seed_plot_info = {'center_world': center_world, 'radius_world': radius_world}

    plot_title = "Initial Simulation Setup: Masks, Arterial Tree/Seeds, Tumor Seed"
    screenshot_path = os.path.join(output_dir, "initial_setup_visualization.png")

    if PYVISTA_AVAILABLE:
        plot_vascular_tree_pyvista(
            graph=initial_arterial_graph, # Plot the parsed VTP tree if available
            title=plot_title,
            output_screenshot_path=screenshot_path,
            tissue_masks=masks_to_plot,
            gbo_seed_points_world=gbo_seeds_viz_data,
            initial_tumor_seed_info=initial_tumor_seed_plot_info,
            color_by_scalar='radius', # Color initial tree by radius
            config_for_viz_params=config # Pass config for viz params
        )
    else:
        logger.warning("PyVista not available, skipping initial setup 3D plot.")


def generate_final_visualizations(
    config: dict, output_dir: str, tissue_data: dict, vascular_graph: Optional[nx.DiGraph],
    perfusion_map: Optional[np.ndarray] = None, pressure_map_tissue: Optional[np.ndarray] = None,
    plot_context_masks: bool = True
    ):
    # ... (This function remains largely the same as the one from the previous "complete" response)
    # ... (It will call plot_vascular_tree_pyvista for radius, flow, pressure plots of the FINAL graph)
    # ... (Ensure it passes `config_for_viz_params=config` to plot_vascular_tree_pyvista)
    logger.info("Generating final visualizations and quantitative analyses...")
    masks_to_plot_for_pyvista: Optional[Dict[str, Tuple[np.ndarray, np.ndarray]]] = None
    if plot_context_masks:
        masks_to_plot_for_pyvista = {}
        # Plot active tumor for final viz, not Tumor_Max_Extent unless specifically desired
        mask_keys = ['domain_mask', 'GM', 'WM', 'CSF', 'Tumor'] # 'Tumor' here is the active one
        for key in mask_keys:
            if tissue_data.get(key) is not None and np.any(tissue_data[key]) and tissue_data.get('affine') is not None:
                masks_to_plot_for_pyvista[key] = (tissue_data[key], tissue_data['affine'])
        if not masks_to_plot_for_pyvista: masks_to_plot_for_pyvista = None
    else: logger.info("Context mask plotting disabled for final visualizations.")

    # GBO seeds are less relevant for final plot if tree exists, but tumor seed might be if small
    # For simplicity, let's not replot GBO seeds here if a tree exists.
    # Tumor seed sphere could be plotted if desired, using its initial config.

    if vascular_graph and vascular_graph.number_of_nodes() > 0:
        # ... (Saving VTP, quantitative analyses - as before) ...
        final_tree_vtp_path = os.path.join(output_dir, "final_plot_vascular_tree.vtp")
        io_utils.save_vascular_tree_vtp(vascular_graph, final_tree_vtp_path, radius_attr='radius', pressure_attr='pressure', flow_attr='flow_solver')
        logger.info(f"Final vascular tree saved for analysis: {final_tree_vtp_path}")
        analyze_radii_distribution(vascular_graph, output_dir)
        analyze_segment_lengths(vascular_graph, output_dir)
        analyze_bifurcation_geometry(vascular_graph, output_dir, murray_exponent=config_manager.get_param(config, "vascular_properties.murray_law_exponent", 3.0))
        analyze_degree_distribution(vascular_graph, output_dir)
        analyze_network_connectivity(vascular_graph, output_dir)
        analyze_volumetric_densities(vascular_graph, tissue_data, output_dir)

        if PYVISTA_AVAILABLE:
            plot_suffix = "" if plot_context_masks else "_no_context"
            # Radius Plot
            pv_plot_title_radius = f"Final Vasculature (Radius, {vascular_graph.number_of_nodes()}N, {vascular_graph.number_of_edges()}E)"
            pv_screenshot_path_radius = os.path.join(output_dir, f"final_vascular_tree_radius_3D{plot_suffix}.png")
            plot_vascular_tree_pyvista(graph=vascular_graph, title=pv_plot_title_radius, output_screenshot_path=pv_screenshot_path_radius,
                                       tissue_masks=masks_to_plot_for_pyvista, color_by_scalar='radius', config_for_viz_params=config)
            # Flow Plot
            if any('flow_solver' in data for _,_,data in vascular_graph.edges(data=True) if 'flow_solver' in data and np.any(np.isfinite(data['flow_solver']))):
                pv_plot_title_flow = f"Final Vasculature (Edge Flow)"
                pv_screenshot_path_flow = os.path.join(output_dir, f"final_vascular_tree_flow_3D{plot_suffix}.png")
                plot_vascular_tree_pyvista(graph=vascular_graph, title=pv_plot_title_flow, output_screenshot_path=pv_screenshot_path_flow,
                                           tissue_masks=masks_to_plot_for_pyvista, color_by_scalar='flow_solver', custom_cmap='coolwarm', 
                                           scalar_bar_title_override="Flow (mm³/s)", config_for_viz_params=config)
            # Pressure Plot
            if any('pressure' in data for _,data in vascular_graph.nodes(data=True) if 'pressure' in data and np.any(np.isfinite(data['pressure']))):
                pv_plot_title_pressure = f"Final Vasculature (Node Pressure)"
                pv_screenshot_path_pressure = os.path.join(output_dir, f"final_vascular_tree_pressure_3D{plot_suffix}.png")
                plot_vascular_tree_pyvista(graph=vascular_graph, title=pv_plot_title_pressure, output_screenshot_path=pv_screenshot_path_pressure,
                                           tissue_masks=masks_to_plot_for_pyvista, color_by_scalar='pressure', custom_cmap='coolwarm', 
                                           scalar_bar_title_override="Pressure (Pa)", config_for_viz_params=config)
    # ... (rest of generate_final_visualizations)
    logger.info("Final visualizations and analyses generation complete.")


def generate_final_visualizations(
    config: dict, output_dir: str, tissue_data: dict, vascular_graph: Optional[nx.DiGraph],
    perfusion_map: Optional[np.ndarray] = None, pressure_map_tissue: Optional[np.ndarray] = None,
    plot_context_masks: bool = True # New argument for controlling mask plotting
    ):
    logger.info("Generating final visualizations and quantitative analyses...")
    
    masks_to_plot_for_pyvista: Optional[Dict[str, Tuple[np.ndarray, np.ndarray]]]
    if plot_context_masks:
        masks_to_plot_for_pyvista = {}
        if tissue_data.get('domain_mask') is not None and tissue_data.get('affine') is not None:
            masks_to_plot_for_pyvista["domain_mask"] = (tissue_data['domain_mask'], tissue_data['affine'])
        if tissue_data.get('GM') is not None and tissue_data.get('affine') is not None:
            masks_to_plot_for_pyvista["GM"] = (tissue_data['GM'], tissue_data['affine'])
        if tissue_data.get('WM') is not None and tissue_data.get('affine') is not None:
            masks_to_plot_for_pyvista["WM"] = (tissue_data['WM'], tissue_data['affine'])
        if tissue_data.get('Tumor') is not None and tissue_data.get('affine') is not None:
            masks_to_plot_for_pyvista["Tumor"] = (tissue_data['Tumor'], tissue_data['affine'])
        if not masks_to_plot_for_pyvista: # If dict is still empty
            logger.info("No valid masks found in tissue_data to plot for context.")
            masks_to_plot_for_pyvista = None # Explicitly set to None
    else:
        logger.info("Context mask plotting is disabled for this visualization call.")
        masks_to_plot_for_pyvista = None


    seed_points_viz_data: List[Tuple[np.ndarray, float, str]] = []
    config_seeds = config_manager.get_param(config, "gbo_growth.seed_points", [])
    if config_seeds and isinstance(config_seeds, list):
        for i, seed_info in enumerate(config_seeds):
            if isinstance(seed_info, dict) and 'position' in seed_info:
                seed_points_viz_data.append((np.array(seed_info.get('position')),
                                             float(seed_info.get('initial_radius',0.1)),
                                             seed_info.get('id',f"S_{i}")))
            else: logger.warning(f"Skipping invalid seed_info in config: {seed_info}")

    if vascular_graph and vascular_graph.number_of_nodes() > 0:
        final_tree_vtp_path = os.path.join(output_dir, "final_plot_vascular_tree.vtp")
        io_utils.save_vascular_tree_vtp(vascular_graph, final_tree_vtp_path,
                                        radius_attr='radius', pressure_attr='pressure', flow_attr='flow_solver')
        logger.info(f"Final vascular tree saved for analysis: {final_tree_vtp_path}")

        analyze_radii_distribution(vascular_graph, output_dir)
        analyze_segment_lengths(vascular_graph, output_dir)
        analyze_bifurcation_geometry(vascular_graph, output_dir,
                                     murray_exponent=config_manager.get_param(config, "vascular_properties.murray_law_exponent", 3.0))
        analyze_degree_distribution(vascular_graph, output_dir)
        analyze_network_connectivity(vascular_graph, output_dir)
        analyze_volumetric_densities(vascular_graph, tissue_data, output_dir)

        # Plotting with Radius (always try this if graph exists)
        pv_plot_title_radius = f"Vasculature (Radius, {vascular_graph.number_of_nodes()}N, {vascular_graph.number_of_edges()}E)"
        pv_screenshot_path_radius = os.path.join(output_dir, f"final_vascular_tree_radius_3D{'' if plot_context_masks else '_no_context'}.png")
        if PYVISTA_AVAILABLE:
            plot_vascular_tree_pyvista(
                graph=vascular_graph, title=pv_plot_title_radius, output_screenshot_path=pv_screenshot_path_radius,
                tissue_masks=masks_to_plot_for_pyvista, seed_points_world=seed_points_viz_data,
                color_by_scalar='radius', custom_cmap=config_manager.get_param(config, "visualization.pyvista_cmap_radius", "viridis")
            )

            # Plotting with Flow
            if any('flow_solver' in data for _,_,data in vascular_graph.edges(data=True) if 'flow_solver' in data and np.any(np.isfinite(data['flow_solver']))):
                pv_plot_title_flow = f"Vasculature (Edge Flow, {vascular_graph.number_of_nodes()}N, {vascular_graph.number_of_edges()}E)"
                pv_screenshot_path_flow = os.path.join(output_dir, f"final_vascular_tree_flow_3D{'' if plot_context_masks else '_no_context'}.png")
                plot_vascular_tree_pyvista(
                    graph=vascular_graph, title=pv_plot_title_flow, output_screenshot_path=pv_screenshot_path_flow,
                    tissue_masks=masks_to_plot_for_pyvista, seed_points_world=seed_points_viz_data,
                    color_by_scalar='flow_solver', custom_cmap='coolwarm', scalar_bar_title_override="Flow (mm³/s)"
                )
            else: logger.info("Skipping flow plot: No valid 'flow_solver' data on edges.")

            # Plotting with Pressure
            if any('pressure' in data for _,data in vascular_graph.nodes(data=True) if 'pressure' in data and np.any(np.isfinite(data['pressure']))):
                pv_plot_title_pressure = f"Vasculature (Node Pressure, {vascular_graph.number_of_nodes()}N, {vascular_graph.number_of_edges()}E)"
                pv_screenshot_path_pressure = os.path.join(output_dir, f"final_vascular_tree_pressure_3D{'' if plot_context_masks else '_no_context'}.png")
                plot_vascular_tree_pyvista(
                    graph=vascular_graph, title=pv_plot_title_pressure, output_screenshot_path=pv_screenshot_path_pressure,
                    tissue_masks=masks_to_plot_for_pyvista, seed_points_world=seed_points_viz_data,
                    color_by_scalar='pressure', custom_cmap='coolwarm', scalar_bar_title_override="Pressure (Pa)"
                )
            else: logger.info("Skipping pressure plot: No valid 'pressure' data on nodes.")
        else: logger.warning("PyVista not available, skipping 3D plots.")
    else: # No valid vascular_graph
        logger.warning("Vascular graph is empty or None. Skipping VTP save and quantitative analyses.")
        if PYVISTA_AVAILABLE:
            plot_vascular_tree_pyvista(None, title=f"Tissue Context & Seeds (No Vasculature){' (No Context Masks)' if not plot_context_masks else ''}",
                                       output_screenshot_path=os.path.join(output_dir, f"context_only_plot{'' if plot_context_masks else '_no_context'}.png"),
                                       tissue_masks=masks_to_plot_for_pyvista, seed_points_world=seed_points_viz_data)

    logger.info("Final visualizations and analyses generation complete.")# src/angiogenesis.py
from __future__ import annotations # Must be first line

import numpy as np
import networkx as nx
import logging
import os
from scipy.spatial import KDTree
from scipy.ndimage import binary_erosion, binary_dilation, gaussian_filter
from typing import Tuple, List, Dict, Optional, Set, Callable

from src import utils, data_structures, constants, config_manager, io_utils
from src.vascular_growth import GBOIterationData

logger = logging.getLogger(__name__)

DEFAULT_RIM_THICKNESS_VOXELS = 3
DEFAULT_VEGF_PRODUCTION_RIM = 1.0
DEFAULT_MIN_TUMOR_TERMINAL_DEMAND = 1e-5

# --- Tumor Morphology and State Update Functions ---
# (initialize_active_tumor_from_seed, update_tumor_rim_and_core,
#  update_metabolic_demand_for_tumor, update_vegf_field_rim_driven,
#  grow_tumor_mass_within_defined_segmentation, coopt_and_modify_vessels,
#  find_angiogenic_sprouting_candidates - These functions remain the same as the previous complete version)
# For brevity, I will assume these functions are present as in the previous complete response.
# If you need them explicitly here, let me know. I'll paste them from the previous version.

# --- PASTE THE FOLLOWING FUNCTIONS FROM THE PREVIOUS RESPONSE HERE ---
# initialize_active_tumor_from_seed
# update_tumor_rim_and_core
# update_metabolic_demand_for_tumor
# update_vegf_field_rim_driven
# grow_tumor_mass_within_defined_segmentation
# coopt_and_modify_vessels
# find_angiogenic_sprouting_candidates
# --- END OF PASTED FUNCTIONS ---

# --- Angiogenesis Core Functions (Sprouting, Growth, Co-option) ---
# (Functions like find_angiogenic_sprouting_candidates, coopt_and_modify_vessels are assumed to be here
#  from the previous "complete" version provided)

# --- PASTE find_angiogenic_sprouting_candidates and coopt_and_modify_vessels here from previous response ---
def initialize_active_tumor_from_seed(tissue_data: dict, config: dict) -> bool:
    """
    Initializes a small 'active' tumor (tissue_data['Tumor'])
    within the bounds of a pre-defined 'Tumor_Max_Extent'.
    If specific seed coordinates are not provided in config, it attempts to find a seed automatically.
    """
    tumor_max_extent_array = tissue_data.get('Tumor_Max_Extent')
    if tumor_max_extent_array is None or not np.any(tumor_max_extent_array):
        logger.error("Cannot initialize seed: 'Tumor_Max_Extent' not found, is None, or is empty in tissue_data.")
        if 'shape' in tissue_data and tissue_data['shape'] is not None:
            tissue_data['Tumor'] = np.zeros(tissue_data['shape'], dtype=bool)
        return False

    tumor_seed_config = config_manager.get_param(config, "tumor_angiogenesis.initial_tumor_seed", {})
    seed_strategy = tumor_seed_config.get("strategy", "auto_center") # New: "auto_center", "auto_random", "manual"
    radius_vox = tumor_seed_config.get('radius_voxels', 3)
    shape = tissue_data['shape']
    center_vox_ijk = None

    if seed_strategy == "manual":
        center_vox_ijk_manual = tumor_seed_config.get('center_voxel_ijk_relative_to_image_grid')
        if not center_vox_ijk_manual or not isinstance(center_vox_ijk_manual, list) or len(center_vox_ijk_manual) != 3:
            logger.error("Tumor seed strategy is 'manual' but 'center_voxel_ijk_relative_to_image_grid' not properly defined. Aborting seed.")
            tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
            return False
        center_vox_ijk = np.array(center_vox_ijk_manual)
        if not utils.is_voxel_in_bounds(center_vox_ijk, shape):
             logger.error(f"Manual tumor seed center {center_vox_ijk} is outside image dimensions {shape}.")
             tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
             return False
    elif seed_strategy == "auto_center":
        from scipy.ndimage import center_of_mass
        if np.any(tumor_max_extent_array):
            # Calculate the center of mass of the Tumor_Max_Extent mask
            # Ensure it's integer coordinates and within bounds
            com_float = center_of_mass(tumor_max_extent_array)
            center_vox_ijk = np.round(com_float).astype(int)
            # Clip to ensure it's within array bounds strictly (for safety with radius)
            for d in range(3):
                center_vox_ijk[d] = np.clip(center_vox_ijk[d], radius_vox, shape[d] - 1 - radius_vox)
            logger.info(f"Automatic seed strategy 'auto_center': calculated COM {com_float}, using seed center {center_vox_ijk}.")
        else: # Should have been caught by the first check, but for safety
            logger.error("'auto_center' seed strategy failed: Tumor_Max_Extent is empty.")
            tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
            return False
    elif seed_strategy == "auto_random":
        true_indices = np.array(np.where(tumor_max_extent_array)).T
        if true_indices.shape[0] > 0:
            random_idx = np.random.choice(true_indices.shape[0])
            center_vox_ijk = true_indices[random_idx]
            logger.info(f"Automatic seed strategy 'auto_random': selected random seed center {center_vox_ijk}.")
        else: # Should have been caught, but for safety
            logger.error("'auto_random' seed strategy failed: Tumor_Max_Extent is empty.")
            tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
            return False
    else:
        logger.error(f"Unknown tumor seed strategy: {seed_strategy}. Choose 'manual', 'auto_center', or 'auto_random'.")
        tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
        return False

    # Create a spherical seed mask at the determined center
    coords = np.ogrid[:shape[0], :shape[1], :shape[2]]
    distance_sq = ((coords[0] - center_vox_ijk[0])**2 +
                   (coords[1] - center_vox_ijk[1])**2 +
                   (coords[2] - center_vox_ijk[2])**2)
    initial_seed_mask = distance_sq <= radius_vox**2

    # Active tumor is the seed AND where it overlaps with the max extent
    tissue_data['Tumor'] = initial_seed_mask & tumor_max_extent_array

    if not np.any(tissue_data['Tumor']):
        logger.warning(f"Initial tumor seed (strategy: {seed_strategy}) at {center_vox_ijk} with radius {radius_vox} "
                       f"resulted in an empty active tumor region within Tumor_Max_Extent. "
                       f"This might happen if Tumor_Max_Extent is very thin or small at the chosen seed location, "
                       f"or if the seed radius is too small to capture any 'True' voxels of Tumor_Max_Extent.")
        # tissue_data['Tumor'] is already an all-False array of the right shape here
        return False

    logger.info(f"Initialized active tumor seed (strategy: {seed_strategy}) within Tumor_Max_Extent: "
                f"center_vox={center_vox_ijk}, radius_vox={radius_vox}, "
                f"num_active_tumor_voxels={np.sum(tissue_data['Tumor'])}")
    return True

def update_tumor_rim_and_core(tissue_data: dict, config: dict):
    """Identifies tumor rim and core based on current active tissue_data['Tumor'] mask."""
    active_tumor_mask_local = tissue_data.get('Tumor') # Get the array first
    if active_tumor_mask_local is None or not np.any(active_tumor_mask_local): # Check if None or all False
        # Ensure these keys exist even if tumor is empty, to prevent KeyErrors later
        if 'shape' in tissue_data and tissue_data['shape'] is not None:
            tissue_data['tumor_rim_mask'] = np.zeros(tissue_data['shape'], dtype=bool)
            tissue_data['tumor_core_mask'] = np.zeros(tissue_data['shape'], dtype=bool)
        else:
            logger.error("Cannot initialize empty rim/core masks as tissue_data['shape'] is missing.")
        return

    # Now we know active_tumor_mask_local is a valid NumPy array with at least one True value
    active_tumor_mask_bool = active_tumor_mask_local.astype(bool) # Ensure boolean for morphology operations
    params = config_manager.get_param(config, "tumor_angiogenesis.tumor_morphology", {})
    rim_thickness = params.get("rim_thickness_voxels", DEFAULT_RIM_THICKNESS_VOXELS)

    if rim_thickness <= 0: # No rim, all core (or all rim if tumor is very small and erosion makes it disappear)
        # If no rim thickness, the whole active tumor is considered rim for VEGF production,
        # and there's no distinct core by erosion.
        # Or, another interpretation: all is core. Let's assume all is rim for VEGF.
        eroded_core = np.zeros_like(active_tumor_mask_bool)
        # If tumor is smaller than rim_thickness, erosion might make it disappear.
        # In such a case, consider the whole tumor as rim.
        if np.sum(active_tumor_mask_bool) > 0 and np.sum(binary_erosion(active_tumor_mask_bool, iterations=rim_thickness, border_value=0)) == 0:
             # If erosion results in nothing, but tumor exists, the whole thing is effectively rim
             tissue_data['tumor_rim_mask'] = active_tumor_mask_bool.copy()
             tissue_data['tumor_core_mask'] = np.zeros_like(active_tumor_mask_bool)
        else:
             eroded_core = binary_erosion(active_tumor_mask_bool, iterations=rim_thickness, border_value=0)
             tissue_data['tumor_core_mask'] = eroded_core
             tissue_data['tumor_rim_mask'] = active_tumor_mask_bool & (~eroded_core)

    else: # rim_thickness > 0
        eroded_core = binary_erosion(active_tumor_mask_bool, iterations=rim_thickness, border_value=0)
        tissue_data['tumor_core_mask'] = eroded_core
        tissue_data['tumor_rim_mask'] = active_tumor_mask_bool & (~eroded_core)
    
    logger.debug(f"Updated tumor rim ({np.sum(tissue_data.get('tumor_rim_mask', np.array([])))} vox) and core ({np.sum(tissue_data.get('tumor_core_mask', np.array([])))} vox).")

def update_metabolic_demand_for_tumor(tissue_data: dict, config: dict):
    """Updates metabolic_demand_map based on tumor rim and core."""
    active_tumor_mask_local = tissue_data.get('Tumor') # Get the array first
    if active_tumor_mask_local is None or not np.any(active_tumor_mask_local): # Check if None or all False
        # No active tumor, so no specific tumor metabolic demand to set.
        # The metabolic_demand_map should reflect healthy tissue or be zero in these areas.
        logger.debug("No active tumor present; skipping tumor-specific metabolic demand update.")
        return

    # Ensure metabolic_demand_map exists and has the correct shape
    if 'metabolic_demand_map' not in tissue_data or \
       tissue_data.get('metabolic_demand_map') is None or \
       tissue_data['metabolic_demand_map'].shape != tissue_data['shape']:
        # If it's missing or wrong shape, it should have been initialized correctly in load_initial_data
        # or after the first tumor growth step. For safety, ensure it's there.
        if 'shape' in tissue_data and tissue_data['shape'] is not None:
            tissue_data['metabolic_demand_map'] = np.zeros(tissue_data['shape'], dtype=float)
            logger.warning("Re-initialized metabolic_demand_map during tumor demand update due to mismatch or absence.")
        else:
            logger.error("Cannot update metabolic demand for tumor: tissue_data['shape'] is missing.")
            return


    rates = config_manager.get_param(config, "tissue_properties.metabolic_rates", {})
    q_met_rim = rates.get("tumor_rim", constants.Q_MET_TUMOR_RIM_PER_ML)
    q_met_core = rates.get("tumor_core", constants.Q_MET_TUMOR_CORE_PER_ML)
    voxel_vol = tissue_data['voxel_volume']

    # metabolic_demand_map should already exist and be initialized (e.g. with healthy demands)
    # We are now OVERWRITING the demand in tumor regions.

    if tissue_data.get('tumor_rim_mask') is not None and np.any(tissue_data['tumor_rim_mask']):
        tissue_data['metabolic_demand_map'][tissue_data['tumor_rim_mask']] = q_met_rim * voxel_vol
    
    if tissue_data.get('tumor_core_mask') is not None and np.any(tissue_data['tumor_core_mask']):
        tissue_data['metabolic_demand_map'][tissue_data['tumor_core_mask']] = q_met_core * voxel_vol
    
    # What about active tumor voxels that are neither rim nor core (e.g., if rim_thickness is 0 or tumor is tiny)?
    # The current update_tumor_rim_and_core logic tries to ensure rim+core covers the active tumor.
    # If there are any 'Tumor' voxels not covered by rim or core (shouldn't happen with current logic),
    # their metabolic rate would remain unchanged (i.e., healthy rate).
    # This is generally fine, as rim/core should define the tumor's metabolic activity.

    logger.debug("Updated metabolic demand map for active tumor regions (rim/core).")

def update_vegf_field_rim_driven(tissue_data: dict, config: dict) -> bool:
    """VEGF produced primarily by the tumor rim. Updates tissue_data['VEGF_field']."""
    tumor_rim_mask_local = tissue_data.get('tumor_rim_mask') # Get the array first
    if tumor_rim_mask_local is None or not np.any(tumor_rim_mask_local): # Check if None or all False
        logger.debug("No tumor rim to produce VEGF. Setting VEGF field to zero.")
        # Ensure VEGF_field exists even if empty
        if 'shape' in tissue_data and tissue_data['shape'] is not None:
            tissue_data['VEGF_field'] = np.zeros(tissue_data['shape'], dtype=float)
        else:
            logger.error("Cannot initialize empty 'VEGF_field' as tissue_data['shape'] is missing.")
        return True # Or False if this state is considered an error for VEGF generation

    # Now we know tumor_rim_mask_local is a valid NumPy array with at least one True value
    vegf_config = config_manager.get_param(config, "tumor_angiogenesis.vegf_settings", {})
    vegf_prod_rim = vegf_config.get("production_rate_rim", DEFAULT_VEGF_PRODUCTION_RIM)
    
    vegf_field = np.zeros(tissue_data['shape'], dtype=float) # Initialize with correct shape
    vegf_field[tumor_rim_mask_local] = vegf_prod_rim # Use the fetched local variable
    
    # Optional: Core contribution (if you add this logic back)
    # vegf_prod_core = vegf_config.get("production_rate_core", DEFAULT_VEGF_PRODUCTION_CORE)
    # tumor_core_mask_local = tissue_data.get('tumor_core_mask')
    # if tumor_core_mask_local is not None and np.any(tumor_core_mask_local):
    #     vegf_field[tumor_core_mask_local] += vegf_prod_core # Additive or max?

    if vegf_config.get("apply_diffusion_blur", True):
        sigma = vegf_config.get("diffusion_blur_sigma", 2.5)
        if sigma > 0 and np.any(vegf_field): # Only blur if there's something to blur
            try:
                vegf_field = gaussian_filter(vegf_field, sigma=sigma)
                logger.debug(f"Applied Gaussian blur (sigma={sigma}) to VEGF field.")
            except Exception as e: # Catch potential errors from gaussian_filter if field is weird
                logger.warning(f"Could not apply Gaussian blur to VEGF field: {e}")
    
    tissue_data['VEGF_field'] = vegf_field
    logger.info(f"Updated rim-driven VEGF field. Max VEGF: {np.max(vegf_field) if np.any(vegf_field) else 0:.2e}")
    return True

def grow_tumor_mass_within_defined_segmentation(tissue_data: dict, config: dict) -> bool:
    tumor_growth_params = config_manager.get_param(config, "tumor_angiogenesis.tumor_growth", {})
    expansion_voxels_per_step = tumor_growth_params.get("expansion_voxels_per_step", 100)
    
    # Get masks
    current_active_tumor_mask = tissue_data.get('Tumor')
    tumor_max_extent_mask = tissue_data.get('Tumor_Max_Extent')
    active_tumor_rim_mask = tissue_data.get('tumor_rim_mask') # This is calculated based on 'Tumor'

    # Check for None before np.any or other operations
    if current_active_tumor_mask is None or \
       tumor_max_extent_mask is None or \
       active_tumor_rim_mask is None: # active_tumor_rim_mask could be None if 'Tumor' was None when it was calculated
        logger.error("grow_tumor_mass: One or more required masks (Tumor, Tumor_Max_Extent, tumor_rim_mask) is None.")
        return False

    if np.all(current_active_tumor_mask == tumor_max_extent_mask): # This is fine, compares two arrays
        logger.info("Tumor Growth: Active tumor has filled 'Tumor_Max_Extent'.")
        return False
    
    # Determine source for dilation based on rim or whole active tumor
    source_for_dilation = active_tumor_rim_mask if np.any(active_tumor_rim_mask) else current_active_tumor_mask
    if not np.any(source_for_dilation): # Correctly checks if the chosen source is empty
        logger.info("Tumor Growth: Source for dilation (rim or active tumor) is empty. Cannot grow.")
        return False # Added this return

    # Ensure domain_mask is also valid before using it in the bitwise AND
    domain_mask_for_growth = tissue_data.get('domain_mask')
    if domain_mask_for_growth is None:
        logger.error("grow_tumor_mass: 'domain_mask' is None. Cannot determine growth candidates.")
        return False

    growth_candidates_mask = binary_dilation(source_for_dilation) & \
                             (~current_active_tumor_mask) & \
                             tumor_max_extent_mask & \
                             domain_mask_for_growth # Use the fetched domain_mask

    candidate_voxel_indices = np.array(np.where(growth_candidates_mask)).T
    if candidate_voxel_indices.shape[0] == 0:
        logger.info("Tumor Growth: No suitable healthy voxels for expansion within constraints.")
        return False
        
    num_to_convert = min(expansion_voxels_per_step, candidate_voxel_indices.shape[0])
    if num_to_convert > 0:
        chosen_indices_idx = np.random.choice(candidate_voxel_indices.shape[0], num_to_convert, replace=False)
        voxels_to_add = candidate_voxel_indices[chosen_indices_idx]
        
        new_active_tumor_mask = current_active_tumor_mask.copy()
        gm_mask = tissue_data.get('GM') # Fetch GM and WM once
        wm_mask = tissue_data.get('WM')

        for vox_idx_tuple in map(tuple, voxels_to_add):
            new_active_tumor_mask[vox_idx_tuple] = True
            if gm_mask is not None and utils.is_voxel_in_bounds(vox_idx_tuple, gm_mask.shape) and gm_mask[vox_idx_tuple]: # Check bounds for safety
                gm_mask[vox_idx_tuple] = False
            if wm_mask is not None and utils.is_voxel_in_bounds(vox_idx_tuple, wm_mask.shape) and wm_mask[vox_idx_tuple]: # Check bounds for safety
                wm_mask[vox_idx_tuple] = False
        
        tissue_data['Tumor'] = new_active_tumor_mask
        # Update GM and WM in tissue_data if they were modified
        if gm_mask is not None: tissue_data['GM'] = gm_mask
        if wm_mask is not None: tissue_data['WM'] = wm_mask

        logger.info(f"Tumor Growth: Expanded by {num_to_convert} vox. Active: {np.sum(new_active_tumor_mask)}, Max: {np.sum(tumor_max_extent_mask)}")
        return True
    return False

def coopt_and_modify_vessels(graph: nx.DiGraph, tissue_data: dict, config: dict):
    active_tumor_mask_local = tissue_data.get('Tumor') # Get the array
    if active_tumor_mask_local is None or not np.any(active_tumor_mask_local): # Corrected check
        logger.debug("Co-option: No active tumor to co-opt vessels from.")
        return

    affine = tissue_data.get('affine')
    if affine is None:
        logger.error("Co-option: Affine matrix not found in tissue_data. Cannot perform co-option.")
        return

    cooption_params = config_manager.get_param(config, "tumor_angiogenesis.cooption", {})
    radius_dilation_factor_mean = cooption_params.get("radius_dilation_factor_mean", 1.1)
    radius_dilation_factor_std = cooption_params.get("radius_dilation_factor_std", 0.05)
    permeability_Lp_tumor_factor = cooption_params.get("permeability_Lp_factor_tumor", 10.0)
    
    nodes_coopted_this_step: Set[str] = set()
    for node_id, data in graph.nodes(data=True):
        if data.get('is_tumor_vessel', False): continue
        
        node_pos = data.get('pos')
        if node_pos is None: continue # Skip nodes without position

        pos_vox_int = np.round(utils.world_to_voxel(node_pos, affine)).astype(int)
        if utils.is_voxel_in_bounds(pos_vox_int, active_tumor_mask_local.shape) and \
           active_tumor_mask_local[tuple(pos_vox_int)]:
            nodes_coopted_this_step.add(node_id)
            
    if not nodes_coopted_this_step:
        logger.debug("Co-option: No new healthy vessels found within active tumor for co-option this step.")
        return

    for node_id in nodes_coopted_this_step:
        node_data = graph.nodes[node_id] # Get node data once
        node_data['is_tumor_vessel'] = True
        node_data['vessel_origin_type'] = 'coopted_healthy'
        
        original_radius = node_data.get('radius', constants.MIN_VESSEL_RADIUS_MM) # Use default if radius missing
        dilation = max(0.5, np.random.normal(radius_dilation_factor_mean, radius_dilation_factor_std))
        new_radius = max(constants.MIN_VESSEL_RADIUS_MM, original_radius * dilation)
        node_data['radius'] = new_radius
        # logger.debug(f"Co-opted node {node_id}. Type: {node_data.get('type')}->coopted_healthy. R: {original_radius:.4f}->{new_radius:.4f}") # Already logged in main loop

        # Mark connected edges and update their radii if they originate from this node
        for u, v, edge_data in graph.out_edges(node_id, data=True): # Edges where this node is the source
            edge_data['is_tumor_vessel'] = True
            edge_data['radius'] = new_radius # Edge takes radius of its (now co-opted) source node
            edge_data['permeability_Lp_factor'] = permeability_Lp_tumor_factor
        for u, v, edge_data in graph.in_edges(node_id, data=True): # Edges where this node is the target
            edge_data['is_tumor_vessel'] = True # The segment is now within tumor influence
            edge_data['permeability_Lp_factor'] = permeability_Lp_tumor_factor
            # Radius of incoming edge is determined by its source node (u), which might also get co-opted

    logger.info(f"Co-opted and modified {len(nodes_coopted_this_step)} nodes and their adjacent edges.")

def find_angiogenic_sprouting_candidates(graph: nx.DiGraph, tissue_data: dict, config: dict) -> List[Tuple[str, str, np.ndarray, np.ndarray]]:
    candidates = []
    vegf_field_local = tissue_data.get('VEGF_field') # Get the array
    if vegf_field_local is None or not np.any(vegf_field_local): # Corrected check
        logger.debug("Sprouting candidates: No VEGF field or VEGF field is all zero.")
        return candidates

    affine = tissue_data.get('affine')
    if affine is None:
        logger.error("Sprouting candidates: Affine matrix not found in tissue_data.")
        return candidates

    sprouting_params = config_manager.get_param(config, "tumor_angiogenesis.sprouting", {})
    min_vegf = sprouting_params.get("min_vegf_concentration", 0.2)
    min_parent_r = sprouting_params.get("min_parent_vessel_radius_mm", 0.02)
    
    grad_ax0, grad_ax1, grad_ax2 = np.gradient(vegf_field_local) # Use local copy

    for u, v, edge_data in graph.edges(data=True):
        node_u_data = graph.nodes[u]
        node_v_data = graph.nodes[v] # Get v data too for position

        parent_radius = node_u_data.get('radius', 0) # Use default 0 if radius missing
        if parent_radius < min_parent_r: 
            continue
        
        pos_u = node_u_data.get('pos')
        pos_v = node_v_data.get('pos')
        if pos_u is None or pos_v is None: continue # Skip if positions are missing

        sprout_origin = (pos_u + pos_v) / 2.0
        sprout_vox_int = np.round(utils.world_to_voxel(sprout_origin, affine)).astype(int)

        if not utils.is_voxel_in_bounds(sprout_vox_int, vegf_field_local.shape): 
            continue
            
        if vegf_field_local[tuple(sprout_vox_int)] >= min_vegf:
            g_ax0 = grad_ax0[tuple(sprout_vox_int)]
            g_ax1 = grad_ax1[tuple(sprout_vox_int)]
            g_ax2 = grad_ax2[tuple(sprout_vox_int)]
            sprout_dir_vox = np.array([g_ax0, g_ax1, g_ax2])
            
            # Transform gradient to world space direction
            # affine[:3,:3] is the rotation/scaling part
            sprout_dir_world = utils.normalize_vector(affine[:3,:3] @ sprout_dir_vox) 

            if np.linalg.norm(sprout_dir_world) > constants.EPSILON:
                candidates.append((u, v, sprout_origin, sprout_dir_world))
    
    max_sprouts = sprouting_params.get("max_new_sprouts_per_iteration", 5)
    if len(candidates) > max_sprouts:
        indices = np.random.choice(len(candidates), max_sprouts, replace=False)
        selected_candidates = [candidates[i] for i in indices]
        logger.info(f"Selected {len(selected_candidates)} sprouts from {len(candidates)} original candidates.")
        return selected_candidates
    elif candidates: 
        logger.info(f"Found {len(candidates)} sprouting candidates.")
    else:
        logger.debug("No sprouting candidates found this step.")
    return candidates


def attempt_anastomosis_tip_to_segment(
    term_gbo: GBOIterationData,
    graph: nx.DiGraph, # The main angiogenic graph
    vessel_kdtree: KDTree, # KDTree of (positions of midpoints of all non-parent segments)
    segment_midpoints_data: List[Dict], # List of {'pos': mid_pos, 'u': seg_u, 'v': seg_v, 'radius': seg_radius}
    config: dict,
    next_synthetic_node_id_ref: List[int] # Pass as list to modify in place
) -> bool:
    """
    Attempts to anastomose the given angiogenic terminal (term_gbo) to a nearby existing segment.
    Modifies graph and term_gbo if successful.
    Returns True if anastomosis occurred, False otherwise.
    """
    anastomosis_params = config_manager.get_param(config, "tumor_angiogenesis.anastomosis", {})
    search_radius = term_gbo.radius * anastomosis_params.get("search_radius_factor", 3.0)
    min_angle_deg = anastomosis_params.get("min_fusion_angle_deg", 120.0) # Angle between tip's last segment and segment to target
    max_dist_to_midpoint_factor = anastomosis_params.get("max_dist_to_midpoint_factor", 1.5) # Tip must be close to midpoint

    if vessel_kdtree is None or not segment_midpoints_data: return False

    nearby_indices = vessel_kdtree.query_ball_point(term_gbo.pos, r=search_radius)
    if not nearby_indices: return False

    parent_of_tip_pos = graph.nodes[term_gbo.parent_id]['pos']
    tip_growth_vector = term_gbo.pos - parent_of_tip_pos # Vector of the last segment of the tip

    best_target_seg_info = None
    min_dist_sq = float('inf')

    for idx in nearby_indices:
        target_seg = segment_midpoints_data[idx]
        target_midpoint = target_seg['pos']
        target_u, target_v = target_seg['u'], target_seg['v']

        # Avoid self-anastomosis or anastomosis with immediate parent segment from bifurcation
        if term_gbo.parent_id == target_u or term_gbo.parent_id == target_v: continue
        # Avoid if target is the segment the tip just grew from (if parent_id was a midpoint)
        # This check needs to be more robust if parent_id can be a segment point.
        # For now, assume parent_id is a node.

        dist_sq = utils.distance_squared(term_gbo.pos, target_midpoint)
        if dist_sq < min_dist_sq and dist_sq < (term_gbo.radius * max_dist_to_midpoint_factor)**2 :
            # Check angle: vector from tip to target_midpoint vs. tip_growth_vector
            vec_tip_to_target = target_midpoint - term_gbo.pos
            if np.linalg.norm(tip_growth_vector) > constants.EPSILON and np.linalg.norm(vec_tip_to_target) > constants.EPSILON:
                cos_angle = np.dot(tip_growth_vector, vec_tip_to_target) / \
                            (np.linalg.norm(tip_growth_vector) * np.linalg.norm(vec_tip_to_target))
                angle_deg = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))
                if angle_deg >= min_angle_deg: # Tip should be "aiming away" or sideways relative to target for good fusion
                    min_dist_sq = dist_sq
                    best_target_seg_info = target_seg
    
    if best_target_seg_info:
        target_u = best_target_seg_info['u']
        target_v = best_target_seg_info['v']
        target_midpoint_pos = best_target_seg_info['pos'] # This will be the new anastomosis node

        logger.info(f"Anastomosis: Tip {term_gbo.id} (parent {term_gbo.parent_id}) fusing with segment {target_u}-{target_v} at {np.round(target_midpoint_pos,2)}.")

        # 1. Create new anastomosis node at target_midpoint_pos
        anastomosis_node_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
        # Radius of anastomosis node can be average or based on fusing vessels
        fused_radius = (term_gbo.radius + best_target_seg_info['radius']) / 2.0
        data_structures.add_node_to_graph(graph, anastomosis_node_id, pos=target_midpoint_pos, radius=fused_radius,
                                          type='anastomosis_point', is_tumor_vessel=True, vessel_origin_type='anastomosis')

        # 2. Remove old target segment (target_u, target_v)
        original_target_edge_data = graph.edges[target_u, target_v].copy() # Assuming directed u->v
        graph.remove_edge(target_u, target_v)

        # 3. Add new segments: target_u -> anastomosis_node, anastomosis_node -> target_v
        data_structures.add_edge_to_graph(graph, target_u, anastomosis_node_id, **original_target_edge_data) # Update length/radius
        graph.edges[target_u, anastomosis_node_id]['length'] = utils.distance(graph.nodes[target_u]['pos'], target_midpoint_pos)
        graph.edges[target_u, anastomosis_node_id]['radius'] = graph.nodes[target_u]['radius'] # Takes radius of upstream node

        data_structures.add_edge_to_graph(graph, anastomosis_node_id, target_v, **original_target_edge_data) # Update length/radius
        graph.edges[anastomosis_node_id, target_v]['length'] = utils.distance(target_midpoint_pos, graph.nodes[target_v]['pos'])
        graph.edges[anastomosis_node_id, target_v]['radius'] = fused_radius # Takes radius of new anastomosis node

        # 4. Connect the parent of the fusing tip to the anastomosis_node
        # The segment was (term_gbo.parent_id) -> term_gbo.id (which is at term_gbo.pos)
        # New segment is (term_gbo.parent_id) -> anastomosis_node_id
        tip_parent_node_id = term_gbo.parent_id
        data_structures.add_edge_to_graph(graph, tip_parent_node_id, anastomosis_node_id,
                                          length=utils.distance(graph.nodes[tip_parent_node_id]['pos'], target_midpoint_pos),
                                          radius=graph.nodes[tip_parent_node_id]['radius'], # Or fused_radius?
                                          type='angiogenic_segment', is_tumor_vessel=True,
                                          permeability_Lp_factor=graph.edges[tip_parent_node_id, term_gbo.id].get('permeability_Lp_factor'))


        # 5. Remove the old tip node (term_gbo.id) and its incoming segment
        graph.remove_edge(tip_parent_node_id, term_gbo.id)
        graph.remove_node(term_gbo.id)
        
        term_gbo.stop_growth = True # Mark this GBOIterationData as done
        return True
    return False


# --- Main Angiogenesis Orchestration ---
def simulate_tumor_angiogenesis_fixed_extent(
    config: dict,
    tissue_data: dict,
    base_vascular_tree: nx.DiGraph,
    output_dir: str,
    perfusion_solver_func: Callable[[nx.DiGraph, dict, Optional[float], Optional[Dict[str, float]]], Optional[nx.DiGraph]]
) -> nx.DiGraph:
    logger.info("--- Starting Tumor Angiogenesis Simulation (Growing within Fixed Extent) ---")
    
    main_params = config_manager.get_param(config, "tumor_angiogenesis", {})
    num_macro_iterations = main_params.get("num_macro_iterations", 20)
    tumor_growth_steps_per_macro = main_params.get("tumor_growth.steps_per_macro_iter", 1)
    angiogenesis_steps_per_macro = main_params.get("angiogenesis.steps_per_macro_iter", 3)
    flow_solve_interval_macro = main_params.get("flow_solve_interval_macro_iters", 5)
    save_interval_macro = main_params.get("save_intermediate_interval_macro_iters", 1)

    sprouting_params = main_params.get("sprouting", {})
    initial_sprout_radius = sprouting_params.get("initial_sprout_radius_mm", constants.MIN_VESSEL_RADIUS_MM * 1.1)
    initial_sprout_length = sprouting_params.get("initial_sprout_length_mm", initial_sprout_radius * 4)
    
    extension_params = main_params.get("extension", {})
    extension_step_length = extension_params.get("step_length_mm", initial_sprout_radius * 2)
    
    branching_params = main_params.get("angiogenic_branching", {})
    branch_probability_factor_vegf = branching_params.get("branch_probability_factor_vegf", 0.1) # Prob = factor * vegf_norm
    branch_angle_spread_deg = branching_params.get("branch_angle_spread_deg", 60.0)


    if not initialize_active_tumor_from_seed(tissue_data, config):
        logger.error("Failed to initialize active tumor seed. Aborting angiogenesis.")
        return base_vascular_tree.copy()

    angiogenic_graph = base_vascular_tree.copy()
    # Pass next_synthetic_node_id as a list so its modification is seen by caller
    max_id_num = 0
    for node_id_str_val in angiogenic_graph.nodes():
        if isinstance(node_id_str_val, str) and node_id_str_val.startswith('s_'):
            try: max_id_num = max(max_id_num, int(node_id_str_val.split('_')[1]))
            except (ValueError, IndexError): pass
    next_synthetic_node_id_ref = [max_id_num + 10000] # List to pass by reference
    
    active_angiogenic_terminals: List[GBOIterationData] = []

    for macro_iter in range(num_macro_iterations):
        logger.info(f"===== Macro Iteration {macro_iter + 1} / {num_macro_iterations} =====")

        any_tumor_growth_this_macro = False
        for _ in range(tumor_growth_steps_per_macro):
            update_tumor_rim_and_core(tissue_data, config)
            if grow_tumor_mass_within_defined_segmentation(tissue_data, config): any_tumor_growth_this_macro = True
            else: break 
        
        update_tumor_rim_and_core(tissue_data, config); update_metabolic_demand_for_tumor(tissue_data, config)
        update_vegf_field_rim_driven(tissue_data, config)
        coopt_and_modify_vessels(angiogenic_graph, tissue_data, config)

        for ag_step in range(angiogenesis_steps_per_macro):
            logger.debug(f"  Angiogenesis Step {ag_step + 1} (Active Tips Before Sprouting: {len(active_angiogenic_terminals)})")
            
            # --- 2a. Sprouting ---
            new_sprouts_info = find_angiogenic_sprouting_candidates(angiogenic_graph, tissue_data, config)
            if new_sprouts_info:
                logger.debug(f"    Found {len(new_sprouts_info)} new sprout candidates this AG step.")

            for parent_u, parent_v, sprout_origin, sprout_dir in new_sprouts_info:
                bif_node_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
                
                parent_u_data = angiogenic_graph.nodes[parent_u]
                parent_v_data = angiogenic_graph.nodes[parent_v] # Get parent_v data as well
                parent_u_radius = parent_u_data['radius']
                
                # Determine vessel_origin_type for the new bifurcation node
                # If parent_u was already a tumor vessel (coopted or angiogenic), the bif is too.
                # Otherwise, it's a bifurcation on a healthy vessel that's now leading to tumor growth.
                bif_origin_type = parent_u_data.get('vessel_origin_type', 'healthy_parent_of_sprout')
                if parent_u_data.get('is_tumor_vessel'):
                    bif_origin_type = parent_u_data.get('vessel_origin_type', 'coopted_healthy') # Default if type was missing

                data_structures.add_node_to_graph(
                    angiogenic_graph, bif_node_id,
                    pos=sprout_origin,
                    radius=parent_u_radius, # Bifurcation point takes radius of the parent segment it's on
                    type='angiogenic_bifurcation',
                    is_tumor_vessel=True, # The bifurcation itself is part of the tumor response
                    vessel_origin_type=bif_origin_type
                )
                
                # Get original edge data before removing
                # Important: Check if edge still exists, could have been modified by another sprout from same segment in same AG step (unlikely but good check)
                if not angiogenic_graph.has_edge(parent_u, parent_v):
                    logger.warning(f"Sprouting target edge {parent_u}-{parent_v} no longer exists. Skipping this sprout.")
                    # Rollback bif_node_id? Or just let it be an isolated node that might get pruned.
                    # For now, continue, but this indicates a potential complex interaction.
                    # To properly handle, would need to process sprouts sequentially and update graph immediately.
                    # Current find_angiogenic_sprouting_candidates finds all then processes.
                    if angiogenic_graph.has_node(bif_node_id): angiogenic_graph.remove_node(bif_node_id) # Clean up unused bif
                    next_synthetic_node_id_ref[0] -=1 # Decrement counter
                    continue

                edge_data_orig = angiogenic_graph.edges[parent_u, parent_v].copy()
                angiogenic_graph.remove_edge(parent_u, parent_v)

                # Add new segments: parent_u -> bif_node_id and bif_node_id -> parent_v
                # These new segments inherit properties and get updated lengths/radii
                data_structures.add_edge_to_graph(angiogenic_graph, parent_u, bif_node_id, **edge_data_orig)
                angiogenic_graph.edges[parent_u, bif_node_id]['length'] = utils.distance(parent_u_data['pos'], sprout_origin)
                angiogenic_graph.edges[parent_u, bif_node_id]['radius'] = parent_u_radius # Takes radius of parent_u

                data_structures.add_edge_to_graph(angiogenic_graph, bif_node_id, parent_v, **edge_data_orig)
                angiogenic_graph.edges[bif_node_id, parent_v]['length'] = utils.distance(sprout_origin, parent_v_data['pos'])
                angiogenic_graph.edges[bif_node_id, parent_v]['radius'] = parent_u_radius # New segment from bif also takes bif radius

                # Mark these split segments as tumor vessels if the original parent was, or if the bif is
                # The bifurcation node itself is marked is_tumor_vessel=True
                # Segments connected to it that are part of the original path should also be marked.
                perm_factor_to_set = default_permeability_factor # Default for new tumor-related segments
                if parent_u_data.get('is_tumor_vessel'):
                    # If parent_u was already a tumor vessel, its perm factor might be already set
                    perm_factor_to_set = edge_data_orig.get('permeability_Lp_factor', default_permeability_factor)

                for e_start, e_end in [(parent_u, bif_node_id), (bif_node_id, parent_v)]:
                    angiogenic_graph.edges[e_start, e_end]['is_tumor_vessel'] = True # Part of the angiogenic event path
                    angiogenic_graph.edges[e_start, e_end]['permeability_Lp_factor'] = perm_factor_to_set


                # Create the new angiogenic sprout (terminal node and its GBOIterationData)
                sprout_tip_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
                sprout_tip_pos = sprout_origin + sprout_dir * initial_sprout_length
                
                # Initial flow for angiogenic sprout can be very small or based on a minimal tumor demand
                sprout_initial_flow = DEFAULT_MIN_TUMOR_TERMINAL_DEMAND 
                
                sprout_gbo = GBOIterationData(
                    terminal_id=sprout_tip_id,
                    pos=sprout_tip_pos,
                    radius=initial_sprout_radius,
                    flow=sprout_initial_flow, 
                    parent_id=bif_node_id
                )
                sprout_gbo.length_from_parent = initial_sprout_length
                active_angiogenic_terminals.append(sprout_gbo)

                data_structures.add_node_to_graph(
                    angiogenic_graph, sprout_tip_id,
                    pos=sprout_tip_pos,
                    radius=initial_sprout_radius,
                    type='angiogenic_terminal',
                    is_tumor_vessel=True,
                    vessel_origin_type='angiogenic_sprout', # Clearly mark its origin
                    parent_id=bif_node_id, 
                    Q_flow=sprout_gbo.flow
                )
                data_structures.add_edge_to_graph(
                    angiogenic_graph, bif_node_id, sprout_tip_id,
                    length=initial_sprout_length,
                    radius=initial_sprout_radius, # Edge to new tip takes tip's radius
                    type='angiogenic_segment',
                    is_tumor_vessel=True,
                    permeability_Lp_factor=default_permeability_factor # New angiogenic segments are leaky
                )
                logger.debug(f"    Created new sprout: {sprout_tip_id} from new bif {bif_node_id} on original edge {parent_u}-{parent_v}.")
            
            
            # --- Growth of Active Angiogenic Terminals ---
            next_iter_active_terminals_this_ag_step = []
            
            # Prepare KDTree for anastomosis (only if there are terminals and potential targets)
            vessel_kdtree = None
            segment_midpoints_data = []
            if active_angiogenic_terminals and angiogenic_graph.number_of_edges() > 0:
                midpoints_pos_list = []
                for u, v, data in angiogenic_graph.edges(data=True):
                    # Exclude very new segments connected to active tips to avoid self-anastomosis with parent segment immediately
                    # This check might need refinement
                    is_parent_of_active_tip = any(term.parent_id == u and term.id == v for term in active_angiogenic_terminals)
                    if not is_parent_of_active_tip:
                        mid_pos = (angiogenic_graph.nodes[u]['pos'] + angiogenic_graph.nodes[v]['pos']) / 2.0
                        midpoints_pos_list.append(mid_pos)
                        segment_midpoints_data.append({'pos': mid_pos, 'u': u, 'v': v, 'radius': data.get('radius', constants.MIN_VESSEL_RADIUS_MM)})
                if midpoints_pos_list:
                    vessel_kdtree = KDTree(np.array(midpoints_pos_list))

            newly_branched_terminals_this_ag_step = [] # To hold children from branching
            for term_gbo in active_angiogenic_terminals:
                if term_gbo.stop_growth: continue
                
                # Attempt Anastomosis
                if attempt_anastomosis_tip_to_segment(term_gbo, angiogenic_graph, vessel_kdtree, segment_midpoints_data, config, next_synthetic_node_id_ref):
                    # term_gbo.stop_growth is set by the function
                    continue # Fused, so process next terminal

                # Attempt Branching (Simplified Stochastic)
                current_pos_vox_int = np.round(utils.world_to_voxel(term_gbo.pos, tissue_data['affine'])).astype(int)
                if not utils.is_voxel_in_bounds(current_pos_vox_int, tissue_data['VEGF_field'].shape):
                    term_gbo.stop_growth = True; continue
                
                vegf_at_tip = tissue_data['VEGF_field'][tuple(current_pos_vox_int)]
                normalized_vegf = vegf_at_tip / (np.max(tissue_data['VEGF_field']) + constants.EPSILON)
                prob_branch = branch_probability_factor_vegf * normalized_vegf
                
                if np.random.rand() < prob_branch:
                    logger.debug(f"Angiogenic terminal {term_gbo.id} branching (VEGF: {vegf_at_tip:.2f}, P_branch: {prob_branch:.2f})")
                    # Change current terminal to bifurcation
                    angiogenic_graph.nodes[term_gbo.id]['type'] = 'angiogenic_bifurcation'
                    # Create two new child GBOIterationData objects
                    parent_growth_dir = utils.normalize_vector(term_gbo.pos - angiogenic_graph.nodes[term_gbo.parent_id]['pos'])
                    
                    for i_child in range(2):
                        child_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
                        # Perturb direction slightly
                        angle_offset = np.deg2rad(np.random.uniform(-branch_angle_spread_deg/2, branch_angle_spread_deg/2))
                        # This is a 2D rotation logic, needs proper 3D random vector perturbation
                        # For simplicity, create a random perturbation and add to parent_growth_dir then renormalize
                        random_perturb = utils.normalize_vector(np.random.rand(3) - 0.5) * 0.5 # Scale of perturbation
                        child_dir = utils.normalize_vector(parent_growth_dir + random_perturb)
                        if np.linalg.norm(child_dir) < constants.EPSILON: child_dir = parent_growth_dir # Fallback

                        child_pos = term_gbo.pos + child_dir * extension_step_length # Initial small extension
                        child_radius = term_gbo.radius # Or slightly smaller
                        child_flow = term_gbo.flow / 2 # Split flow (very rough)

                        child_gbo = GBOIterationData(child_id, child_pos, child_radius, child_flow, parent_id=term_gbo.id)
                        child_gbo.length_from_parent = extension_step_length
                        newly_branched_terminals_this_ag_step.append(child_gbo)

                        data_structures.add_node_to_graph(angiogenic_graph, child_id, pos=child_pos, radius=child_radius,
                                                          type='angiogenic_terminal', is_tumor_vessel=True, vessel_origin_type='angiogenic_sprout',
                                                          parent_id=term_gbo.id, Q_flow=child_flow)
                        data_structures.add_edge_to_graph(angiogenic_graph, term_gbo.id, child_id, length=extension_step_length,
                                                          radius=child_radius, type='angiogenic_segment', is_tumor_vessel=True,
                                                          permeability_Lp_factor=cooption_params.get("permeability_Lp_factor_tumor", 10.0))
                    term_gbo.stop_growth = True # Parent tip stops, children take over
                    continue

                # Extension (if not anastomosed or branched)
                grad_ax0_f, grad_ax1_f, grad_ax2_f = np.gradient(tissue_data['VEGF_field'])
                g_ax0 = grad_ax0_f[tuple(current_pos_vox_int)]; g_ax1 = grad_ax1_f[tuple(current_pos_vox_int)]; g_ax2 = grad_ax2_f[tuple(current_pos_vox_int)]
                growth_dir_vox = np.array([g_ax0, g_ax1, g_ax2])
                growth_dir_world = utils.normalize_vector(tissue_data['affine'][:3,:3] @ growth_dir_vox)

                if np.linalg.norm(growth_dir_world) > constants.EPSILON:
                    new_pos = term_gbo.pos + growth_dir_world * extension_step_length
                    old_tip_id = term_gbo.id
                    angiogenic_graph.nodes[old_tip_id]['type'] = 'angiogenic_segment_point'
                    new_tip_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
                    
                    term_gbo.id = new_tip_id; term_gbo.pos = new_pos; term_gbo.parent_id = old_tip_id
                    term_gbo.length_from_parent = extension_step_length
                    
                    data_structures.add_node_to_graph(angiogenic_graph, new_tip_id, pos=new_pos, radius=term_gbo.radius,
                                                      type='angiogenic_terminal', is_tumor_vessel=True, vessel_origin_type='angiogenic_sprout',
                                                      parent_id=old_tip_id, Q_flow=term_gbo.flow)
                    data_structures.add_edge_to_graph(angiogenic_graph, old_tip_id, new_tip_id, length=extension_step_length,
                                                      radius=term_gbo.radius, type='angiogenic_segment', is_tumor_vessel=True,
                                                      permeability_Lp_factor=cooption_params.get("permeability_Lp_factor_tumor", 10.0))
                    next_iter_active_terminals_this_ag_step.append(term_gbo)
                else:
                    next_iter_active_terminals_this_ag_step.append(term_gbo) # Stalled, keep for next try
            
            active_angiogenic_terminals = [t for t in next_iter_active_terminals_this_ag_step if not t.stop_growth]
            active_angiogenic_terminals.extend(newly_branched_terminals_this_ag_step) # Add new children from branching

            if not active_angiogenic_terminals and not new_sprouts_info : break # from ag_steps
        
        # --- Flow Solve & Adaptation ---
        if (macro_iter + 1) % flow_solve_interval_macro == 0:
            logger.info(f"Running global flow solver and adaptation (Macro Iter {macro_iter + 1})...")
            # ... (Flow solve and differentiated radius adaptation logic - as in previous complete version) ...
            # This part needs careful Q_flow assignment to all terminals (healthy, coopted, angiogenic)
            all_terminals_in_graph = [nid for nid, data in angiogenic_graph.nodes(data=True) if angiogenic_graph.out_degree(nid) == 0 and angiogenic_graph.in_degree(nid) > 0]
            for term_id in all_terminals_in_graph:
                term_node_data = angiogenic_graph.nodes[term_id]
                term_pos_vox = np.round(utils.world_to_voxel(term_node_data['pos'], tissue_data['affine'])).astype(int)
                demand = term_node_data.get('Q_flow', 0.0) # Keep existing Q_flow if not overridden

                if utils.is_voxel_in_bounds(term_pos_vox, tissue_data['shape']):
                    if tissue_data.get('Tumor') is not None and tissue_data['Tumor'][tuple(term_pos_vox)]:
                        # TODO: Better demand based on local tumor voxel demand sum
                        demand = config_manager.get_param(config, "tumor_angiogenesis.min_tumor_terminal_demand", DEFAULT_MIN_TUMOR_TERMINAL_DEMAND)
                    elif not term_node_data.get('is_tumor_vessel'): # Healthy terminal in healthy tissue
                        # This demand should come from GBO's Voronoi refinement for healthy tissue
                        # For now, if not set, use a default. This part needs robust integration with healthy GBO state.
                        demand = term_node_data.get('Q_flow', constants.INITIAL_TERMINAL_FLOW_Q)
                term_node_data['Q_flow'] = demand
            
            temp_graph_for_solver = angiogenic_graph.copy()
            solved_graph = perfusion_solver_func(temp_graph_for_solver, config, None, None)

            if solved_graph:
                angiogenic_graph = solved_graph
                min_r_healthy = config_manager.get_param(config, "vascular_properties.min_radius")
                k_m = config_manager.get_param(config, "vascular_properties.k_murray_scaling_factor")
                m_exp = config_manager.get_param(config, "vascular_properties.murray_law_exponent")
                adapt_params = main_params.get("adaptation", {})
                tumor_radius_factor = adapt_params.get("tumor_radius_factor", 1.0)
                min_r_tumor = adapt_params.get("min_tumor_vessel_radius_mm", min_r_healthy * 1.1)

                for node_id, data in angiogenic_graph.nodes(data=True):
                    if data.get('type') == 'measured_root' or data.get('is_flow_root'): continue
                    
                    actual_node_flow = 0.0 # Recalculate based on solved edge flows
                    if angiogenic_graph.out_degree(node_id) == 0 and angiogenic_graph.in_degree(node_id) > 0: # Sink
                        for _, _, edge_data_in in angiogenic_graph.in_edges(node_id, data=True):
                            actual_node_flow += abs(edge_data_in.get('flow_solver', 0.0))
                    elif angiogenic_graph.out_degree(node_id) > 0: # Source-like or bifurcation
                        for _, _, edge_data_out in angiogenic_graph.out_edges(node_id, data=True):
                            actual_node_flow += abs(edge_data_out.get('flow_solver', 0.0))
                    
                    if abs(actual_node_flow) > constants.EPSILON:
                        target_r = k_m * (abs(actual_node_flow) ** (1.0 / m_exp))
                        if data.get('is_tumor_vessel'):
                            target_r *= tumor_radius_factor
                            data['radius'] = max(min_r_tumor, target_r)
                        else: data['radius'] = max(min_r_healthy, target_r)
                    else: data['radius'] = min_r_tumor if data.get('is_tumor_vessel') else min_r_healthy
                logger.info("Global flow solve and differentiated radius adaptation complete.")


        if (macro_iter + 1) % save_interval_macro == 0:
            # ... (saving logic as before) ...
            logger.info(f"Saving intermediate state for macro iteration {macro_iter + 1}...")
            io_utils.save_vascular_tree_vtp(angiogenic_graph, os.path.join(output_dir, f"angiogenesis_iter_{macro_iter+1}.vtp"))
            io_utils.save_nifti_image(tissue_data['Tumor'].astype(np.uint8), tissue_data['affine'], os.path.join(output_dir, f"active_tumor_mask_iter_{macro_iter+1}.nii.gz"))
            if 'VEGF_field' in tissue_data and tissue_data['VEGF_field'] is not None:
                 io_utils.save_nifti_image(tissue_data['VEGF_field'].astype(np.float32), tissue_data['affine'], os.path.join(output_dir, f"vegf_field_iter_{macro_iter+1}.nii.gz"))

        if np.all(tissue_data['Tumor'] == tissue_data['Tumor_Max_Extent']) and not any_tumor_growth_this_macro:
            logger.info(f"Tumor filled Tumor_Max_Extent. Stopping macro iterations at {macro_iter + 1}.")
            break

    logger.info(f"--- Tumor Angiogenesis (Fixed Extent) Finished. Final graph: {angiogenic_graph.number_of_nodes()} N, {angiogenic_graph.number_of_edges()} E ---")
    return angiogenic_graph# src/utils.py
import numpy as np
import random
import logging
import os
import shutil
from typing import Tuple 

logger = logging.getLogger(__name__)

def set_rng_seed(seed: int):
    """Sets the random seed for Python's random, NumPy, and potentially other libraries."""
    random.seed(seed)
    np.random.seed(seed)
    logger.info(f"Random Number Generator seed set to: {seed}")
    # Add other libraries like TensorFlow/PyTorch if used:
    # tf.random.set_seed(seed)
    # torch.manual_seed(seed)

def get_voxel_volume_from_affine(affine: np.ndarray) -> float:
    """
    Calculates the volume of a single voxel from the NIfTI affine matrix.
    Assumes the affine matrix maps voxel coordinates to physical coordinates.
    The volume is the absolute value of the determinant of the first 3x3 submatrix.

    Args:
        affine (np.ndarray): The 4x4 affine matrix.

    Returns:
        float: The volume of a single voxel.
    """
    return abs(np.linalg.det(affine[:3, :3]))

def voxel_to_world(voxel_coords: np.ndarray, affine: np.ndarray) -> np.ndarray:
    """
    Converts voxel coordinates to world (physical) coordinates.

    Args:
        voxel_coords (np.ndarray): A (N, 3) array of voxel coordinates (i, j, k).
        affine (np.ndarray): The 4x4 NIfTI affine matrix.

    Returns:
        np.ndarray: A (N, 3) array of world coordinates (x, y, z).
    """
    voxel_coords = np.asarray(voxel_coords)
    if voxel_coords.ndim == 1:
        voxel_coords = voxel_coords.reshape(1, -1)
    
    # Add homogeneous coordinate
    homogeneous_coords = np.hstack((voxel_coords, np.ones((voxel_coords.shape[0], 1))))
    
    # Apply affine transformation
    world_coords_homogeneous = homogeneous_coords @ affine.T
    
    return world_coords_homogeneous[:, :3]

def world_to_voxel(world_coords: np.ndarray, affine: np.ndarray) -> np.ndarray:
    """
    Converts world (physical) coordinates to voxel coordinates.
    Uses the inverse of the affine matrix. Resulting voxel coordinates might be fractional.

    Args:
        world_coords (np.ndarray): A (N, 3) array of world coordinates (x, y, z).
        affine (np.ndarray): The 4x4 NIfTI affine matrix.

    Returns:
        np.ndarray: A (N, 3) array of voxel coordinates (i, j, k).
    """
    world_coords = np.asarray(world_coords)
    if world_coords.ndim == 1:
        world_coords = world_coords.reshape(1, -1)

    # Add homogeneous coordinate
    homogeneous_coords = np.hstack((world_coords, np.ones((world_coords.shape[0], 1))))
    
    # Invert affine matrix
    inv_affine = np.linalg.inv(affine)
    
    # Apply inverse affine transformation
    voxel_coords_homogeneous = homogeneous_coords @ inv_affine.T
    
    return voxel_coords_homogeneous[:, :3]

def distance_squared(p1: np.ndarray, p2: np.ndarray) -> float:
    """Computes the squared Euclidean distance between two 3D points."""
    return np.sum((p1 - p2)**2)

def distance(p1: np.ndarray, p2: np.ndarray) -> float:
    """Computes the Euclidean distance between two 3D points."""
    return np.sqrt(np.sum((p1 - p2)**2))

def normalize_vector(v: np.ndarray) -> np.ndarray:
    """Normalizes a vector."""
    norm = np.linalg.norm(v)
    if norm == 0:
        return v
    return v / norm

def create_output_directory(base_dir: str, sim_name: str = "gbo_sim", timestamp: bool = True) -> str:
    """
    Creates a unique output directory.
    Example: base_dir/YYYYMMDD_HHMMSS_sim_name or base_dir/sim_name
    """
    from datetime import datetime
    if timestamp:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        dir_name = f"{ts}_{sim_name}"
    else:
        dir_name = sim_name
    
    full_path = os.path.join(base_dir, dir_name)
    
    if os.path.exists(full_path):
        # Option 1: Overwrite (dangerous)
        # shutil.rmtree(full_path) 
        # Option 2: Add a suffix
        count = 1
        new_full_path = f"{full_path}_{count}"
        while os.path.exists(new_full_path):
            count += 1
            new_full_path = f"{full_path}_{count}"
        full_path = new_full_path
        logger.warning(f"Output directory {os.path.join(base_dir, dir_name)} existed. Using {full_path} instead.")

    os.makedirs(full_path, exist_ok=True)
    logger.info(f"Created output directory: {full_path}")
    return full_path

def is_voxel_in_bounds(voxel_coord: np.ndarray, shape: Tuple[int, ...]) -> bool:
    """Checks if a voxel coordinate is within the bounds of a given shape."""
    voxel_coord = np.asarray(voxel_coord) # Ensure it's a numpy array
    if voxel_coord.ndim == 0 or voxel_coord.shape[0] != len(shape): # Check for scalar or mismatched dimensions
        return False
    return all(0 <= voxel_coord[d] < shape[d] for d in range(len(shape)))



if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    # Test RNG seed
    set_rng_seed(123)
    print(f"Random float after seed 123: {random.random()}")
    set_rng_seed(123)
    print(f"Random float after seed 123 again: {random.random()}")
    print(f"Numpy random array after seed 123: {np.random.rand(3)}")
    set_rng_seed(123)
    print(f"Numpy random array after seed 123 again: {np.random.rand(3)}")

    # Test affine transformations
    # A typical NIfTI affine for 1mm isotropic voxels, origin at corner
    dummy_affine = np.array([
        [1.0, 0.0, 0.0, 0.0],
        [0.0, 1.0, 0.0, 0.0],
        [0.0, 0.0, 1.0, 0.0],
        [0.0, 0.0, 0.0, 1.0]
    ])
    # A more realistic affine (e.g. -1mm x, 1mm y, 1mm z, with an offset)
    # RAS orientation: X points Left to Right, Y Posterior to Anterior, Z Inferior to Superior
    # If voxel (0,0,0) is at world (-90, 90, -120) and voxels are 1mm:
    # This means i maps to -X, j maps to +Y, k maps to +Z (LPI orientation for data array)
    # If data array is stored radiological (i from R->L), then first column of affine is positive.
    # Assuming standard interpretation (voxel index increases, world coordinate increases along axis basis vector)
    # Let's use a simple affine for testing:
    test_affine = np.array([
        [-1.0, 0.0, 0.0, 100.0],  # Voxel i -> World -x direction; (0,0,0) maps to x=100
        [0.0, 1.0, 0.0, -50.0],  # Voxel j -> World +y direction; (0,0,0) maps to y=-50
        [0.0, 0.0, 2.0, -20.0],  # Voxel k -> World +z direction, 2mm thick; (0,0,0) maps to z=-20
        [0.0, 0.0, 0.0, 1.0]
    ])

    print(f"Voxel volume for test_affine: {get_voxel_volume_from_affine(test_affine)} mm^3 (expected 2)")

    voxel_pts = np.array([[0,0,0], [10,20,5]])
    world_pts = voxel_to_world(voxel_pts, test_affine)
    print(f"Voxel points:\n{voxel_pts}")
    print(f"Converted to World points:\n{world_pts}")
    # Expected for [0,0,0]: [100, -50, -20]
    # Expected for [10,20,5]: [-1*10+100, 1*20-50, 2*5-20] = [90, -30, -10]

    reconverted_voxel_pts = world_to_voxel(world_pts, test_affine)
    print(f"Reconverted to Voxel points:\n{reconverted_voxel_pts}")
    assert np.allclose(voxel_pts, reconverted_voxel_pts), "Voxel-World-Voxel conversion failed"

    # Test distance
    p1 = np.array([0,0,0])
    p2 = np.array([3,4,0])
    print(f"Distance squared between {p1} and {p2}: {distance_squared(p1, p2)} (expected 25)")
    print(f"Distance between {p1} and {p2}: {distance(p1, p2)} (expected 5)")

    # Test output directory
    base_output = "temp_test_output"
    os.makedirs(base_output, exist_ok=True)
    path1 = create_output_directory(base_output, "my_sim")
    path2 = create_output_directory(base_output, "my_sim") # Should create my_sim_1
    print(f"Path1: {path1}")
    print(f"Path2: {path2}")
    shutil.rmtree(base_output)
    print("Cleaned up temp_test_output directory.")# src/io_utils.py
import nibabel as nib
import numpy as np
import pyvista as pv
import networkx as nx
import os
import logging
import yaml
from typing import Any, Union, Dict, List, Tuple

logger = logging.getLogger(__name__)

def load_nifti_image(filepath: str) -> tuple[np.ndarray, np.ndarray, Any] | tuple[None, None, None]:
    """
    Loads a NIfTI image.

    Args:
        filepath (str): Path to the .nii or .nii.gz file.

    Returns:
        tuple: (data_array, affine_matrix, header) or (None, None, None) if loading fails.
               data_array is usually in (L,P,I) or (R,A,S) depending on how it was saved.
               Affine maps voxel indices to world space (often RAS).
    """
    if not os.path.exists(filepath):
        logger.warning(f"NIfTI file not found: {filepath}. Skipping.")
        return None, None, None
    try:
        img = nib.load(filepath)
        data = img.get_fdata()
        affine = img.affine
        header = img.header
        logger.info(f"Loaded NIfTI image: {filepath}, Shape: {data.shape}, Voxel size from affine: {np.diag(affine)[:3]}")
        return data.astype(np.float32), affine, header # Cast to float for calculations
    except Exception as e:
        logger.error(f"Error loading NIfTI file {filepath}: {e}")
        return None, None, None

def save_nifti_image(data_array: np.ndarray, affine: np.ndarray, filepath: str, header: nib.Nifti1Header = None):
    """
    Saves a NumPy array as a NIfTI image.

    Args:
        data_array (np.ndarray): The image data.
        affine (np.ndarray): The affine matrix for the image.
        filepath (str): Path to save the .nii or .nii.gz file.
        header (nib.Nifti1Header, optional): NIfTI header. If None, a minimal one is created.
    """
    try:
        # Ensure data type is compatible, e.g. float32 or int16
        # Nifti1Image constructor will handle appropriate dtype based on data.
        # If data is boolean, convert to int8 or uint8
        if data_array.dtype == bool:
            data_array = data_array.astype(np.uint8)
            
        img = nib.Nifti1Image(data_array, affine, header=header)
        nib.save(img, filepath)
        logger.info(f"Saved NIfTI image to: {filepath}")
    except Exception as e:
        logger.error(f"Error saving NIfTI file {filepath}: {e}")
        raise

def load_arterial_centerlines_vtp(filepath: str) -> pv.PolyData | None:
    """
    Loads arterial centerlines from a VTP file.
    Assumes the VTP file contains points and lines representing vessel segments.
    It should ideally have a 'radius' point data array.

    Args:
        filepath (str): Path to the .vtp file.

    Returns:
        pyvista.PolyData: The loaded PolyData object or None if loading fails.
    """
    if not os.path.exists(filepath):
        logger.warning(f"Arterial centerline file not found: {filepath}. Skipping.")
        return None
    try:
        mesh = pv.read(filepath)
        logger.info(f"Loaded arterial centerlines from VTP: {filepath}")
        if 'radius' not in mesh.point_data:
            logger.warning(f"VTP file {filepath} does not contain 'radius' point data. Defaulting or errors might occur.")
        # Could add more checks here, e.g., for line connectivity
        return mesh
    except Exception as e:
        logger.error(f"Error loading VTP file {filepath}: {e}")
        return None

def load_arterial_centerlines_txt(filepath: str, radius_default: float = 0.1) -> pv.PolyData | None:
    """
    Loads arterial centerlines from a TXT file and converts to PyVista PolyData.
    Expected TXT format:
    Each line: x y z [radius]
    If radius is not present, radius_default is used.
    Segments are assumed to connect consecutive points.
    A more robust format might specify connectivity explicitly. For now, assume polylines.

    Args:
        filepath (str): Path to the .txt file.
        radius_default (float): Default radius if not specified in the file.

    Returns:
        pyvista.PolyData: A PolyData object representing the centerlines, or None if loading fails.
    """
    if not os.path.exists(filepath):
        logger.warning(f"Arterial centerline TXT file not found: {filepath}. Skipping.")
        return None
    
    points = []
    radii = []
    try:
        with open(filepath, 'r') as f:
            for line_num, line in enumerate(f):
                line = line.strip()
                if not line or line.startswith('#'): # Skip empty lines or comments
                    continue
                parts = list(map(float, line.split()))
                if len(parts) == 3:
                    points.append(parts)
                    radii.append(radius_default)
                elif len(parts) == 4:
                    points.append(parts[:3])
                    radii.append(parts[3])
                else:
                    logger.warning(f"Skipping malformed line {line_num+1} in {filepath}: {line}")
        
        if not points:
            logger.error(f"No valid points found in TXT file: {filepath}")
            return None

        points_np = np.array(points)
        radii_np = np.array(radii)

        # Create PolyData: assumes a single polyline for simplicity
        # For multiple disconnected arteries, the TXT format would need to be richer or
        # processed to identify separate polylines.
        num_points = len(points_np)
        lines = np.empty((num_points - 1, 3), dtype=int)
        lines[:, 0] = 2  # Each line segment has 2 points
        lines[:, 1] = np.arange(num_points - 1)
        lines[:, 2] = np.arange(1, num_points)
        
        poly = pv.PolyData(points_np, lines=lines)
        poly.point_data['radius'] = radii_np
        
        logger.info(f"Loaded arterial centerlines from TXT: {filepath}, {num_points} points.")
        return poly

    except Exception as e:
        logger.error(f"Error loading TXT file {filepath}: {e}")
        return None


def save_vascular_tree_vtp(graph: nx.DiGraph, filepath: str,
                           pos_attr='pos', radius_attr='radius', pressure_attr='pressure', flow_attr='flow'):
    """
    Saves a vascular tree (NetworkX graph) to a VTP file.
    Nodes store positions and radii. Edges define connectivity.
    """
    points = []
    point_radii = []
    point_pressures = []
    lines_connectivity = [] # Changed name for clarity, this is for pv.PolyData(points, lines=HERE)
    edge_flows = [] 

    node_to_idx = {node_id: i for i, node_id in enumerate(graph.nodes())}

    for node_id, data in graph.nodes(data=True):
        if pos_attr not in data:
            logger.warning(f"Node {node_id} missing '{pos_attr}' attribute. Skipping for point data.")
            continue
        points.append(data[pos_attr])
        point_radii.append(data.get(radius_attr, 0.0))
        point_pressures.append(data.get(pressure_attr, np.nan)) 

    if not points:
        logger.error("No points to save in the vascular tree. VTP file will be empty or invalid.")
        empty_poly = pv.PolyData()
        empty_poly.save(filepath)
        logger.error(f"Saved an empty VTP file to: {filepath} due to no valid points in the graph.")
        return

    # Build the lines array for PolyData constructor
    # Format: [n_points_in_line0, pt0_idx, pt1_idx, n_points_in_line1, ptA_idx, ptB_idx, ...]
    raw_lines_for_pv = []
    for u, v, data in graph.edges(data=True):
        if u in node_to_idx and v in node_to_idx:
            raw_lines_for_pv.extend([2, node_to_idx[u], node_to_idx[v]]) # Each line segment has 2 points
            edge_flows.append(data.get(flow_attr, np.nan)) 
        else:
            logger.warning(f"Edge ({u}-{v}) references missing node. Skipping this edge for line connectivity.")

    # Create PolyData object
    # If there are lines, pass them to the constructor.
    # Otherwise, it's just a point cloud.
    if raw_lines_for_pv:
        poly_data = pv.PolyData(np.array(points), lines=np.array(raw_lines_for_pv))
    else:
        poly_data = pv.PolyData(np.array(points)) # Will be a point cloud if no edges

    logger.debug(f"PolyData created. Number of points: {poly_data.n_points}, Number of cells (lines): {poly_data.n_cells}")
    logger.debug(f"Number of edge_flows collected: {len(edge_flows)}")
    
    # Add point data
    if points: # Check if points list is not empty before trying to assign
        poly_data.point_data[radius_attr] = np.array(point_radii)
        if any(not np.isnan(p) for p in point_pressures): 
            poly_data.point_data[pressure_attr] = np.array(point_pressures)
    
    # Add cell data (flow) only if there are cells and corresponding flow data
    if edge_flows and poly_data.n_cells > 0:
        if poly_data.n_cells == len(edge_flows):
            poly_data.cell_data[flow_attr] = np.array(edge_flows)
        else:
            # This case should ideally not be hit if graph processing and polydata creation are correct
            logger.error(
                f"Critical mismatch assigning cell data! "
                f"PolyData n_cells: {poly_data.n_cells}, "
                f"Number of flow values: {len(edge_flows)}. "
                f"Flow data will NOT be saved for cells."
            )
            # Decide: either don't add flow data, or pad/truncate (not ideal)
            # For now, we won't add it if there's a mismatch.
    
    try:
        poly_data.save(filepath)
        logger.info(f"Saved vascular tree to VTP: {filepath}")
    except Exception as e:
        logger.error(f"Error saving vascular tree to VTP {filepath}: {e}")
        raise

def save_simulation_parameters(config: dict, filepath: str):
    """Saves the simulation configuration to a YAML file."""
    try:
        with open(filepath, 'w') as f:
            yaml.dump(config, f, sort_keys=False, indent=4)
        logger.info(f"Saved simulation parameters to: {filepath}")
    except Exception as e:
        logger.error(f"Error saving simulation parameters to {filepath}: {e}")
        raise


if __name__ == '__main__':
    # Setup basic logging for testing
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Create dummy data directory
    test_data_dir = "temp_io_test_data"
    os.makedirs(test_data_dir, exist_ok=True)

    # --- Test NIfTI I/O ---
    dummy_nifti_path = os.path.join(test_data_dir, "dummy.nii.gz")
    shape = (10, 10, 10)
    affine = np.eye(4)
    affine[0,0] = affine[1,1] = affine[2,2] = 0.5 # 0.5mm isotropic voxels
    data = np.random.rand(*shape).astype(np.float32)
    
    print(f"\n--- Testing NIfTI I/O ---")
    save_nifti_image(data, affine, dummy_nifti_path)
    loaded_data, loaded_affine, _ = load_nifti_image(dummy_nifti_path)
    
    if loaded_data is not None:
        assert np.allclose(data, loaded_data), "NIfTI data mismatch"
        assert np.allclose(affine, loaded_affine), "NIfTI affine mismatch"
        print("NIfTI I/O test successful.")
    else:
        print("NIfTI loading failed.")

    # Test loading non-existent NIfTI
    load_nifti_image("non_existent.nii.gz")


    # --- Test VTP I/O (arterial centerlines) ---
    print(f"\n--- Testing VTP I/O (arterial centerlines) ---")
    dummy_vtp_path = os.path.join(test_data_dir, "dummy_arteries.vtp")
    # Create a simple PyVista PolyData object
    points = np.array([[0,0,0], [1,1,0], [2,0,0]], dtype=float)
    lines = np.array([2, 0, 1, 2, 1, 2]) # Connect point 0-1, then 1-2
    poly = pv.PolyData(points, lines=lines)
    poly.point_data['radius'] = np.array([0.5, 0.4, 0.3])
    poly.save(dummy_vtp_path)
    
    loaded_poly = load_arterial_centerlines_vtp(dummy_vtp_path)
    if loaded_poly:
        assert loaded_poly.n_points == 3, "VTP point count mismatch"
        assert 'radius' in loaded_poly.point_data, "VTP radius data missing"
        print("VTP arterial centerline I/O test successful.")
    else:
        print("VTP loading failed.")
    
    # Test loading non-existent VTP
    load_arterial_centerlines_vtp("non_existent.vtp")

    # --- Test TXT I/O (arterial centerlines) ---
    print(f"\n--- Testing TXT I/O (arterial centerlines) ---")
    dummy_txt_path = os.path.join(test_data_dir, "dummy_arteries.txt")
    with open(dummy_txt_path, "w") as f:
        f.write("# Test TXT file\n")
        f.write("0 0 0 0.5\n")
        f.write("1 1 0 0.4\n")
        f.write("2 0 0\n") # Test with default radius
        f.write("3 1 1 0.2\n")
    
    loaded_poly_txt = load_arterial_centerlines_txt(dummy_txt_path, radius_default=0.1)
    if loaded_poly_txt:
        assert loaded_poly_txt.n_points == 4, "TXT point count mismatch"
        assert 'radius' in loaded_poly_txt.point_data, "TXT radius data missing"
        expected_radii = np.array([0.5, 0.4, 0.1, 0.2])
        assert np.allclose(loaded_poly_txt.point_data['radius'], expected_radii), "TXT radii mismatch"
        print(f"TXT loaded radii: {loaded_poly_txt.point_data['radius']}")
        print("TXT arterial centerline I/O test successful.")
    else:
        print("TXT loading failed.")

    # --- Test Vascular Tree (NetworkX to VTP) Save ---
    print(f"\n--- Testing Vascular Tree (NetworkX to VTP) Save ---")
    graph = nx.DiGraph()
    # Add nodes with positions and radii
    graph.add_node(0, pos=np.array([0,0,0]), radius=0.5, pressure=100.0)
    graph.add_node(1, pos=np.array([1,0,0]), radius=0.4, pressure=90.0)
    graph.add_node(2, pos=np.array([1,1,0]), radius=0.3, pressure=80.0)
    # Add edges with flow
    graph.add_edge(0, 1, flow=10.0)
    graph.add_edge(1, 2, flow=8.0)

    tree_vtp_path = os.path.join(test_data_dir, "vascular_tree.vtp")
    save_vascular_tree_vtp(graph, tree_vtp_path)
    
    # Verify by loading it back with PyVista
    if os.path.exists(tree_vtp_path):
        loaded_tree_poly = pv.read(tree_vtp_path)
        assert loaded_tree_poly.n_points == 3, "Saved tree VTP point count mismatch"
        assert 'radius' in loaded_tree_poly.point_data, "Saved tree VTP radius missing"
        assert 'pressure' in loaded_tree_poly.point_data, "Saved tree VTP pressure missing"
        # Check if flow is present as cell data
        if loaded_tree_poly.n_cells > 0 : # n_cells corresponds to number of lines/edges
             assert 'flow' in loaded_tree_poly.cell_data, "Saved tree VTP flow missing"
        print("Vascular tree (NetworkX to VTP) save test successful.")
    else:
        print("Vascular tree VTP save failed.")
        
    # Test saving an empty graph
    empty_graph = nx.DiGraph()
    empty_tree_vtp_path = os.path.join(test_data_dir, "empty_vascular_tree.vtp")
    save_vascular_tree_vtp(empty_graph, empty_tree_vtp_path)
    if os.path.exists(empty_tree_vtp_path):
        loaded_empty_tree_poly = pv.read(empty_tree_vtp_path)
        assert loaded_empty_tree_poly.n_points == 0, "Empty graph should result in VTP with 0 points."
        print("Saving empty graph to VTP test successful.")

    # --- Test Saving Simulation Parameters ---
    print(f"\n--- Testing Saving Simulation Parameters ---")
    dummy_params_path = os.path.join(test_data_dir, "sim_params.yaml")
    test_config = {"param1": 10, "nested": {"param2": "test"}}
    save_simulation_parameters(test_config, dummy_params_path)
    # Verify by loading
    with open(dummy_params_path, 'r') as f:
        loaded_params = yaml.safe_load(f)
    assert loaded_params["param1"] == 10, "Param save/load mismatch"
    print("Simulation parameter saving test successful.")

    # Clean up dummy data directory
    import shutil
    shutil.rmtree(test_data_dir)
    print(f"\nCleaned up temporary test directory: {test_data_dir}")# src/data_structures.py
import networkx as nx
import numpy as np
import logging

logger = logging.getLogger(__name__)

# --- Vascular Tree (NetworkX Graph) Conventions ---
# Nodes in the graph represent points in 3D space (e.g., bifurcations, terminals, points along a segment).
# Edges represent vessel segments connecting these points.

# Node Attributes:
# - 'id': (any, unique) Unique identifier for the node. Often the NetworkX node key itself.
# - 'pos': (np.ndarray, shape (3,)) 3D coordinates [x, y, z] in physical units (e.g., mm). REQUIRED.
# - 'radius': (float) Radius of the vessel at this node/point in physical units. REQUIRED for many operations.
# - 'type': (str) Type of node, e.g., 'root', 'bifurcation', 'terminal', 'segment_point'.
# - 'pressure': (float) Blood pressure at this node (computed during perfusion modeling).
# - 'flow_demand': (float) For terminal nodes, the flow Q_i required by its territory.
# - 'territory_voxels': (list or np.ndarray) Indices or coordinates of voxels supplied by this terminal.
# - 'parent_measured_terminal_id': (any) For synthetic terminals, ID of the measured artery terminal they originated from.
# - 'is_tumor_vessel': (bool) True if this node is part of a tumor-induced vessel.

# Edge Attributes (for edge u -> v):
# - 'length': (float) Length of the vessel segment in physical units. Can be calculated from node positions.
# - 'radius': (float) Radius of the segment. Can be average of u and v radii, or u's radius if flow is from u to v.
#           Consistency needed: often derived from flow and Murray's law.
# - 'flow': (float) Blood flow rate through the segment (computed during perfusion modeling).
# - 'resistance': (float) Hydraulic resistance of the segment (computed for perfusion modeling).
# - 'is_tumor_vessel': (bool) True if this segment is part of a tumor-induced vessel.


def create_empty_vascular_graph() -> nx.DiGraph:
    """Creates an empty directed graph for the vascular tree."""
    return nx.DiGraph()

def add_node_to_graph(graph: nx.DiGraph, node_id: any, pos: np.ndarray, radius: float, 
                      node_type: str = 'default', **kwargs):
    """
    Adds a node with standard attributes to the vascular graph.
    
    Args:
        graph (nx.DiGraph): The graph to add the node to.
        node_id (any): Unique ID for the node.
        pos (np.ndarray): 3D position [x,y,z].
        radius (float): Vessel radius at this node.
        node_type (str): Type of node.
        **kwargs: Additional attributes to set for the node.
    """
    if not isinstance(pos, np.ndarray) or pos.shape != (3,):
        raise ValueError("Position 'pos' must be a 3-element NumPy array.")
    if not isinstance(radius, (int, float)) or radius < 0:
        raise ValueError("Radius must be a non-negative number.")

    attrs = {
        'pos': pos,
        'radius': radius,
        'type': node_type,
    }
    attrs.update(kwargs) # Add any extra attributes
    graph.add_node(node_id, **attrs)
    # logger.debug(f"Added node {node_id} with attributes: {attrs}")

def add_edge_to_graph(graph: nx.DiGraph, u_id: any, v_id: any, **kwargs):
    """
    Adds an edge with standard attributes to the vascular graph.
    Length is automatically calculated if node positions exist.
    
    Args:
        graph (nx.DiGraph): The graph to add the edge to.
        u_id (any): ID of the source node.
        v_id (any): ID of the target node.
        **kwargs: Additional attributes to set for the edge.
    """
    if not graph.has_node(u_id) or not graph.has_node(v_id):
        logger.error(f"Cannot add edge ({u_id}-{v_id}): one or both nodes do not exist.")
        raise ValueError(f"Nodes {u_id} or {v_id} not in graph.")

    attrs = {}
    # Calculate length
    pos_u = graph.nodes[u_id].get('pos')
    pos_v = graph.nodes[v_id].get('pos')
    if pos_u is not None and pos_v is not None:
        length = np.linalg.norm(pos_u - pos_v)
        attrs['length'] = length
    else:
        logger.warning(f"Could not calculate length for edge ({u_id}-{v_id}) due to missing node positions.")

    # Example: Edge radius could be based on upstream node or average
    # For now, let's assume it might be set explicitly or derived later
    # radius_u = graph.nodes[u_id].get('radius')
    # if radius_u is not None:
    #    attrs['radius'] = radius_u 

    attrs.update(kwargs) # Add any extra attributes
    graph.add_edge(u_id, v_id, **attrs)
    # logger.debug(f"Added edge ({u_id}-{v_id}) with attributes: {attrs}")


# --- Tissue Data Structure ---
# Represented as a dictionary of NumPy arrays, plus affine and voxel volume.
# Example:
# tissue_data = {
#     'WM': wm_array,         # (X, Y, Z) binary or fractional mask
#     'GM': gm_array,         # (X, Y, Z)
#     'Tumor': tumor_array,   # (X, Y, Z)
#     'CSF': csf_array,       # (X, Y, Z)
#     'domain_mask': combined_mask, # (X, Y, Z) boolean array defining relevant voxels
#     'metabolic_demand_map': demand_map, # (X, Y, Z) float array of q_met per voxel
#     'affine': affine_matrix, # 4x4 np.ndarray
#     'voxel_volume': volume_per_voxel, # float (mm^3 or m^3)
#     'world_coords_flat': world_coords_of_domain_voxels # (N_domain_voxels, 3)
#     'voxel_indices_flat': voxel_indices_of_domain_voxels # (N_domain_voxels, 3)
# }

def get_metabolic_demand_map(tissue_segmentations: dict, config: dict, voxel_volume: float) -> np.ndarray:
    """
    Generates a metabolic demand map (q_met * dV) from tissue segmentations.
    
    Args:
        tissue_segmentations (dict): Dictionary of tissue type arrays (e.g., 'WM', 'GM', 'Tumor').
                                     Values are masks (0 or 1, or fractional 0-1).
        config (dict): Configuration dictionary with metabolic rates.
        voxel_volume (float): Volume of a single voxel (e.g., in mm^3).

    Returns:
        np.ndarray: A 3D array of the same shape as segmentations, where each voxel
                    contains the total metabolic demand (e.g., in mm^3_blood/s).
    """
    from src import config_manager as cfg_mgr # to use get_param

    # Get shape from one of the segmentations
    shape = None
    for seg_name, seg_array in tissue_segmentations.items():
        if seg_array is not None:
            shape = seg_array.shape
            break
    if shape is None:
        logger.error("No valid tissue segmentations provided to create metabolic map.")
        return None

    demand_map = np.zeros(shape, dtype=np.float32)
    
    q_rates = cfg_mgr.get_param(config, "tissue_properties.metabolic_rates")

    if 'GM' in tissue_segmentations and tissue_segmentations['GM'] is not None:
        demand_map += tissue_segmentations['GM'] * q_rates.get('gm', 0.0)
    if 'WM' in tissue_segmentations and tissue_segmentations['WM'] is not None:
        demand_map += tissue_segmentations['WM'] * q_rates.get('wm', 0.0)
    if 'CSF' in tissue_segmentations and tissue_segmentations['CSF'] is not None:
        demand_map += tissue_segmentations['CSF'] * q_rates.get('csf', 0.0) # usually 0
    
    # Tumor can have rim/core distinction if available, or a single tumor type
    if 'Tumor' in tissue_segmentations and tissue_segmentations['Tumor'] is not None:
        # Simple model: use 'tumor_rim' for all tumor voxels if 'tumor_core' isn't specified
        # or if the tumor segmentation isn't further divided.
        # A more complex model would require separate Tumor_Rim and Tumor_Core segmentations.
        tumor_rate = q_rates.get('tumor_rim', q_rates.get('tumor', 0.0)) # Fallback to 'tumor' if 'tumor_rim' not there
        demand_map += tissue_segmentations['Tumor'] * tumor_rate
    elif 'Tumor_Rim' in tissue_segmentations and tissue_segmentations['Tumor_Rim'] is not None:
         demand_map += tissue_segmentations['Tumor_Rim'] * q_rates.get('tumor_rim', 0.0)
         if 'Tumor_Core' in tissue_segmentations and tissue_segmentations['Tumor_Core'] is not None:
             demand_map += tissue_segmentations['Tumor_Core'] * q_rates.get('tumor_core', 0.0)
             
    return demand_map * voxel_volume # Now it's total demand per voxel (e.g. mm^3/s)

if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)

    # Test vascular graph functions
    g = create_empty_vascular_graph()
    add_node_to_graph(g, 0, pos=np.array([0.,0.,0.]), radius=0.5, node_type='root', pressure=100)
    add_node_to_graph(g, 1, pos=np.array([1.,0.,0.]), radius=0.4, node_type='bifurcation')
    add_node_to_graph(g, 2, pos=np.array([2.,1.,0.]), radius=0.3, node_type='terminal', flow_demand=5)
    add_node_to_graph(g, 3, pos=np.array([2.,-1.,0.]), radius=0.3, node_type='terminal', flow_demand=5)

    add_edge_to_graph(g, 0, 1, flow=10)
    add_edge_to_graph(g, 1, 2, flow=5)
    add_edge_to_graph(g, 1, 3, flow=5)

    print("\n--- Vascular Graph Test ---")
    print(f"Nodes: {g.nodes(data=True)}")
    print(f"Edges: {g.edges(data=True)}")
    assert np.isclose(g.edges[(0,1)]['length'], 1.0)
    assert g.nodes[2]['flow_demand'] == 5

    # Test metabolic demand map
    print("\n--- Metabolic Demand Map Test ---")
    dummy_config = {
        "tissue_properties": {
            "metabolic_rates": {
                "gm": 0.01, "wm": 0.003, "csf": 0.0, "tumor_rim": 0.02
            }
        }
    }
    gm_seg = np.zeros((3,3,3), dtype=np.uint8)
    gm_seg[1,1,1] = 1
    wm_seg = np.zeros((3,3,3), dtype=np.uint8)
    wm_seg[0,0,0] = 1
    
    tissue_segs = {'GM': gm_seg, 'WM': wm_seg}
    voxel_vol = 2.0 # mm^3

    demand_map = get_metabolic_demand_map(tissue_segs, dummy_config, voxel_vol)
    if demand_map is not None:
        print(f"Demand map at (1,1,1) (GM): {demand_map[1,1,1]} (Expected: 0.01 * 2.0 = 0.02)")
        assert np.isclose(demand_map[1,1,1], 0.01 * voxel_vol)
        print(f"Demand map at (0,0,0) (WM): {demand_map[0,0,0]} (Expected: 0.003 * 2.0 = 0.006)")
        assert np.isclose(demand_map[0,0,0], 0.003 * voxel_vol)
        print(f"Demand map at (2,2,2) (Background): {demand_map[2,2,2]} (Expected: 0.0)")
        assert np.isclose(demand_map[2,2,2], 0.0)
        print("Metabolic demand map test successful.")
    else:
        print("Metabolic demand map test failed.")# main.py
import argparse
import logging
import os
import time
import numpy as np
import networkx as nx # For type hinting and graph operations
from typing import Optional

# PyVista import for VTP parsing, ensure it's available (used by io_utils indirectly)
try:
    import pyvista as pv
    PYVISTA_AVAILABLE = True
except ImportError:
    PYVISTA_AVAILABLE = False
    pv = None

from src import config_manager, io_utils, utils
from src import data_structures
from src import vascular_growth, angiogenesis, perfusion_solver, visualization
from src.constants import DEFAULT_VOXEL_SIZE_MM, Q_MET_TUMOR_RIM_PER_ML, INITIAL_TERMINAL_FLOW_Q # Added more constants

logger = logging.getLogger(__name__) # Ensure logger is defined for this function's scope


def setup_logging(log_level_str: str, log_file: str):
    """Configures logging for the simulation."""
    numeric_level = getattr(logging, log_level_str.upper(), logging.INFO)
    if not isinstance(numeric_level, int): # Fallback if level is invalid
        print(f"Warning: Invalid log level '{log_level_str}'. Defaulting to INFO.")
        numeric_level = logging.INFO

    # Make sure log directory exists
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    
    logging.basicConfig(
        level=numeric_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='w'), # Overwrite log file each run
            logging.StreamHandler() # Also print to console
        ]
    )
    # Suppress overly verbose logs if not in DEBUG
    if numeric_level > logging.DEBUG:
        logging.getLogger('pyvista').setLevel(logging.WARNING)
        logging.getLogger('matplotlib').setLevel(logging.WARNING)


def parse_arguments():
    """Parses command-line arguments."""
    parser = argparse.ArgumentParser(description="GBO Brain Vasculature Simulation Framework")
    parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Path to the YAML configuration file (default: config.yaml)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="Override output directory from config file."
    )
    return parser.parse_args()

def load_initial_data(config: dict) -> tuple[dict, Optional[nx.DiGraph]]:
    """
    Loads all necessary input data based on the configuration.
    """
    logger.info("Loading initial data...")
    
    tissue_data = {
        'WM': None, 'GM': None, 'CSF': None, 
        'Tumor_Max_Extent': None, 'Tumor': None, # Active tumor
        'tumor_rim_mask': None, 'tumor_core_mask': None,
        'VEGF_field': None, 'affine': None, 'voxel_volume': None, 
        'domain_mask': None, # This will be set to gbo_growth_domain_mask
        'anatomical_domain_mask': None, # GM+WM+CSF+TumorMax
        'gbo_growth_domain_mask': None, # GM+WM
        'metabolic_demand_map': None,
        'world_coords_flat': None, # Derived from gbo_growth_domain_mask
        'voxel_indices_flat': None,  # Derived from gbo_growth_domain_mask
        'shape': None
    }

    paths = config_manager.get_param(config, "paths", {})
    # Load primary masks and determine affine and common_shape
    wm_data, affine_wm, _ = io_utils.load_nifti_image(paths.get("wm_nifti",""))
    gm_data, affine_gm, _ = io_utils.load_nifti_image(paths.get("gm_nifti",""))
    
    # Determine primary affine (WM > GM > Tumor (for shape/affine only) > Default)
    # This affine will be used for ALL world coordinate conversions
    if affine_wm is not None:
        tissue_data['affine'] = affine_wm
        logger.info("Using affine from WM NIfTI as primary.")
    elif affine_gm is not None:
        tissue_data['affine'] = affine_gm
        logger.info("Using affine from GM NIfTI as primary.")
    
    common_shape = None
    if wm_data is not None:
        common_shape = wm_data.shape
        tissue_data['WM'] = wm_data.astype(bool)
    if gm_data is not None:
        if common_shape and gm_data.shape != common_shape:
            logger.error(f"GM shape {gm_data.shape} mismatch WM {common_shape}. Ensure co-registration to the primary affine space.")
            # Decide error handling: raise error, or proceed with caution, or try to resample (advanced)
        elif not common_shape: 
            common_shape = gm_data.shape
        tissue_data['GM'] = gm_data.astype(bool)

    # If affine or shape still not set, try from tumor_nifti (final extent)
    if paths.get("tumor_nifti"):
        tumor_final_data, affine_tumor, _ = io_utils.load_nifti_image(paths["tumor_nifti"])
        if tumor_final_data is not None:
            if common_shape is None:
                common_shape = tumor_final_data.shape
                logger.info(f"Derived common_shape {common_shape} from tumor_nifti.")
            elif tumor_final_data.shape != common_shape:
                logger.error(f"Tumor_Max_Extent shape {tumor_final_data.shape} mismatch domain {common_shape}. Check co-registration.")
                # Do not use this tumor_data_final if shape mismatches established common_shape
                tumor_final_data = None # Mark as unusable for mask

            if tissue_data['affine'] is None and affine_tumor is not None:
                tissue_data['affine'] = affine_tumor
                logger.info("Using affine from Tumor NIfTI as primary (WM/GM affine missing).")
            
            if tumor_final_data is not None: # If shape was okay or it set the common_shape
                 tissue_data['Tumor_Max_Extent'] = tumor_final_data.astype(bool)
                 logger.info(f"Loaded Tumor_Max_Extent segmentation: {np.sum(tissue_data['Tumor_Max_Extent'])} voxels.")

    if tissue_data['affine'] is None: # Absolute fallback
        logger.warning("No NIfTI found to determine affine. Using default identity affine and 1mm voxel size.")
        tissue_data['affine'] = np.eye(4)
        for i in range(3): tissue_data['affine'][i,i] = constants.DEFAULT_VOXEL_SIZE_MM # Ensure DEFAULT_VOXEL_SIZE_MM is from constants
        if common_shape is None:
             raise ValueError("Cannot determine tissue domain shape AND no affine available. Critical error.")
    
    if common_shape is None: # If still none after all attempts
        raise ValueError("Critical: Could not determine a common shape for any tissue data.")
    tissue_data['shape'] = common_shape
    tissue_data['voxel_volume'] = utils.get_voxel_volume_from_affine(tissue_data['affine'])

    # Initialize all mask keys to prevent KeyErrors, ensuring they have the common_shape
    for key_mask_init in ['CSF', 'Tumor', 'tumor_rim_mask', 'tumor_core_mask', 'VEGF_field', 
                          'anatomical_domain_mask', 'gbo_growth_domain_mask']:
        if key_mask_init not in tissue_data or tissue_data[key_mask_init] is None:
             tissue_data[key_mask_init] = np.zeros(common_shape, dtype=float if key_mask_init == 'VEGF_field' else bool)
    if tissue_data.get('Tumor_Max_Extent') is None: # Ensure Tumor_Max_Extent exists
        tissue_data['Tumor_Max_Extent'] = np.zeros(common_shape, dtype=bool)


    # Load optional CSF
    if paths.get("csf_nifti"):
        csf_data, _, _ = io_utils.load_nifti_image(paths["csf_nifti"])
        if csf_data is not None and csf_data.shape == common_shape: 
            tissue_data['CSF'] = csf_data.astype(bool)
        elif csf_data is not None: 
            logger.warning(f"CSF shape {csf_data.shape} mismatch domain {common_shape}. Skipping CSF.")

    # --- Create different domain masks based on your strategy ---
    # 1. Anatomical domain: GM + WM + CSF + Tumor_Max_Extent (where any part of the model can exist)
    anatomical_domain = np.zeros(common_shape, dtype=bool)
    if tissue_data.get('GM') is not None: anatomical_domain = np.logical_or(anatomical_domain, tissue_data['GM'])
    if tissue_data.get('WM') is not None: anatomical_domain = np.logical_or(anatomical_domain, tissue_data['WM'])
    if tissue_data.get('CSF') is not None: anatomical_domain = np.logical_or(anatomical_domain, tissue_data['CSF'])
    if tissue_data.get('Tumor_Max_Extent') is not None: anatomical_domain = np.logical_or(anatomical_domain, tissue_data['Tumor_Max_Extent'])
    tissue_data['anatomical_domain_mask'] = anatomical_domain

    
    # Ensure the inspection directory exists
    inspection_dir = config_manager.get_param(config, "paths.inspection_dir", "data/inspection")  
    os.makedirs(inspection_dir, exist_ok=True) # inspection_dir should be defined
    # Save the anatomical domain mask for inspection
    key_to_save = 'anatomical_domain_mask'  # This is the mask we want to save for inspection
    if key_to_save in tissue_data and \
    isinstance(tissue_data[key_to_save], np.ndarray) and \
    np.any(tissue_data[key_to_save]) and \
    tissue_data.get('affine') is not None:
        try:
            io_utils.save_nifti_image(
                tissue_data[key_to_save].astype(np.uint8),  # Masks are boolean, save as uint8
                tissue_data['affine'],
                os.path.join(inspection_dir, f"inspect_{key_to_save}.nii.gz")
            )
            logger.info(f"Saved '{key_to_save}' for inspection.")
        except Exception as e_save_inspect:
            logger.error(f"Could not save inspection mask '{key_to_save}': {e_save_inspect}")
    else:
        logger.warning(f"Mask '{key_to_save}' not found, is empty, or affine is missing. Not saving for inspection.")


    logger.info(f"Anatomical domain mask (GM+WM+CSF+TumorMax) created with {np.sum(anatomical_domain)} voxels.")

    # 2. GBO growth domain: GM + WM only (where new healthy GBO vessels should grow)
    gbo_growth_domain = np.zeros(common_shape, dtype=bool)
    if tissue_data.get('GM') is not None: gbo_growth_domain = np.logical_or(gbo_growth_domain, tissue_data['GM'])
    if tissue_data.get('WM') is not None: gbo_growth_domain = np.logical_or(gbo_growth_domain, tissue_data['WM'])
    tissue_data['gbo_growth_domain_mask'] = gbo_growth_domain
    
    # This is the key that vascular_growth.py will use for its operations
    tissue_data['domain_mask'] = gbo_growth_domain 
    logger.info(f"GBO growth domain_mask (GM+WM) set with {np.sum(tissue_data['domain_mask'])} voxels.")

    #Save the GBO growth domain mask for inspection
    key_to_save = 'gbo_growth_domain_mask'  # This is the mask we want to save for inspection
    if key_to_save in tissue_data and \
    isinstance(tissue_data[key_to_save], np.ndarray) and \
    np.any(tissue_data[key_to_save]) and \
    tissue_data.get('affine') is not None:
        try:
            io_utils.save_nifti_image(
                tissue_data[key_to_save].astype(np.uint8),  # Masks are boolean, save as uint8
                tissue_data['affine'],
                os.path.join(inspection_dir, f"inspect_{key_to_save}.nii.gz")
            )
            logger.info(f"Saved '{key_to_save}' for inspection.")
        except Exception as e_save_inspect:
            logger.error(f"Could not save inspection mask '{key_to_save}': {e_save_inspect}")
    else:
        logger.warning(f"Mask '{key_to_save}' not found, is empty, or affine is missing. Not saving for inspection.")
        
    # Precompute world coordinates for the GBO GROWTH DOMAIN for KDTree
    voxel_indices_gbo_domain = np.array(np.where(tissue_data['domain_mask'])).T
    if voxel_indices_gbo_domain.size > 0:
        tissue_data['voxel_indices_flat'] = voxel_indices_gbo_domain
        tissue_data['world_coords_flat'] = utils.voxel_to_world(voxel_indices_gbo_domain, tissue_data['affine'])
    else:
        tissue_data['voxel_indices_flat'] = np.empty((0,3), dtype=int)
        tissue_data['world_coords_flat'] = np.empty((0,3), dtype=float)
        logger.warning("GBO growth domain_mask (GM+WM) is empty. Healthy GBO might not find tissue to grow into.")

    # Initial metabolic demand map for HEALTHY tissue only.
    # Tumor demand is added dynamically by the angiogenesis module.
    segmentations_for_initial_demand = {
        k: tissue_data[k] for k in ['WM', 'GM', 'CSF'] 
        if tissue_data.get(k) is not None and np.any(tissue_data[k])
    }
    tissue_data['metabolic_demand_map'] = data_structures.get_metabolic_demand_map(
        segmentations_for_initial_demand, config, tissue_data['voxel_volume']
    )
    if tissue_data['metabolic_demand_map'] is None: # Should be initialized now
        tissue_data['metabolic_demand_map'] = np.zeros(common_shape, dtype=np.float32)


    # --- Arterial Centerline Loading and Processing ---
    initial_arterial_graph = None
    centerline_path_key = "arterial_centerlines"
    centerline_file_str = paths.get(centerline_path_key)
    
    if centerline_file_str:
        input_data_base_dir = paths.get("input_data_dir", ".") # Default to current dir
        if not os.path.isabs(centerline_file_str):
            full_centerline_path = os.path.join(input_data_base_dir, centerline_file_str)
        else:
            full_centerline_path = centerline_file_str
        
        logger.info(f"Attempting to load arterial centerlines from: {full_centerline_path}")
        poly_data = None # PyVista PolyData object
        if full_centerline_path.endswith((".vtp", ".vtk")):
            poly_data = io_utils.load_arterial_centerlines_vtp(full_centerline_path)
        elif full_centerline_path.endswith(".txt"):
            default_radius_centerline = config_manager.get_param(config, "vascular_properties.centerline_default_radius", 0.1)
            poly_data = io_utils.load_arterial_centerlines_txt(full_centerline_path, radius_default=default_radius_centerline)
        else: 
            logger.warning(f"Unsupported arterial centerline file format: {full_centerline_path}")

        if poly_data and poly_data.n_points > 0:
            logger.info(f"Processing VTP/PolyData with {poly_data.n_points} points and {poly_data.n_cells} cells.")
            print(f"--- MAIN.PY (VTP Info) ---") 
            print(f"VTP ('{full_centerline_path}') loaded. Its internal point bounds: {poly_data.bounds}")
            if poly_data.n_points > 5: print(f"First 5 points from VTP (as loaded by PyVista):\n{poly_data.points[:5]}")
            print(f"--------------------------\n")

            initial_arterial_graph = data_structures.create_empty_vascular_graph()
            pv_point_to_nx_node: Dict[int, str] = {} 
            node_id_counter = 0
            default_min_radius = config_manager.get_param(config, "vascular_properties.min_radius", 0.01)

            radii_array = poly_data.point_data.get('radius') 
            if radii_array is None: logger.warning("'Radius' point data array not found in VTP. Using default_min_radius.")
            elif len(radii_array) != poly_data.n_points:
                logger.warning(f"Mismatch in 'Radius' array length. Using default_min_radius."); radii_array = None

            for pt_idx in range(poly_data.n_points):
                pos = poly_data.points[pt_idx]
                radius = float(radii_array[pt_idx]) if radii_array is not None and pt_idx < len(radii_array) else default_min_radius
                current_node_id = f"m_{node_id_counter}"; node_id_counter += 1
                data_structures.add_node_to_graph(initial_arterial_graph, current_node_id,
                                                  pos=pos, radius=radius, type='measured_point')
                pv_point_to_nx_node[pt_idx] = current_node_id
            logger.debug(f"Added {initial_arterial_graph.number_of_nodes()} nodes from VTP points.")

            vtp_polyline_start_nodes: Set[str] = set()
            vtp_polyline_end_nodes: Set[str] = set()

            for i_cell in range(poly_data.n_cells):
                cell_point_indices = poly_data.get_cell(i_cell).point_ids
                if len(cell_point_indices) < 1: continue
                
                start_node_id_nx = pv_point_to_nx_node.get(cell_point_indices[0])
                if start_node_id_nx: vtp_polyline_start_nodes.add(start_node_id_nx)

                if len(cell_point_indices) >= 2:
                    end_node_id_nx = pv_point_to_nx_node.get(cell_point_indices[-1])
                    if end_node_id_nx: vtp_polyline_end_nodes.add(end_node_id_nx)
                    for k_edge in range(len(cell_point_indices) - 1):
                        node_u_id = pv_point_to_nx_node.get(cell_point_indices[k_edge])
                        node_v_id = pv_point_to_nx_node.get(cell_point_indices[k_edge + 1])
                        if node_u_id and node_v_id and node_u_id != node_v_id:
                            rad_u = initial_arterial_graph.nodes[node_u_id]['radius']
                            rad_v = initial_arterial_graph.nodes[node_v_id]['radius']
                            source_node, target_node = node_u_id, node_v_id
                            if (rad_u < rad_v and not np.isclose(rad_u, rad_v)): 
                                source_node, target_node = node_v_id, node_u_id
                            if not initial_arterial_graph.has_edge(source_node, target_node):
                                data_structures.add_edge_to_graph(initial_arterial_graph, source_node, target_node, type='measured_segment')
            
            logger.info(f"Added {initial_arterial_graph.number_of_edges()} edges from VTP cells.")
            logger.info(f"Identified {len(vtp_polyline_start_nodes)} VTP polyline start nodes and {len(vtp_polyline_end_nodes)} VTP polyline end nodes.")
            
            # Refine node types based on connectivity AND whether they are polyline ends
            # And also based on whether they are inside the ANATOMICAL domain for external outlet classification
            roi_mask_for_vtp_typing = tissue_data.get('anatomical_domain_mask') # GM+WM+CSF+TumorMax
            
            for node_id in list(initial_arterial_graph.nodes()):
                in_deg = initial_arterial_graph.in_degree(node_id)
                out_deg = initial_arterial_graph.out_degree(node_id)
                node_data = initial_arterial_graph.nodes[node_id]
                is_vtp_cell_end = node_id in vtp_polyline_end_nodes

                if in_deg == 0 and out_deg > 0 : node_data['type'] = 'measured_root'
                elif out_deg == 0 and in_deg > 0: # Potential terminal
                    if is_vtp_cell_end:
                        # Now check if this VTP cell end is within the broader anatomical domain
                        is_within_anatomical_domain = False
                        if roi_mask_for_vtp_typing is not None and tissue_data['affine'] is not None:
                            pos_world = node_data['pos']
                            pos_vox_int = np.round(utils.world_to_voxel(pos_world, tissue_data['affine'])).astype(int)
                            if utils.is_voxel_in_bounds(pos_vox_int, roi_mask_for_vtp_typing.shape) and \
                               roi_mask_for_vtp_typing[tuple(pos_vox_int)]:
                                is_within_anatomical_domain = True
                        
                        if is_within_anatomical_domain:
                            node_data['type'] = 'measured_terminal_in_anatomical_domain'
                        else:
                            node_data['type'] = 'measured_external_outlet'
                            ext_flow = config_manager.get_param(config, "vascular_properties.external_outlet_default_flow", 0.001)
                            node_data['Q_flow'] = -ext_flow # Negative for outflow for solver
                    else: # out_deg=0, in_deg>0, but not a VTP cell end (e.g. artifact of direction heuristic)
                        node_data['type'] = 'measured_segment_point' 
                        logger.debug(f"Node {node_id} has out_deg=0 but not VTP end. Type: seg_point.")
                elif out_deg > 1: node_data['type'] = 'measured_bifurcation'
                elif in_deg > 1 and out_deg == 1: node_data['type'] = 'measured_convergence'
                elif in_deg == 1 and out_deg == 1: node_data['type'] = 'measured_segment_point'
                elif in_deg == 0 and out_deg == 0: node_data['type'] = 'measured_isolated_point'
                else: node_data['type'] = 'measured_complex_junction'

            num_roots = sum(1 for _, data in initial_arterial_graph.nodes(data=True) if data['type'] == 'measured_root')
            num_terminals_roi = sum(1 for _, data in initial_arterial_graph.nodes(data=True) if data['type'] == 'measured_terminal_in_anatomical_domain')
            num_terminals_ext = sum(1 for _, data in initial_arterial_graph.nodes(data=True) if data['type'] == 'measured_external_outlet')
            logger.info(f"Refined VTP node types: {num_roots} roots, {num_terminals_roi} ROI terminals, {num_terminals_ext} external outlets.")
    else: 
        logger.info("No 'arterial_centerlines' path specified in config. GBO will start from config seeds or fallback.")

    return tissue_data, initial_arterial_graph


def main():
    args = parse_arguments()
    try:
        config = config_manager.load_config(args.config)
    except Exception as e:
        print(f"CRITICAL: Failed to load configuration file '{args.config}': {e}")
        try: # Attempt to set up basic logging for more details on config load failure
            os.makedirs("output/error_logs", exist_ok=True)
            setup_logging("ERROR", "output/error_logs/config_load_error.log") # Log to a fixed error file
            logging.getLogger(__name__).critical(f"Failed to load configuration file '{args.config}': {e}", exc_info=True)
        except Exception as log_e: print(f"Additionally, failed to set up error logging: {log_e}")
        return

    sim_name = config_manager.get_param(config, "simulation.simulation_name", "gbo_sim")
    base_output_dir_config = config_manager.get_param(config, "paths.output_dir", "output")
    base_output_dir = args.output_dir if args.output_dir else base_output_dir_config
    
    os.makedirs(base_output_dir, exist_ok=True) # Ensure base output dir exists
    output_dir = utils.create_output_directory(base_output_dir, sim_name, timestamp=True)
    
    log_level = config_manager.get_param(config, "simulation.log_level", "INFO")
    log_file_path = os.path.join(output_dir, f"{sim_name}.log")
    setup_logging(log_level, log_file_path)
    
    main_logger = logging.getLogger(__name__)
    main_logger.info(f"Simulation started. Output directory: {output_dir}")
    main_logger.info(f"Using configuration file: {os.path.abspath(args.config)}")

    try: io_utils.save_simulation_parameters(config, os.path.join(output_dir, "config_used.yaml"))
    except Exception as e_save_config: main_logger.error(f"Could not save used configuration file: {e_save_config}")

    seed_val = config_manager.get_param(config, "simulation.random_seed", None)
    if seed_val is not None:
        try: utils.set_rng_seed(int(seed_val))
        except ValueError: main_logger.warning(f"Invalid random_seed value '{seed_val}'. Using system default.")

    start_time = time.time()
    main_logger.info("--- Loading Initial Data ---")
    try:
        tissue_data, initial_arterial_graph = load_initial_data(config)
    except ValueError as ve:
        main_logger.critical(f"Critical error during data loading: {ve}", exc_info=True); return
    except Exception as e:
        main_logger.critical(f"Unexpected error during data loading: {e}", exc_info=True); return
    
    if config_manager.get_param(config, "visualization.plot_initial_setup", True):
        main_logger.info("--- Visualizing Initial Setup ---")
        try:
            visualization.visualize_initial_setup(config=config, output_dir=output_dir,
                                                  tissue_data=tissue_data, initial_arterial_graph=initial_arterial_graph)
        except Exception as e_viz_init: main_logger.error(f"Failed to generate initial setup visualization: {e_viz_init}", exc_info=True)
    
    if config_manager.get_param(config, "visualization.save_initial_masks", False):
        main_logger.info("--- Saving Initial Masks (as NIfTI) ---")
        for key in ['WM', 'GM', 'CSF', 'Tumor_Max_Extent', 'domain_mask', 'metabolic_demand_map', 'Tumor', 'tumor_rim_mask', 'tumor_core_mask', 'VEGF_field']:
            if key in tissue_data and isinstance(tissue_data[key], np.ndarray) and np.any(tissue_data[key]):
                arr_to_save = tissue_data[key]
                dtype_to_save = np.float32 if key in ['metabolic_demand_map', 'VEGF_field'] else np.uint8
                if arr_to_save.dtype == bool: arr_to_save = arr_to_save.astype(np.uint8)
                if tissue_data.get('affine') is None: main_logger.warning(f"Cannot save '{key}': Affine missing."); continue
                try: io_utils.save_nifti_image(arr_to_save.astype(dtype_to_save), tissue_data['affine'], os.path.join(output_dir, f"debug_initial_tissue_{key}.nii.gz"))
                except Exception as e_save: main_logger.error(f"Could not save initial mask {key}: {e_save}")
        if initial_arterial_graph and initial_arterial_graph.number_of_nodes() > 0 :
            try: io_utils.save_vascular_tree_vtp(initial_arterial_graph, os.path.join(output_dir, "debug_initial_arterial_graph_parsed.vtp"))
            except Exception as e_save_vtp: main_logger.error(f"Could not save initial parsed arterial graph: {e_save_vtp}")

    healthy_vascular_tree = None
    if config_manager.get_param(config, "gbo_growth.enabled", True):
        main_logger.info("--- Starting Healthy Vascular Development (GBO) ---")
        healthy_vascular_tree = vascular_growth.grow_healthy_vasculature(
            config=config, tissue_data=tissue_data, initial_graph=initial_arterial_graph, output_dir=output_dir
        )
        if healthy_vascular_tree:
            main_logger.info(f"Healthy GBO finished. Tree: {healthy_vascular_tree.number_of_nodes()} N, {healthy_vascular_tree.number_of_edges()} E.")
            io_utils.save_vascular_tree_vtp(healthy_vascular_tree, os.path.join(output_dir, "healthy_vascular_tree.vtp"))
        else: main_logger.error("Healthy GBO failed or returned no tree.")
    else:
        main_logger.info("Healthy GBO growth skipped by config.")
        healthy_vascular_tree = initial_arterial_graph if initial_arterial_graph else data_structures.create_empty_vascular_graph()
        if initial_arterial_graph: main_logger.info("Using provided initial arterial graph as base for subsequent steps.")
        else: main_logger.info("No initial arterial graph and GBO skipped. Starting with empty tree.")


    final_vascular_tree = healthy_vascular_tree if healthy_vascular_tree else data_structures.create_empty_vascular_graph()

    if config_manager.get_param(config, "tumor_angiogenesis.enabled", False):
        if tissue_data.get('Tumor_Max_Extent') is not None and np.any(tissue_data['Tumor_Max_Extent']):
            main_logger.info("--- Starting Tumor Growth and Angiogenesis Simulation ---")
            base_for_angiogenesis = final_vascular_tree.copy() # Use the result of GBO (or initial graph if GBO skipped)
            final_vascular_tree = angiogenesis.simulate_tumor_angiogenesis_fixed_extent(
                config=config, tissue_data=tissue_data,
                base_vascular_tree=base_for_angiogenesis,
                output_dir=output_dir,
                perfusion_solver_func=perfusion_solver.solve_1d_poiseuille_flow
            )
            if final_vascular_tree:
                main_logger.info(f"Tumor angiogenesis finished. Final tree: {final_vascular_tree.number_of_nodes()} N, {final_vascular_tree.number_of_edges()} E.")
                io_utils.save_vascular_tree_vtp(final_vascular_tree, os.path.join(output_dir, "final_tumor_vascular_tree.vtp"))
            else:
                main_logger.error("Tumor angiogenesis returned no tree. Using pre-angiogenesis tree.")
                final_vascular_tree = base_for_angiogenesis 
        else: main_logger.info("Tumor angiogenesis skipped (No Tumor_Max_Extent defined or empty).")
    else: main_logger.info("Tumor angiogenesis disabled in config.")

    perfusion_map_3d, pressure_map_3d_tissue = None, None
    if config_manager.get_param(config, "perfusion_solver.run_final_1d_flow_solve", True) and \
       final_vascular_tree and final_vascular_tree.number_of_nodes() > 0:
        main_logger.info("--- Running Final 1D Flow Solve ---")
        
        for node_id_f, data_f in final_vascular_tree.nodes(data=True):
            if final_vascular_tree.out_degree(node_id_f) == 0 and final_vascular_tree.in_degree(node_id_f) > 0:
                term_pos_vox = np.round(utils.world_to_voxel(data_f['pos'], tissue_data['affine'])).astype(int)
                demand = data_f.get('Q_flow', 0.0) 
                if utils.is_voxel_in_bounds(term_pos_vox, tissue_data['shape']):
                    if data_f.get('is_tumor_vessel') and tissue_data.get('Tumor') is not None and tissue_data['Tumor'][tuple(term_pos_vox)]:
                        demand = config_manager.get_param(config, "tumor_angiogenesis.min_tumor_terminal_demand", DEFAULT_MIN_TUMOR_TERMINAL_DEMAND)
                    elif not data_f.get('is_tumor_vessel') and tissue_data.get('metabolic_demand_map') is not None:
                        # For healthy terminals, try to get demand from the metabolic map at their location
                        # This is a simplification; proper territory demand is better.
                        demand = tissue_data['metabolic_demand_map'][tuple(term_pos_vox)]
                        if demand < constants.EPSILON: # If healthy tissue demand is zero here for some reason
                            demand = constants.INITIAL_TERMINAL_FLOW_Q # Fallback small flow
                data_f['Q_flow'] = demand
        
        final_tree_copy_for_solve = final_vascular_tree.copy()
        final_vascular_tree_with_flow = perfusion_solver.solve_1d_poiseuille_flow(
            final_tree_copy_for_solve, config, None, None
        )
        if final_vascular_tree_with_flow:
            final_vascular_tree = final_vascular_tree_with_flow
            io_utils.save_vascular_tree_vtp(final_vascular_tree, os.path.join(output_dir, "final_vascular_tree_with_flowdata.vtp"))
            main_logger.info("Final 1D flow solution computed and saved on tree.")
        else: main_logger.error("Final 1D flow solution failed.")
    else: main_logger.info("Final 1D flow solve skipped.")

    main_logger.info("--- Generating Final Visualizations ---")
    visualization.generate_final_visualizations(
        config=config, output_dir=output_dir, tissue_data=tissue_data,
        vascular_graph=final_vascular_tree,
        perfusion_map=perfusion_map_3d, 
        pressure_map_tissue=pressure_map_3d_tissue,
        plot_context_masks=config_manager.get_param(config, "visualization.plot_context_masks_final", True)
    )

    main_logger.info(f"Simulation finished. Total time: {time.time() - start_time:.2f}s. Output: {output_dir}")

if __name__ == "__main__":
    temp_config_path = "config.yaml"
    if not os.path.exists(temp_config_path):
        try:
            # Assuming config_manager can create a default if you implement it
            # config_manager.create_default_config(temp_config_path)
            print(f"Warning: {temp_config_path} not found. Attempting to run with default parameters where possible.")
            print("Please create a config.yaml or provide one via --config argument.")
            # Create a minimal dummy config if none exists, so get_param doesn't fail immediately
            if not os.path.exists(temp_config_path):
                 with open(temp_config_path, "w") as f_cfg:
                    f_cfg.write("paths:\n  input_data_dir: \"data\"\n  output_dir: \"output/simulation_results\"\n")
                 print(f"Created minimal dummy {temp_config_path}. Please customize it.")
        except Exception as e: print(f"Could not create dummy config: {e}")

    # Ensure data and output directories are attempted to be created based on a minimal config load
    loaded_temp_config = {}
    if os.path.exists(temp_config_path):
        try: loaded_temp_config = config_manager.load_config(temp_config_path)
        except: print(f"Could not load {temp_config_path} for directory creation.")
    
    os.makedirs(config_manager.get_param(loaded_temp_config, "paths.input_data_dir", "data"), exist_ok=True)
    os.makedirs(config_manager.get_param(loaded_temp_config, "paths.output_dir", "output"), exist_ok=True)
    
    main()import numpy as np
import nibabel as nib
from skimage import measure, morphology # Removed filters as it wasn't used
from scipy import ndimage
# from scipy.spatial import distance_matrix # Not used in the revised extract_centerline_paths
import networkx as nx
# from sklearn.neighbors import KDTree # Replaced with scipy.spatial.KDTree for consistency
from scipy.spatial import KDTree # Using scipy's KDTree
import os
from typing import Union, Tuple
from tqdm import tqdm
import logging


logger = logging.getLogger(__name__) # Or use print() if you prefer for this standalone script
if not logger.hasHandlers(): # Basic logging setup if running standalone
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


EPSILON = 1e-9



# Function load_mask is ALREADY CORRECT - it returns affine
def load_mask(mask_path):
    """Load mask from various file formats"""
    if mask_path.endswith(('.nii', '.nii.gz')):
        img = nib.load(mask_path)
        # Ensure data is boolean or int for morphology operations
        data = (img.get_fdata() > 0).astype(np.uint8)
        return data, img.affine
    elif mask_path.endswith('.npy'):
        data = (np.load(mask_path) > 0).astype(np.uint8)
        return data, np.eye(4) # Assume identity affine for .npy
    else:
        raise ValueError(f"Unsupported format: {mask_path}")

# remove_small_components seems okay

def remove_small_components(mask, min_size=1000):
    """Remove disconnected components smaller than min_size"""
    labeled_mask, num_features = ndimage.label(mask) # Use ndimage.label
    if num_features == 0:
        # raise ValueError("No connected components found in mask") # Or return empty mask
        print("Warning: No connected components found in mask during remove_small_components.")
        return np.zeros_like(mask, dtype=bool)

    component_sizes = np.bincount(labeled_mask.ravel())
    # component_sizes[0] is background, find largest foreground component
    if len(component_sizes) > 1:
        largest_component_label = component_sizes[1:].argmax() + 1
        largest_component_area = component_sizes[largest_component_label]

        if largest_component_area < min_size : # Check if even the largest is too small
            print(f"Largest component ({largest_component_area} voxels) is smaller than min_size ({min_size}). Returning empty mask.")
            return np.zeros_like(mask, dtype=bool)

        clean_mask = (labeled_mask == largest_component_label)
        print(f"Kept largest component with {largest_component_area} voxels")
        print(f"Removed {num_features - 1} smaller or zero-sized components")
    else: # Only background or no components
        clean_mask = np.zeros_like(mask, dtype=bool)
        print("No foreground components found or only background present.")

    return clean_mask.astype(mask.dtype) # Return with original dtype

# extract_skeleton_3d seems okay

def extract_skeleton_3d(mask):
    """Extract 3D skeleton using medial axis transform"""
    # Ensure mask is boolean for skeletonization
    mask_bool = (mask > 0)

    # Optional: smooth the mask slightly to reduce artifacts
    # mask_bool = ndimage.binary_closing(mask_bool, iterations=1)
    # mask_bool = ndimage.binary_opening(mask_bool, iterations=1)
    # These might remove thin structures, use with caution or make them optional

    distance = ndimage.distance_transform_edt(mask_bool)
    skeleton = morphology.skeletonize(mask_bool)
    
    return skeleton, distance


def skeleton_to_graph(skeleton, distance_map):
    """Convert skeleton to networkx graph with node positions (voxel coords) and radii (voxel units)"""
    points_voxel = np.argwhere(skeleton > 0) # These are voxel coordinates (i,j,k)
    
    if len(points_voxel) == 0:
        raise ValueError("No skeleton points found")
    print(f"Found {len(points_voxel)} skeleton points")
    
    tree = KDTree(points_voxel)
    G = nx.Graph()
    
    for i, point_vox in enumerate(points_voxel):
        radius_vox_units = distance_map[tuple(point_vox)]
        # Store VOXEL coordinates and radius in VOXEL units
        G.add_node(i, pos_voxel=point_vox, radius_voxel_units=radius_vox_units)
    
    for i, point_vox in enumerate(points_voxel):
        # Neighbors in voxel space, r=sqrt(3) for 26-connectivity is fine
        neighbors_indices = tree.query_ball_point(point_vox.reshape(1, -1), r=1.733)[0] # sqrt(3) ~ 1.732
        for j in neighbors_indices:
            if j > i: G.add_edge(i, j)
    
    return G # points_voxel is not needed as output, it's in G.nodes[n]['pos_voxel']

# prune_skeleton_graph seems okay (operates on graph topology)

def prune_skeleton_graph(G, min_branch_length=5):
    """Optimized pruning with progress tracking"""
    print(f"Starting pruning with min_branch_length={min_branch_length}")
    nodes_to_remove = set()
    
    # Iteratively remove short endpoint branches
    while True:
        endpoints = [n for n in G.nodes() if G.degree(n) == 1 and n not in nodes_to_remove]
        if not endpoints: break
        
        removed_in_pass = 0
        for endpoint in tqdm(endpoints, desc="Checking endpoints", leave=False):
            if endpoint in nodes_to_remove: continue
            
            branch = [endpoint]
            current = endpoint
            valid_branch = True
            
            for _ in range(min_branch_length -1): # Trace up to min_branch_length
                neighbors = [n for n in G.neighbors(current) if n not in branch and n not in nodes_to_remove]
                if len(neighbors) != 1: # Reached junction, another endpoint, or already processed part
                    valid_branch = (G.degree(current) > 2 if len(neighbors) > 1 else False) # Valid if ended at junction
                    break
                current = neighbors[0]
                branch.append(current)
            else: # Loop completed without break, means branch is at least min_branch_length or ended at junction
                valid_branch = (G.degree(current) > 2 or G.degree(current) == 1) # Valid if ended at junction or another endpoint

            if not valid_branch and len(branch) < min_branch_length :
                # Mark branch for removal (excluding the potential junction it connects to)
                # If the last point in branch is a junction, don't remove it.
                # If the branch is shorter than min_length and doesn't end in a junction, remove all of it.
                last_node_in_branch = branch[-1]
                if G.degree(last_node_in_branch) > 2 and last_node_in_branch not in nodes_to_remove:
                     nodes_to_remove.update(branch[:-1])
                     removed_in_pass += len(branch[:-1])
                else: # Short branch not ending in a clear junction, or ending in another endpoint
                     nodes_to_remove.update(branch)
                     removed_in_pass += len(branch)
        
        if removed_in_pass == 0: break # No more short branches removed in a full pass

    G.remove_nodes_from(list(nodes_to_remove)) # Convert set to list for removal
    print(f"Removed {len(nodes_to_remove)} nodes from short branches.")
    
    isolated = list(nx.isolates(G))
    G.remove_nodes_from(isolated)
    print(f"Removed {len(isolated)} isolated nodes")
    
    if len(G) > 0:
        components = list(nx.connected_components(G))
        if len(components) > 1:
            largest_cc = max(components, key=len)
            nodes_to_keep = set(largest_cc)
            nodes_to_remove_cc = set(G.nodes()) - nodes_to_keep
            G.remove_nodes_from(list(nodes_to_remove_cc))
            print(f"Kept largest connected component ({len(largest_cc)} nodes)")
    
    return G


def extract_centerline_paths(G):
    """Extract main centerline paths from graph. Points are in VOXEL coordinates."""
    if len(G) == 0: raise ValueError("Graph is empty after pruning")
    
    endpoints = [n for n in G.nodes() if G.degree(n) == 1]
    print(f"Found {len(endpoints)} endpoints in pruned graph for path extraction.")
    
    if len(endpoints) < 2:
        nodes = list(G.nodes())
        if len(nodes) < 2: raise ValueError("Not enough nodes for centerline extraction")
        # Fallback: use most distant nodes if too few endpoints
        positions_voxel = np.array([G.nodes[n]['pos_voxel'] for n in nodes])
        from scipy.spatial import distance_matrix # Moved import here
        dist_matrix = distance_matrix(positions_voxel, positions_voxel)
        i_dm, j_dm = np.unravel_index(dist_matrix.argmax(), dist_matrix.shape)
        endpoints = [nodes[i_dm], nodes[j_dm]]
        print(f"Using {len(endpoints)} most distant nodes as effective endpoints: {endpoints}")
    
    # Heuristic for inlet: endpoint with largest radius (in voxel units from distance_map)
    inlet = max(endpoints, key=lambda n: G.nodes[n]['radius_voxel_units']) if endpoints else None
    if inlet is None: raise ValueError("Could not determine an inlet node.")

    outlets = [e for e in endpoints if e != inlet]
    if not outlets and len(endpoints) == 1 : # If only one endpoint, it becomes an outlet, graph is a single path
        outlets = endpoints 
    elif not outlets and len(endpoints) > 1: # Should not happen if inlet is one of them
        logger.warning("No distinct outlets found even with multiple endpoints. Using first non-inlet endpoint.")
        outlets = [ep for ep in endpoints if ep != inlet][:1]


    centerlines = []
    for outlet in outlets:
        try:
            path_node_ids = nx.shortest_path(G, inlet, outlet)
            # Points are still VOXEL coordinates here
            centerline_points_voxel = np.array([G.nodes[n]['pos_voxel'] for n in path_node_ids])
            centerline_radii_voxel_units = np.array([G.nodes[n]['radius_voxel_units'] for n in path_node_ids])
            centerlines.append({
                'points_voxel': centerline_points_voxel,
                'radii_voxel_units': centerline_radii_voxel_units
            })
        except nx.NetworkXNoPath:
            print(f"No path found from inlet {inlet} to outlet {outlet}")
            
    if not centerlines and len(G) > 0 and len(endpoints) >=2 : # Fallback if no inlet-outlet paths found but graph exists
        logger.warning("No inlet-to-outlet paths found. Attempting longest path as a single centerline.")
        # ... (your longest path fallback logic can be adapted here if needed) ...
        # For now, we'll rely on inlet/outlet logic. If it fails, centerlines list will be empty.


    return centerlines, inlet, outlets

def smooth_centerline(points_voxel, radii_voxel_units, smoothing_factor=0.1):
    """Smooth centerline (voxel coords) and radii (voxel units) using spline interpolation"""
    from scipy.interpolate import UnivariateSpline
    if len(points_voxel) < 4: return points_voxel, radii_voxel_units # Spline needs enough points
    
    diffs = np.diff(points_voxel, axis=0)
    distances = np.sqrt(np.sum(diffs**2, axis=1))
    arc_length = np.concatenate([[0], np.cumsum(distances)])
    
    smoothed_points_voxel = np.zeros_like(points_voxel)
    for i in range(points_voxel.shape[1]): # Iterate over dimensions (0, 1, 2)
        try:
            spline = UnivariateSpline(arc_length, points_voxel[:, i], s=smoothing_factor * len(points_voxel))
            smoothed_points_voxel[:, i] = spline(arc_length)
        except Exception as e:
            print(f"Warning: Spline smoothing failed for coordinate {i}, keeping original. Error: {e}")
            smoothed_points_voxel[:, i] = points_voxel[:, i]
    
    try:
        radius_spline = UnivariateSpline(arc_length, radii_voxel_units, s=smoothing_factor * len(radii_voxel_units))
        smoothed_radii_voxel_units = radius_spline(arc_length)
        smoothed_radii_voxel_units[smoothed_radii_voxel_units < 0] = 0 # Ensure non-negative radii
    except Exception as e:
        print(f"Warning: Spline smoothing failed for radii, keeping original. Error: {e}")
        smoothed_radii_voxel_units = radii_voxel_units
    
    return smoothed_points_voxel, smoothed_radii_voxel_units

def transform_to_world(points_voxel: np.ndarray, radii_voxel_units: Union[np.ndarray, float], affine: np.ndarray) -> Tuple[np.ndarray, Union[np.ndarray, float]]:
    """Transforms voxel coordinates to world and scales radii to physical units.
    Handles both array and scalar radii_voxel_units.
    """
    points_voxel = np.asarray(points_voxel) # Ensure it's an array
    if points_voxel.ndim == 1:
        points_voxel = points_voxel.reshape(1, -1) # Make it (1,3) if it's a single point (3,)

    homogeneous_coords = np.hstack((points_voxel, np.ones((points_voxel.shape[0], 1))))
    world_coords_homogeneous = homogeneous_coords @ affine.T
    points_world = world_coords_homogeneous[:, :3]

    voxel_spacing = np.abs(np.diag(affine)[:3])
    # Use only non-zero spacings for mean to avoid issues if an axis has zero spacing (e.g. 2D data in 3D affine)
    valid_spacings = voxel_spacing[voxel_spacing > EPSILON] # Assuming constants.EPSILON is defined
    if len(valid_spacings) > 0:
        mean_spacing = np.mean(valid_spacings)
    else:
        mean_spacing = 1.0 # Fallback if all spacings are zero (unlikely for 3D)
        print("Warning: All voxel spacings are zero or near zero. Using mean_spacing=1.0 for radius conversion.")

    radii_world = np.asarray(radii_voxel_units) * mean_spacing # Works if radii_voxel_units is scalar or array
    
    # If original points_voxel was a single point (ndim initially 1), return single point_world and scalar radius
    if world_coords_homogeneous.shape[0] == 1:
        return points_world[0], radii_world.item() if radii_world.ndim == 0 else radii_world[0] # Ensure scalar radius if input was scalar
    return points_world, radii_world

def save_centerlines_vtk(centerlines_data, output_path, affine): # Added affine
    """Save centerlines in VTK format with coordinates in WORLD space."""
    try:
        import vtk
        
        polyData = vtk.vtkPolyData()
        points_vtk = vtk.vtkPoints()
        lines_vtk = vtk.vtkCellArray()
        radius_vtk_array = vtk.vtkDoubleArray()
        radius_vtk_array.SetName("Radius") # Standard name
        radius_vtk_array.SetNumberOfComponents(1)

        point_id_counter = 0
        for cl_dict in centerlines_data: # cl_dict contains 'points_voxel' and 'radii_voxel_units'
            points_voxel = cl_dict['points_voxel']
            radii_voxel_units = cl_dict['radii_voxel_units']

            # Transform to world space for saving
            points_world, radii_world = transform_to_world(points_voxel, radii_voxel_units, affine)

            line = vtk.vtkPolyLine()
            line.GetPointIds().SetNumberOfIds(len(points_world))
            
            for j, (point_w, radius_w) in enumerate(zip(points_world, radii_world)):
                points_vtk.InsertNextPoint(point_w) # Save WORLD coordinates
                radius_vtk_array.InsertNextTuple1(radius_w) # Save WORLD radii
                line.GetPointIds().SetId(j, point_id_counter)
                point_id_counter += 1
            
            lines_vtk.InsertNextCell(line)
        
        polyData.SetPoints(points_vtk)
        polyData.SetLines(lines_vtk)
        polyData.GetPointData().AddArray(radius_vtk_array)
        polyData.GetPointData().SetActiveScalars("Radius")
        
        writer = vtk.vtkXMLPolyDataWriter()
        writer.SetFileName(output_path)
        writer.SetInputData(polyData)
        writer.Write()
        print(f"Saved VTK file with world coordinates: {output_path}")
    except ImportError: print("VTK Python library not available, skipping VTK export.")
    except Exception as e: print(f"Error saving VTK file {output_path}: {e}")

def save_centerlines_txt(centerlines_data, output_path, affine): # Added affine
    """Save centerlines in simple text format with WORLD coordinates and radii."""
    with open(output_path, 'w') as f:
        f.write("# Centerline data in WORLD coordinates\n")
        f.write("# Format: centerline_id point_id x_world y_world z_world radius_world\n")
        
        for cl_id, cl_dict in enumerate(centerlines_data):
            points_voxel = cl_dict['points_voxel']
            radii_voxel_units = cl_dict['radii_voxel_units']
            points_world, radii_world = transform_to_world(points_voxel, radii_voxel_units, affine)

            f.write(f"\n# Centerline {cl_id}\n")
            for pt_id, (point_w, radius_w) in enumerate(zip(points_world, radii_world)):
                f.write(f"{cl_id} {pt_id} {point_w[0]:.6f} {point_w[1]:.6f} "
                       f"{point_w[2]:.6f} {radius_w:.6f}\n")
    print(f"Saved TXT file with world coordinates: {output_path}")


def extract_centerlines_from_mask(mask_path, output_dir,
                                 min_branch_length=10,
                                 smoothing_factor=0.1,
                                 min_component_size=1000):
    os.makedirs(output_dir, exist_ok=True)

    logger.info("Loading mask...")
    mask, affine_matrix_from_input_mask = load_mask(mask_path)

    # --- ADD THIS SECTION FOR DEBUGGING AFFINE ---
    logger.info(f"--- AFFINE DEBUG ---")
    logger.info(f"Affine matrix loaded from {mask_path}:\n{affine_matrix_from_input_mask}")
    try:
        # Ensure nibabel is imported (it usually is if you're using nib.load)
        # If not, add 'import nibabel as nib' at the top of this function or the script
        ax_codes = nib.aff2axcodes(affine_matrix_from_input_mask)
        logger.info(f"Corresponding axis codes (e.g., RAS, LPS): {ax_codes}")
    except Exception as e:
        logger.warning(f"Could not determine axis codes from affine: {e}")
    logger.info(f"--------------------")
    # --- END OF AFFINE DEBUG SECTION ---


    logger.info(f"--- EXTRACT_CENTERLINES.PY ---") # This log line was already there
    logger.info(f"Using mask: {mask_path}")
    logger.info(f"Mask shape: {mask.shape}, Affine loaded with this mask (used for VTP world coords):\n{affine_matrix_from_input_mask}") # This line also prints the affine, redundant but fine
    logger.info(f"-------------------------------")

    logger.info("Removing disconnected components...")
    mask_cleaned = remove_small_components(mask, min_component_size)
    if not np.any(mask_cleaned):
        raise ValueError("Mask is empty after removing small components.")

    logger.info("Extracting skeleton...")
    skeleton, distance_map = extract_skeleton_3d(mask_cleaned)
    if not np.any(skeleton):
        raise ValueError("Skeleton is empty. Check mask or skeletonization parameters.")
    
    # Save skeleton for debugging (optional, but can be useful)
    # np.save(os.path.join(output_dir, "debug_skeleton_voxels.npy"), skeleton)

    logger.info("Converting skeleton to graph...")
    # G has pos_voxel, radius_voxel_units
    G = skeleton_to_graph(skeleton, distance_map)
    logger.info(f"Initial graph: {len(G.nodes())} N, {len(G.edges())} E")
    if len(G.nodes()) == 0:
        raise ValueError("Graph from skeleton is empty.")

    logger.info("Pruning small branches...")
    # Pass a copy for pruning
    G_pruned = prune_skeleton_graph(G.copy(), min_branch_length)
    logger.info(f"Pruned graph: {len(G_pruned.nodes())} N, {len(G_pruned.edges())} E")
    if len(G_pruned.nodes()) == 0:
        raise ValueError("Graph empty after pruning.")

    logger.info("Extracting centerline paths...")
    # centerlines will contain dicts with 'points_voxel' and 'radii_voxel_units'
    centerlines_voxel, inlet_node_idx, outlet_node_indices = extract_centerline_paths(G_pruned)
    if not centerlines_voxel:
        raise ValueError("No centerlines extracted.")

    # Terminal information (still in voxel coordinates and units initially)
    terminals_voxel = {}
    if inlet_node_idx in G_pruned: # Check if node exists in the potentially pruned graph
        terminals_voxel['inlet_pos_voxel'] = G_pruned.nodes[inlet_node_idx]['pos_voxel']
        terminals_voxel['inlet_radius_voxel_units'] = G_pruned.nodes[inlet_node_idx]['radius_voxel_units']
    else:
        logger.warning(f"Inlet node index {inlet_node_idx} not found in pruned graph G_pruned. Inlet info might be incomplete.")
        # Fallback or error handling might be needed if inlet is critical
        terminals_voxel['inlet_pos_voxel'] = None
        terminals_voxel['inlet_radius_voxel_units'] = None


    terminals_voxel['outlets_pos_voxel'] = [G_pruned.nodes[n]['pos_voxel'] for n in outlet_node_indices if n in G_pruned]
    terminals_voxel['outlets_radii_voxel_units'] = [G_pruned.nodes[n]['radius_voxel_units'] for n in outlet_node_indices if n in G_pruned]

    # Smooth centerlines (still in voxel space)
    if smoothing_factor > 0:
        logger.info("Smoothing centerlines...")
        for cl_dict in centerlines_voxel:
            cl_dict['points_voxel'], cl_dict['radii_voxel_units'] = smooth_centerline(
                cl_dict['points_voxel'], cl_dict['radii_voxel_units'], smoothing_factor)

    logger.info("Saving results...")
    # Save centerlines as VTK and TXT (now with world coordinates)
    # Pass the correct affine matrix
    save_centerlines_vtk(centerlines_voxel, os.path.join(output_dir, "centerlines_world.vtp"), affine_matrix_from_input_mask)
    save_centerlines_txt(centerlines_voxel, os.path.join(output_dir, "centerlines_world.txt"), affine_matrix_from_input_mask)

    # Save terminal information in WORLD coordinates
    with open(os.path.join(output_dir, "terminals_world.txt"), 'w') as f:
        f.write("# Terminal data in WORLD coordinates\n")
        f.write("# Format: Type X_world Y_world Z_world Radius_world\n")
        if terminals_voxel.get('inlet_pos_voxel') is not None and terminals_voxel.get('inlet_radius_voxel_units') is not None:
            inlet_w_coord, inlet_r_w_scalar = transform_to_world(
                terminals_voxel['inlet_pos_voxel'],
                terminals_voxel['inlet_radius_voxel_units'],
                affine_matrix_from_input_mask # Use correct affine
            )
            f.write(f"Inlet: {inlet_w_coord[0]:.6f}, {inlet_w_coord[1]:.6f}, {inlet_w_coord[2]:.6f}, radius: {inlet_r_w_scalar:.6f}\n")
        
        if terminals_voxel.get('outlets_pos_voxel'): # Check if key exists
            for i, (outlet_v_vox, radius_v_vox) in enumerate(zip(terminals_voxel['outlets_pos_voxel'], terminals_voxel['outlets_radii_voxel_units'])):
                if outlet_v_vox is None or radius_v_vox is None: continue

                outlet_w_coord, outlet_r_w_scalar = transform_to_world(
                    outlet_v_vox, radius_v_vox, affine_matrix_from_input_mask # Use correct affine
                )
                f.write(f"Outlet_{i+1}: {outlet_w_coord[0]:.6f}, {outlet_w_coord[1]:.6f}, {outlet_w_coord[2]:.6f}, radius: {outlet_r_w_scalar:.6f}\n")

    centerline_data_to_save = {
        'centerlines_voxel_data': centerlines_voxel,
        'terminals_voxel_data': terminals_voxel,
        'affine_matrix': affine_matrix_from_input_mask # Save the affine used
    }
    np.save(os.path.join(output_dir, "centerline_data_with_affine.npy"),
            centerline_data_to_save, allow_pickle=True)

    logger.info(f"\nResults saved to: {output_dir}")
    logger.info("Output VTK and TXT files now contain coordinates in WORLD space based on input mask's affine.")

    return centerlines_voxel, terminals_voxel, affine_matrix_from_input_mask

# Visualization function
def visualize_centerlines(mask_path, centerline_data_path):
    """Simple visualization using matplotlib"""
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    
    # Load data
    mask, _ = load_mask(mask_path)
    data = np.load(centerline_data_path, allow_pickle=True).item()
    centerlines = data['centerlines']
    terminals = data['terminals']
    
    fig = plt.figure(figsize=(12, 10))
    
    # 3D plot
    ax = fig.add_subplot(111, projection='3d')
    
    # Plot centerlines with radius as line width
    for cl in centerlines:
        points = cl['points']
        radii = cl['radii']
        
        # Plot centerline
        ax.plot(points[:, 0], points[:, 1], points[:, 2], 'b-', linewidth=2)
        
        # Optionally plot radius at some points
        step = max(1, len(points) // 20)
        ax.scatter(points[::step, 0], points[::step, 1], points[::step, 2], 
                  s=radii[::step]*20, c='cyan', alpha=0.5)
    
    # Plot terminals
    ax.scatter(*terminals['inlet'], c='red', s=200, marker='o', label='Inlet')
    for i, outlet in enumerate(terminals['outlets']):
        ax.scatter(*outlet, c='green', s=150, marker='o', 
                  label='Outlet' if i == 0 else '')
    
    ax.set_xlabel('X (mm)')
    ax.set_ylabel('Y (mm)')
    ax.set_zlabel('Z (mm)')
    ax.legend()
    ax.set_title('Extracted Arterial Centerlines')
    
    plt.tight_layout()
    plt.show()

# Example usage
if __name__ == "__main__":
    mask_path = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/mida_processed_masks/mida_arteries_mask.nii.gz" # Example
    output_directory = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/data/arteries_mida_world" # New output
    
    try:
        _, _, _ = extract_centerlines_from_mask( # We don't need to assign the return for this test run
            mask_path=mask_path,
            output_dir=output_directory,
            min_branch_length=5, # Adjusted for potentially finer skeletons
            smoothing_factor=0.05,
            min_component_size=500
        )
        print("\nExtraction completed successfully!")
    except Exception as e:
        print(f"\nError during extraction: {e}")
        import traceback
        traceback.print_exc()
        
        # Optional: visualize results
        # visualize_centerlines(mask_path, 
        #                      os.path.join(output_directory, "centerline_data.npy"))
        
    except Exception as e:
        print(f"\nError during extraction: {e}")
        print("\nTroubleshooting tips:")
        print("1. Ensure your mask is binary (0 and 1 values only)")
        print("2. Check that your mask contains a connected arterial structure")
        print("3. Try reducing min_branch_length if graph is empty after pruning")
        print("4. Verify mask file format is supported (.nii, .nii.gz, .npy)")import nibabel as nib
import numpy as np
from scipy.ndimage import binary_fill_holes, generate_binary_structure, binary_closing
import os

# --- Configuration ---
base_path = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/data/mida_reoriented_NETWORK_v1/other_reoriented_masks_TARGET_RAS" # Or wherever your files are
gm_mask_path = os.path.join(base_path, "mida_total_gm_mask_reoriented_RAS.nii.gz")
wm_mask_path = os.path.join(base_path, "mida_total_wm_mask_reoriented_RAS.nii.gz") # ADDED
csf_ventricle_mask_path = os.path.join(base_path, "mida_total_csf_mask_reoriented_RAS.nii.gz")
output_solid_parenchyma_path = os.path.join(base_path, "brain_solid_parenchyma_mask.nii.gz")
# NEW output path for cavity-filled GM
output_cavity_filled_gm_path = os.path.join(base_path, "grey_matter_cavity_filled_mask.nii.gz")


# --- Helper function to load and validate masks ---
def load_mask(path, reference_shape=None, mask_name="Mask"):
    print(f"Loading {mask_name} from: {path}")
    try:
        nii = nib.load(path)
        data = nii.get_fdata().astype(bool) # Ensure binary
        if reference_shape and data.shape != reference_shape:
            print(f"ERROR: {mask_name} shape {data.shape} does not match reference shape {reference_shape}")
            raise ValueError(f"{mask_name} shape mismatch")
        print(f"{mask_name} shape: {data.shape}, Unique values: {np.unique(data)}")
        return nii, data
    except FileNotFoundError:
        print(f"ERROR: {mask_name} not found at {path}")
        raise
    except Exception as e:
        print(f"ERROR: Could not load {mask_name}: {e}")
        raise

# --- Step 1: Load GM, WM, and CSF masks ---
try:
    gm_nii, gm_data = load_mask(gm_mask_path, mask_name="Grey Matter")
    ref_shape = gm_data.shape

    wm_nii, wm_data = load_mask(wm_mask_path, reference_shape=ref_shape, mask_name="White Matter")
    csf_nii, csf_data = load_mask(csf_ventricle_mask_path, reference_shape=ref_shape, mask_name="CSF-Ventricle")
except Exception:
    print("Exiting due to mask loading errors.")
    exit()

# --- Step 2: Combine GM and WM masks ---
print("Combining GM and WM masks...")
initial_parenchyma_data = gm_data | wm_data
print(f"Initial Parenchyma (GM|WM) voxels: {np.sum(initial_parenchyma_data)}")

# --- Step 3: Fill ALL internal holes in the combined GM+WM mask ---
print("Filling all internal holes in the combined GM+WM mask (binary_fill_holes)...")
# fill_structure = generate_binary_structure(gm_data.ndim, gm_data.ndim) # Max connectivity
# filled_parenchyma_candidate = binary_fill_holes(initial_parenchyma_data, structure=fill_structure)
filled_parenchyma_candidate = binary_fill_holes(initial_parenchyma_data) # Default structure often fine
print(f"Filled Parenchyma Candidate voxels: {np.sum(filled_parenchyma_candidate)}")

# --- Step 4: Subtract the CSF-Ventricle mask to get solid parenchyma ---
print("Subtracting CSF-Ventricle mask to get solid parenchyma...")
solid_parenchyma_no_ventricles_data = filled_parenchyma_candidate & (~csf_data)
print(f"Solid Parenchyma (no ventricles) voxels: {np.sum(solid_parenchyma_no_ventricles_data)}")

# --- Step 5: Save the solid parenchyma mask (this was final_domain_mask_data before) ---
output_solid_parenchyma_int = solid_parenchyma_no_ventricles_data.astype(np.uint8)
output_solid_parenchyma_nii = nib.Nifti1Image(output_solid_parenchyma_int, gm_nii.affine, gm_nii.header)
print(f"Saving the solid parenchyma mask to: {output_solid_parenchyma_path}")
try:
    nib.save(output_solid_parenchyma_nii, output_solid_parenchyma_path)
    print("Solid parenchyma mask saved successfully.")
except Exception as e:
    print(f"ERROR: Could not save solid parenchyma mask: {e}")


# --- Step 6: Create Cavity-Filled Grey Matter mask ---
# Subtract White Matter from the solid parenchyma (which already has ventricles removed)
print("Creating cavity-filled Grey Matter mask by subtracting WM...")
cavity_filled_gm_data = solid_parenchyma_no_ventricles_data & (~wm_data)
print(f"Cavity-filled GM voxels: {np.sum(cavity_filled_gm_data)}")


# --- Step 7: Save the Cavity-Filled Grey Matter mask ---
output_cavity_filled_gm_int = cavity_filled_gm_data.astype(np.uint8)
output_cavity_filled_gm_nii = nib.Nifti1Image(output_cavity_filled_gm_int, gm_nii.affine, gm_nii.header) # Use original GM affine/header

print(f"Saving the cavity-filled GM mask to: {output_cavity_filled_gm_path}")
try:
    nib.save(output_cavity_filled_gm_nii, output_cavity_filled_gm_path)
    print("Cavity-filled GM mask saved successfully.")
except Exception as e:
    print(f"ERROR: Could not save cavity-filled GM mask: {e}")


# --- Verification ---
print("\nVerification (counts of voxels):")
print(f"Original GM: {np.sum(gm_data)}")
print(f"Original WM: {np.sum(wm_data)}")
print(f"GM | WM (Initial Parenchyma): {np.sum(initial_parenchyma_data)}")
print(f"CSF/Ventricle: {np.sum(csf_data)}")
print(f"Filled Parenchyma Candidate (GM+WM+InternalVoids+Ventricles_implicit): {np.sum(filled_parenchyma_candidate)}")
removed_by_csf_mask = np.sum(filled_parenchyma_candidate & csf_data)
print(f"Voxels removed by CSF mask subtraction: {removed_by_csf_mask}")
print(f"Solid Parenchyma (no ventricles): {np.sum(solid_parenchyma_no_ventricles_data)}")

# Check consistency
expected_solid_parenchyma = np.sum(filled_parenchyma_candidate) - removed_by_csf_mask
if np.sum(solid_parenchyma_no_ventricles_data) == expected_solid_parenchyma:
    print("Solid Parenchyma voxel count matches expectation.")
else:
    print("Solid Parenchyma voxel count discrepancy. This can occur if CSF mask extends beyond the filled parenchyma candidate.")

print(f"--- For Cavity-Filled GM ---")
# What part of the solid parenchyma was also original WM?
wm_in_solid_parenchyma = np.sum(solid_parenchyma_no_ventricles_data & wm_data)
print(f"WM component within Solid Parenchyma: {wm_in_solid_parenchyma}")
print(f"Cavity-Filled GM (Solid Parenchyma - WM): {np.sum(cavity_filled_gm_data)}")

expected_cavity_filled_gm = np.sum(solid_parenchyma_no_ventricles_data) - wm_in_solid_parenchyma
if np.sum(cavity_filled_gm_data) == expected_cavity_filled_gm:
    print("Cavity-filled GM voxel count matches expectation.")
else:
    print("Cavity-filled GM voxel count discrepancy.")

# Compare with original GM
# Voxels gained in cavity_filled_gm compared to original gm_data
# These are the "filled cavities" within the GM territory
gained_voxels_in_gm = np.sum(cavity_filled_gm_data & (~gm_data))
print(f"Voxels in Cavity-Filled GM that were NOT in original GM (filled cavities): {gained_voxels_in_gm}")
# Voxels lost (should be 0 if logic is sound and wm_data doesn't overlap gm_data significantly)
lost_voxels_from_gm = np.sum(gm_data & (~cavity_filled_gm_data))
print(f"Voxels in original GM that are NOT in Cavity-Filled GM (should be minimal/zero): {lost_voxels_from_gm}")
import numpy as np
import nibabel as nib
import nibabel.orientations as nio
from skimage import measure, morphology # Ensure morphology is imported
from scipy import ndimage
import networkx as nx
from scipy.spatial import KDTree
import os
from typing import Union, Tuple, List, Dict
from tqdm import tqdm # Keep if you want progress bars in pruning, or remove if not used
import logging
import vtk

# --- Configuration ---
TARGET_ORIENTATION_AXCODES = ('R', 'A', 'S') # Target canonical orientation (Right, Anterior, Superior)
EPSILON = 1e-9

# --- Logger Setup ---
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Utility Functions ---

def load_and_reorient_mask(mask_path: str, target_axcodes: Tuple[str, str, str] = TARGET_ORIENTATION_AXCODES) -> Union[Tuple[np.ndarray, np.ndarray], Tuple[None, None]]:
    """
    Load a mask, reorient its data and affine to the target axis codes.
    Returns the reoriented mask data (numpy array) and its new affine, or (None, None) on failure.
    """
    if not (mask_path.endswith(('.nii', '.nii.gz'))):
        if mask_path.endswith('.npy'):
            logger.warning(f"Loading .npy file: {mask_path}. Assuming identity affine and no reorientation needed/possible.")
            try:
                data = (np.load(mask_path) > 0).astype(np.uint8)
                return data, np.eye(4)
            except Exception as e:
                logger.error(f"Failed to load .npy file {mask_path}: {e}")
                return None, None
        else:
            logger.error(f"Unsupported format for reorientation: {mask_path}. Only .nii, .nii.gz supported.")
            return None, None

    try:
        img_original = nib.load(mask_path)
    except Exception as e:
        logger.error(f"nibabel failed to load image {mask_path}: {e}")
        return None, None

    original_axcodes = nib.aff2axcodes(img_original.affine)
    logger.info(f"Loading mask: {mask_path}")
    logger.info(f"Original orientation: {original_axcodes}, Original shape: {img_original.shape}")

    if img_original.get_data_dtype() == object or not hasattr(img_original, 'get_fdata'):
        logger.error(f"Image {mask_path} has an unusual data type or is not a standard spatial image. Cannot get float data.")
        return None, None

    if original_axcodes == target_axcodes:
        logger.info(f"Mask is already in target orientation {target_axcodes}.")
        try:
            data_reoriented = (img_original.get_fdata(dtype=np.float32) > 0).astype(np.uint8)
            affine_reoriented = img_original.affine
        except Exception as e:
            logger.error(f"Failed to get data from already oriented image {mask_path}: {e}")
            return None, None
    else:
        logger.info(f"Reorienting mask to {target_axcodes}...")
        try:
            current_ornt = nio.io_orientation(img_original.affine)
            target_ornt = nio.axcodes2ornt(target_axcodes)
            transform_ornt = nio.ornt_transform(current_ornt, target_ornt)
            original_data = np.asanyarray(img_original.get_fdata(dtype=np.float32))
            data_reoriented_numpy = nio.apply_orientation(original_data, transform_ornt)
            data_reoriented = (data_reoriented_numpy > 0).astype(np.uint8)
            affine_reoriented = img_original.affine @ nio.inv_ornt_aff(transform_ornt, img_original.shape)
            
            final_axcodes = nib.aff2axcodes(affine_reoriented)
            logger.info(f"New orientation: {final_axcodes}, New shape: {data_reoriented.shape}")
            if final_axcodes != target_axcodes:
                logger.warning(f"Achieved axcodes {final_axcodes} after reorientation differ slightly from target {target_axcodes}. "
                               f"This can happen with complex initial affines. World space should still be {target_axcodes} oriented.")
        except IndexError as ie:
            logger.error(f"IndexError during nibabel reorientation for {mask_path}: {ie}.")
            return None, None
        except Exception as e:
            logger.error(f"Failed during reorientation process for {mask_path}: {e}")
            return None, None
    return data_reoriented, affine_reoriented

def save_mask_as_nifti(data: np.ndarray, affine: np.ndarray, output_path: str):
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        nifti_img = nib.Nifti1Image(data.astype(np.uint8), affine)
        nib.save(nifti_img, output_path)
        logger.info(f"Saved NIFTI mask to: {output_path}")
    except Exception as e:
        logger.error(f"Failed to save NIFTI file {output_path}: {e}")

# --- Centerline Extraction Functions ---
def remove_small_components(mask_data: np.ndarray, min_size=1000) -> np.ndarray:
    labeled_mask, num_features = ndimage.label(mask_data)
    if num_features == 0:
        logger.warning("No connected components found in mask_data during remove_small_components.")
        return np.zeros_like(mask_data, dtype=bool)
    component_sizes = np.bincount(labeled_mask.ravel())
    if len(component_sizes) > 1:
        largest_component_label = component_sizes[1:].argmax() + 1
        largest_component_area = component_sizes[largest_component_label]
        if largest_component_area < min_size:
            logger.warning(f"Largest component ({largest_component_area} voxels) is smaller than min_size ({min_size}). Returning empty mask.")
            return np.zeros_like(mask_data, dtype=bool)
        clean_mask_data = (labeled_mask == largest_component_label)
        logger.info(f"Kept largest component with {largest_component_area} voxels from mask_data.")
    else:
        clean_mask_data = np.zeros_like(mask_data, dtype=bool)
        logger.info("No foreground components found in mask_data.")
    return clean_mask_data.astype(mask_data.dtype)

def extract_skeleton_3d(mask_data: np.ndarray,
                        output_dir_for_debug: Union[str, None] = None,
                        reoriented_affine_for_debug: Union[np.ndarray, None] = None
                        ) -> Tuple[np.ndarray, np.ndarray]:
    """
    Extract 3D skeleton with pre-processing options.
    Includes saving of intermediate debug masks if paths are provided.
    """
    mask_bool = (mask_data > 0)
    logger.info(f"Initial mask_bool shape for skeletonization: {mask_bool.shape}, True voxels: {np.sum(mask_bool)}")

    # --- PRE-PROCESSING TO IMPROVE SKELETON ---
    # Experiment with these options. Option 2 (Open then Close) is often a good start.
    
    # Option 1: Binary Closing only
    # iterations_closing = 2
    # structure = ndimage.generate_binary_structure(3, 3) # 26-connectivity
    # logger.info(f"Applying binary closing ({iterations_closing} iter, 26-conn) before skeletonization.")
    # mask_bool_processed = ndimage.binary_closing(mask_bool, structure=structure, iterations=iterations_closing)

    # Option 2: Binary Opening then Closing
    iterations_opening = 1
    iterations_closing = 1 # May need more closing after opening
    logger.info(f"Applying binary opening ({iterations_opening} iter) then closing ({iterations_closing} iter).")
    structure_open = ndimage.generate_binary_structure(3, 1) # 6-connectivity (conservative opening)
    structure_close = ndimage.generate_binary_structure(3, 2) # 18-connectivity (moderate closing)
    # Or for more aggressive closing: structure_close = ndimage.generate_binary_structure(3, 3) 
    
    mask_bool_opened = ndimage.binary_opening(mask_bool, iterations=iterations_opening, structure=structure_open)
    logger.info(f"Mask after opening: True voxels: {np.sum(mask_bool_opened)}")
    mask_bool_processed = ndimage.binary_closing(mask_bool_opened, iterations=iterations_closing, structure=structure_close)
    logger.info(f"Mask after subsequent closing: True voxels: {np.sum(mask_bool_processed)}")
    
    # Option 3: No additional morphological operations
    # mask_bool_processed = mask_bool
    # logger.info("Using mask without additional pre-processing for skeletonization.")
    # --- END PRE-PROCESSING ---

    if output_dir_for_debug and reoriented_affine_for_debug is not None:
        debug_mask_path = os.path.join(output_dir_for_debug, "debug_mask_fed_to_skeletonize.nii.gz")
        save_mask_as_nifti(mask_bool_processed.astype(np.uint8), reoriented_affine_for_debug, debug_mask_path)

    if not np.any(mask_bool_processed):
        logger.warning("Mask is empty after pre-processing before skeletonization. Returning empty skeleton.")
        return np.zeros_like(mask_bool_processed, dtype=bool), np.zeros_like(mask_bool_processed, dtype=float)

    logger.info("Calculating distance transform...")
    distance = ndimage.distance_transform_edt(mask_bool_processed)
    
    logger.info("Performing skeletonization (skimage.morphology.skeletonize)...")
    skeleton = morphology.skeletonize(mask_bool_processed)
    logger.info(f"Skeletonization complete. Skeleton True voxels: {np.sum(skeleton)}")
    
    return skeleton, distance

def skeleton_to_graph(skeleton_data: np.ndarray, distance_map_data: np.ndarray) -> nx.Graph:
    points_voxel = np.argwhere(skeleton_data > 0)
    if len(points_voxel) == 0:
        raise ValueError("No skeleton points found in skeleton_data.")
    logger.info(f"Found {len(points_voxel)} skeleton points.")
    tree = KDTree(points_voxel)
    G = nx.Graph()
    for i, point_vox in enumerate(points_voxel):
        radius_vox_units = distance_map_data[tuple(point_vox)]
        G.add_node(i, pos_voxel=point_vox, radius_voxel_units=radius_vox_units)
    for i, point_vox in enumerate(points_voxel):
        neighbors_indices = tree.query_ball_point(point_vox.reshape(1, -1), r=1.733)[0]
        for j in neighbors_indices:
            if j > i: G.add_edge(i, j)
    return G

def prune_skeleton_graph(G: nx.Graph, min_branch_length: int = 5) -> nx.Graph:
    logger.info(f"Starting pruning with min_branch_length={min_branch_length}")
    G_pruned = G.copy()
    
    # Pruning logic from user's script (can be refined if needed)
    nodes_to_remove = set()
    while True:
        endpoints = [n for n in G_pruned.nodes() if G_pruned.degree(n) == 1 and n not in nodes_to_remove]
        if not endpoints: break
        
        removed_in_pass = 0
        for endpoint in endpoints: # tqdm can be added here if desired: tqdm(endpoints, desc="Checking endpoints", leave=False)
            if endpoint in nodes_to_remove: continue
            
            branch = [endpoint]
            current = endpoint
            valid_branch = True
            
            for _ in range(min_branch_length -1): 
                neighbors = [n for n in G_pruned.neighbors(current) if n not in branch and n not in nodes_to_remove]
                if len(neighbors) != 1: 
                    valid_branch = (G_pruned.degree(current) > 2 if len(neighbors) > 1 else False) 
                    break
                current = neighbors[0]
                branch.append(current)
            else: 
                valid_branch = (G_pruned.degree(current) > 2 or G_pruned.degree(current) == 1)

            if not valid_branch and len(branch) < min_branch_length :
                last_node_in_branch = branch[-1]
                if G_pruned.degree(last_node_in_branch) > 2 and last_node_in_branch not in nodes_to_remove and len(branch) > 1: # check len(branch)>1
                     nodes_to_remove.update(branch[:-1])
                     removed_in_pass += len(branch[:-1])
                else: 
                     nodes_to_remove.update(branch)
                     removed_in_pass += len(branch)
        
        if removed_in_pass == 0: break
    
    G_pruned.remove_nodes_from(list(nodes_to_remove))
    logger.info(f"Removed {len(nodes_to_remove)} nodes from short branches.")
    
    isolated = list(nx.isolates(G_pruned))
    G_pruned.remove_nodes_from(isolated)
    logger.info(f"Removed {len(isolated)} isolated nodes after pruning.")
    
    if len(G_pruned) > 0:
        components = list(nx.connected_components(G_pruned))
        if len(components) > 1:
            largest_cc = max(components, key=len)
            nodes_to_keep = set(largest_cc)
            nodes_to_remove_cc = set(G_pruned.nodes()) - nodes_to_keep
            G_pruned.remove_nodes_from(list(nodes_to_remove_cc))
            logger.info(f"Kept largest connected component ({len(largest_cc)} nodes), removed {len(nodes_to_remove_cc)} other nodes.")
    return G_pruned

def extract_centerline_paths_from_graph(G_pruned: nx.Graph) -> Tuple[List[Dict], Union[int, None], List[int]]:
    if len(G_pruned) == 0: raise ValueError("Graph is empty after pruning, cannot extract paths.")
    
    endpoints = [n for n in G_pruned.nodes() if G_pruned.degree(n) == 1]
    logger.info(f"Found {len(endpoints)} endpoints in pruned graph for path extraction.")
    
    inlet_node_idx = None
    outlet_node_indices = []

    if len(endpoints) < 2:
        if len(G_pruned.nodes()) < 2:
             logger.warning("Not enough nodes for centerline extraction even with fallback. Returning empty.")
             return [], None, []
        logger.warning("Less than 2 endpoints. Using most distant nodes as effective endpoints.")
        nodes = list(G_pruned.nodes())
        positions_voxel = np.array([G_pruned.nodes[n]['pos_voxel'] for n in nodes])
        from scipy.spatial.distance import cdist
        dist_matrix = cdist(positions_voxel, positions_voxel)
        np.fill_diagonal(dist_matrix, -np.inf) # Use -inf to ensure diagonal is not chosen
        i_dm, j_dm = np.unravel_index(dist_matrix.argmax(), dist_matrix.shape)
        endpoints = [nodes[i_dm], nodes[j_dm]]
        logger.info(f"Using {len(endpoints)} most distant nodes as effective endpoints: {endpoints}")

    if endpoints:
        inlet_node_idx = max(endpoints, key=lambda n: G_pruned.nodes[n]['radius_voxel_units'])
        outlet_node_indices = [e for e in endpoints if e != inlet_node_idx]
        if not outlet_node_indices and len(endpoints) == 1:
            outlet_node_indices = endpoints 
        elif not outlet_node_indices and len(endpoints) > 1:
             logger.warning("No distinct outlets found even with multiple endpoints. Using first non-inlet endpoint.")
             outlet_node_indices = [ep for ep in endpoints if ep != inlet_node_idx][:1]
    else:
        logger.warning("No endpoints identified. Cannot define inlet/outlets conventionally.")
        return [], None, []

    centerlines_voxel_data = []
    processed_outlets = set()
    if inlet_node_idx is not None: # Ensure inlet is defined
        for outlet_idx in outlet_node_indices:
            if outlet_idx == inlet_node_idx and len(outlet_node_indices) > 1: # Avoid path from inlet to itself if other outlets exist
                continue
            if outlet_idx in processed_outlets:
                continue
            try:
                path_node_ids = nx.shortest_path(G_pruned, inlet_node_idx, outlet_idx)
                centerline_points_voxel = np.array([G_pruned.nodes[n]['pos_voxel'] for n in path_node_ids])
                centerline_radii_voxel_units = np.array([G_pruned.nodes[n]['radius_voxel_units'] for n in path_node_ids])
                centerlines_voxel_data.append({
                    'points_voxel': centerline_points_voxel,
                    'radii_voxel_units': centerline_radii_voxel_units
                })
                processed_outlets.add(outlet_idx)
            except nx.NetworkXNoPath:
                logger.warning(f"No path found from inlet {inlet_node_idx} to outlet {outlet_idx}")
            except nx.NodeNotFound:
                logger.warning(f"Node not found during path search: inlet {inlet_node_idx} or outlet {outlet_idx}")
            
    if not centerlines_voxel_data and inlet_node_idx is not None and not outlet_node_indices and len(G_pruned.nodes) > 1:
        logger.info("Attempting to find longest path from single identified inlet/endpoint.")
        path_node_ids_lengths = nx.single_source_shortest_path_length(G_pruned, inlet_node_idx)
        if len(path_node_ids_lengths) > 1:
            furthest_node = max(path_node_ids_lengths, key=path_node_ids_lengths.get)
            try:
                actual_path = nx.shortest_path(G_pruned, inlet_node_idx, furthest_node)
                centerline_points_voxel = np.array([G_pruned.nodes[n]['pos_voxel'] for n in actual_path])
                centerline_radii_voxel_units = np.array([G_pruned.nodes[n]['radius_voxel_units'] for n in actual_path])
                centerlines_voxel_data.append({
                    'points_voxel': centerline_points_voxel,
                    'radii_voxel_units': centerline_radii_voxel_units
                })
                outlet_node_indices.append(furthest_node)
            except nx.NetworkXNoPath:
                 logger.warning(f"Fallback: No path found from inlet {inlet_node_idx} to furthest node {furthest_node}")
    return centerlines_voxel_data, inlet_node_idx, outlet_node_indices

def smooth_centerline(points_voxel: np.ndarray, radii_voxel_units: np.ndarray, smoothing_factor: float = 0.1):
    from scipy.interpolate import UnivariateSpline
    if len(points_voxel) < 4: return points_voxel, radii_voxel_units
    
    diffs = np.diff(points_voxel, axis=0)
    distances = np.sqrt(np.sum(diffs**2, axis=1))
    arc_length = np.concatenate([[0], np.cumsum(distances)])
    
    smoothed_points_voxel = np.zeros_like(points_voxel, dtype=float)
    for i in range(points_voxel.shape[1]):
        try:
            spline = UnivariateSpline(arc_length, points_voxel[:, i], s=smoothing_factor * len(points_voxel))
            smoothed_points_voxel[:, i] = spline(arc_length)
        except Exception as e:
            logger.warning(f"Spline smoothing failed for coordinate {i}, keeping original. Error: {e}")
            smoothed_points_voxel[:, i] = points_voxel[:, i]
    
    try:
        radius_spline = UnivariateSpline(arc_length, radii_voxel_units, s=smoothing_factor * len(radii_voxel_units))
        smoothed_radii_voxel_units = radius_spline(arc_length)
        smoothed_radii_voxel_units[smoothed_radii_voxel_units < 0] = 0
    except Exception as e:
        logger.warning(f"Spline smoothing failed for radii, keeping original. Error: {e}")
        smoothed_radii_voxel_units = radii_voxel_units
    
    return smoothed_points_voxel, smoothed_radii_voxel_units

def transform_voxel_to_world(points_voxel: np.ndarray, radii_voxel_units: Union[np.ndarray, float], reoriented_affine: np.ndarray) -> Tuple[np.ndarray, Union[np.ndarray, float]]:
    points_voxel_arr = np.asarray(points_voxel)
    if points_voxel_arr.ndim == 1:
        points_voxel_arr = points_voxel_arr.reshape(1, -1)

    homogeneous_coords = np.hstack((points_voxel_arr, np.ones((points_voxel_arr.shape[0], 1))))
    world_coords_homogeneous = homogeneous_coords @ reoriented_affine.T
    points_world = world_coords_homogeneous[:, :3]

    voxel_spacing = np.sqrt(np.sum(reoriented_affine[:3, :3]**2, axis=0))
    valid_spacings = voxel_spacing[voxel_spacing > EPSILON]
    mean_spacing = np.mean(valid_spacings) if len(valid_spacings) > 0 else 1.0
    if mean_spacing == 1.0 and len(valid_spacings) == 0:
        logger.warning("All voxel spacings are zero or near zero. Using mean_spacing=1.0 for radius conversion.")

    radii_world = np.asarray(radii_voxel_units) * mean_spacing
    
    if world_coords_homogeneous.shape[0] == 1:
        return points_world[0], radii_world.item() if radii_world.ndim == 0 else radii_world[0]
    return points_world, radii_world

def save_centerlines_vtk(centerlines_world_data: List[Dict], output_path: str):
    try:
        import vtk
    except ImportError:
        logger.error("VTK Python library not available, skipping VTK export.")
        return

    polyData = vtk.vtkPolyData()
    points_vtk = vtk.vtkPoints()
    lines_vtk = vtk.vtkCellArray()
    radius_vtk_array = vtk.vtkDoubleArray()
    radius_vtk_array.SetName("Radius")
    radius_vtk_array.SetNumberOfComponents(1)

    point_id_counter = 0
    for cl_dict in centerlines_world_data:
        points_world = cl_dict['points_world']
        radii_world = cl_dict['radii_world']
        if len(points_world) == 0: continue # Skip empty centerlines

        line = vtk.vtkPolyLine()
        line.GetPointIds().SetNumberOfIds(len(points_world))
        
        for j, (point_w, radius_w) in enumerate(zip(points_world, radii_world)):
            points_vtk.InsertNextPoint(point_w)
            radius_vtk_array.InsertNextTuple1(radius_w)
            line.GetPointIds().SetId(j, point_id_counter)
            point_id_counter += 1
        
        lines_vtk.InsertNextCell(line)
    
    if points_vtk.GetNumberOfPoints() > 0: # Only set if there are points
        polyData.SetPoints(points_vtk)
        polyData.SetLines(lines_vtk)
        polyData.GetPointData().AddArray(radius_vtk_array)
        polyData.GetPointData().SetActiveScalars("Radius")
        
        writer = vtk.vtkXMLPolyDataWriter()
        writer.SetFileName(output_path)
        writer.SetInputData(polyData)
        writer.Write()
        logger.info(f"Saved VTK file with world coordinates: {output_path}")
    else:
        logger.warning(f"No points to save in VTK file: {output_path}")


def save_centerlines_txt(centerlines_world_data: List[Dict], output_path: str):
    with open(output_path, 'w') as f:
        f.write(f"# Centerline data in WORLD coordinates (Target Orientation: {TARGET_ORIENTATION_AXCODES})\n")
        f.write("# Format: centerline_id point_id x_world y_world z_world radius_world\n")
        
        for cl_id, cl_dict in enumerate(centerlines_world_data):
            points_world = cl_dict['points_world']
            radii_world = cl_dict['radii_world']
            if len(points_world) == 0: continue
            f.write(f"\n# Centerline {cl_id}\n")
            for pt_id, (point_w, radius_w) in enumerate(zip(points_world, radii_world)):
                f.write(f"{cl_id} {pt_id} {point_w[0]:.6f} {point_w[1]:.6f} "
                       f"{point_w[2]:.6f} {radius_w:.6f}\n")
    logger.info(f"Saved TXT file with world coordinates: {output_path}")

# --- Main Centerline Extraction Workflow ---
def extract_and_save_centerlines(
    primary_mask_path: str,
    output_dir: str,
    min_branch_length: int = 10,
    smoothing_factor: float = 0.1,
    min_component_size: int = 1000
) -> Tuple[Union[Dict, None], Union[np.ndarray, None]]:
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"--- Starting Centerline Extraction for: {primary_mask_path} ---")
    logger.info(f"Target orientation for all outputs: {TARGET_ORIENTATION_AXCODES}")

    reoriented_mask_data, reoriented_affine = load_and_reorient_mask(primary_mask_path, TARGET_ORIENTATION_AXCODES)
    if reoriented_mask_data is None or reoriented_affine is None:
        logger.error(f"Failed to load or reorient primary mask: {primary_mask_path}. Aborting centerline extraction.")
        return None, None
        
    base, ext = os.path.splitext(os.path.basename(primary_mask_path))
    if ext == ".gz": base, ext2 = os.path.splitext(base); ext = ext2 + ext
    reoriented_mask_filename = f"{base}_reoriented_{''.join(TARGET_ORIENTATION_AXCODES)}{ext if ext.startswith('.') else '.nii.gz'}"
    reoriented_mask_output_path = os.path.join(output_dir, reoriented_mask_filename)
    save_mask_as_nifti(reoriented_mask_data, reoriented_affine, reoriented_mask_output_path)

    logger.info("Removing disconnected components from reoriented mask...")
    mask_cleaned = remove_small_components(reoriented_mask_data, min_component_size)
    if not np.any(mask_cleaned):
        logger.error("Reoriented mask is empty after removing small components.")
        return None, reoriented_affine

    debug_mask_cleaned_path = os.path.join(output_dir, "debug_mask_cleaned_before_skeleton_preprocessing.nii.gz")
    save_mask_as_nifti(mask_cleaned.astype(np.uint8), reoriented_affine, debug_mask_cleaned_path)
    logger.info(f"Saved debug mask (after remove_small_components) to: {debug_mask_cleaned_path}")

    logger.info("Extracting skeleton from reoriented mask...")
    skeleton_data, distance_map_data = extract_skeleton_3d(mask_cleaned, output_dir, reoriented_affine)
    if not np.any(skeleton_data):
        logger.error("Skeleton is empty. Check reoriented mask or skeletonization parameters.")
        return None, reoriented_affine

    debug_skeleton_path = os.path.join(output_dir, "debug_skeleton_output_after_processing.nii.gz")
    save_mask_as_nifti(skeleton_data.astype(np.uint8), reoriented_affine, debug_skeleton_path)
    logger.info(f"Saved debug skeleton (output of extract_skeleton_3d) to: {debug_skeleton_path}")

    logger.info("Converting skeleton to graph...")
    try:
        G = skeleton_to_graph(skeleton_data, distance_map_data)
    except ValueError as e:
        logger.error(f"Error converting skeleton to graph: {e}")
        return None, reoriented_affine
        
    logger.info(f"Initial graph from reoriented skeleton: {len(G.nodes())} N, {len(G.edges())} E")
    if len(G.nodes()) == 0:
        logger.error("Graph from reoriented skeleton is empty.")
        return None, reoriented_affine

    logger.info("Pruning small branches...")
    G_pruned = prune_skeleton_graph(G, min_branch_length)
    logger.info(f"Pruned graph: {len(G_pruned.nodes())} N, {len(G_pruned.edges())} E")
    if len(G_pruned.nodes()) == 0:
        logger.error("Graph empty after pruning.")
        return None, reoriented_affine

    logger.info("Extracting centerline paths (voxel coords w.r.t. reoriented mask)...")
    try:
        centerlines_voxel_coords, inlet_node_idx, outlet_node_indices = extract_centerline_paths_from_graph(G_pruned)
    except ValueError as e:
        logger.error(f"Error extracting centerline paths: {e}")
        return None, reoriented_affine

    if not centerlines_voxel_coords:
        logger.warning("No centerlines extracted from graph. This might be normal for some inputs or indicate issues with skeleton/graph.")
        # Continue to save empty results if needed, or return None here too
        # return None, reoriented_affine 

    if smoothing_factor > 0 and centerlines_voxel_coords: # Check if list is not empty
        logger.info("Smoothing centerlines (voxel coords)...")
        for cl_dict in centerlines_voxel_coords:
            cl_dict['points_voxel'], cl_dict['radii_voxel_units'] = smooth_centerline(
                cl_dict['points_voxel'], cl_dict['radii_voxel_units'], smoothing_factor)

    centerlines_world_data = []
    if centerlines_voxel_coords: # Check if list is not empty
        for cl_vox_dict in centerlines_voxel_coords:
            points_w, radii_w = transform_voxel_to_world(
                cl_vox_dict['points_voxel'],
                cl_vox_dict['radii_voxel_units'],
                reoriented_affine
            )
            centerlines_world_data.append({'points_world': points_w, 'radii_world': radii_w})

    terminals_data_voxel = {}
    terminals_data_world = {}
    if inlet_node_idx is not None and inlet_node_idx in G_pruned:
        terminals_data_voxel['inlet_pos_voxel'] = G_pruned.nodes[inlet_node_idx]['pos_voxel']
        terminals_data_voxel['inlet_radius_voxel_units'] = G_pruned.nodes[inlet_node_idx]['radius_voxel_units']
        iw_pos, iw_rad = transform_voxel_to_world(terminals_data_voxel['inlet_pos_voxel'], terminals_data_voxel['inlet_radius_voxel_units'], reoriented_affine)
        terminals_data_world['inlet_pos_world'] = iw_pos
        terminals_data_world['inlet_radius_world'] = iw_rad
    
    terminals_data_voxel['outlets_pos_voxel'] = [G_pruned.nodes[n]['pos_voxel'] for n in outlet_node_indices if n in G_pruned]
    terminals_data_voxel['outlets_radii_voxel_units'] = [G_pruned.nodes[n]['radius_voxel_units'] for n in outlet_node_indices if n in G_pruned]
    
    terminals_data_world['outlets_pos_world'] = []
    terminals_data_world['outlets_radii_world'] = []
    for ov_pos, ov_rad in zip(terminals_data_voxel.get('outlets_pos_voxel', []), terminals_data_voxel.get('outlets_radii_voxel_units', [])):
        ow_pos, ow_rad = transform_voxel_to_world(ov_pos, ov_rad, reoriented_affine)
        terminals_data_world['outlets_pos_world'].append(ow_pos)
        terminals_data_world['outlets_radii_world'].append(ow_rad)

    logger.info("Saving centerline results (VTP, TXT will have world coordinates)...")
    save_centerlines_vtk(centerlines_world_data, os.path.join(output_dir, "centerlines_world.vtp"))
    save_centerlines_txt(centerlines_world_data, os.path.join(output_dir, "centerlines_world.txt"))

    centerline_save_data_package = {
        'centerlines_voxel_space': centerlines_voxel_coords if centerlines_voxel_coords else [],
        'centerlines_world_space': centerlines_world_data,
        'terminals_voxel_space': terminals_data_voxel,
        'terminals_world_space': terminals_data_world,
        'reoriented_affine_matrix': reoriented_affine,
        'target_orientation': TARGET_ORIENTATION_AXCODES
    }
    np.save(os.path.join(output_dir, "centerline_data_package.npy"),
            centerline_save_data_package, allow_pickle=True)

    with open(os.path.join(output_dir, "terminals_world.txt"), 'w') as f:
        f.write(f"# Terminal data in WORLD coordinates (Target Orientation: {TARGET_ORIENTATION_AXCODES})\n")
        f.write("# Format: Type X_world Y_world Z_world Radius_world\n")
        if 'inlet_pos_world' in terminals_data_world:
            ipw, irw = terminals_data_world['inlet_pos_world'], terminals_data_world['inlet_radius_world']
            f.write(f"Inlet: {ipw[0]:.6f}, {ipw[1]:.6f}, {ipw[2]:.6f}, radius: {irw:.6f}\n")
        for i, (opw, orw) in enumerate(zip(terminals_data_world.get('outlets_pos_world',[]), terminals_data_world.get('outlets_radii_world',[]))):
            f.write(f"Outlet_{i+1}: {opw[0]:.6f}, {opw[1]:.6f}, {opw[2]:.6f}, radius: {orw:.6f}\n")

    logger.info(f"Centerline extraction finished. Results in: {output_dir}")
    return centerline_save_data_package, reoriented_affine

def extract_centerline_network_from_graph(
    G_skeleton: nx.Graph,
    reoriented_affine: np.ndarray,
    smoothing_factor: float = 0.0 # Smoothing usually applied to segments later if needed
) -> Tuple[List[Dict[str, Union[np.ndarray, int, float]]], Dict[int, Dict]]:
    """
    Decomposes the skeleton graph (G_skeleton) into unique centerline segments
    connecting junction points and endpoints.

    Args:
        G_skeleton: The pruned graph of skeleton voxels. Nodes have 'pos_voxel' and 'radius_voxel_units'.
        reoriented_affine: Affine matrix to convert voxel coordinates to world coordinates.
        smoothing_factor: Smoothing factor for individual segments (optional).

    Returns:
        A tuple containing:
        - segments_data_world (List[Dict]): A list of dictionaries, where each dictionary
          represents a unique vessel segment with keys:
            'id': A unique ID for the segment.
            'points_world': Nx3 numpy array of world coordinates for the segment.
            'radii_world': N numpy array of world radii for the segment.
            'start_node_id': Original G_skeleton node ID of the segment's start.
            'end_node_id': Original G_skeleton node ID of the segment's end.
            'avg_radius_world': Average radius of the segment in world units.
            'length_world': Length of the segment in world units.
        - junction_info (Dict[int, Dict]): Info about junction points in G_skeleton.
            Key: G_skeleton node_id of the junction.
            Value: {'pos_world': (x,y,z), 'connected_segment_ids': [segment_id_1, ...]}
    """
    if len(G_skeleton) == 0:
        logger.warning("Skeleton graph is empty. Cannot extract network.")
        return [], {}

    logger.info("Extracting centerline network with unique segments...")

    # Identify junction nodes (degree > 2) and endpoint nodes (degree == 1)
    junction_nodes = {n for n, d in G_skeleton.degree() if d > 2}
    endpoint_nodes = {n for n, d in G_skeleton.degree() if d == 1}
    # Degree 0 nodes (isolated) should have been removed by pruning
    # Degree 2 nodes are internal to segments

    logger.info(f"Found {len(junction_nodes)} junction nodes and {len(endpoint_nodes)} endpoint nodes in skeleton graph.")

    segments_data = [] # Will store dicts for each segment
    segment_id_counter = 0
    visited_skeleton_edges = set() # To keep track of (u,v) edges in G_skeleton already part of a segment

    # Store junction information
    junction_info_world = {}
    for j_node in junction_nodes:
        j_pos_vox = G_skeleton.nodes[j_node]['pos_voxel']
        j_rad_vox = G_skeleton.nodes[j_node]['radius_voxel_units']
        j_pos_w, _ = transform_voxel_to_world(j_pos_vox, j_rad_vox, reoriented_affine)
        junction_info_world[j_node] = {'pos_world': j_pos_w, 'connected_segment_ids': []}


    # Start tracing segments from endpoints and also from junctions (for internal segments)
    nodes_to_start_tracing_from = list(endpoint_nodes) + list(junction_nodes)

    for start_node_skel_id in nodes_to_start_tracing_from:
        for neighbor_skel_id in G_skeleton.neighbors(start_node_skel_id):
            # Check if this G_skeleton edge has already been traversed for a segment
            edge_skel = tuple(sorted((start_node_skel_id, neighbor_skel_id)))
            if edge_skel in visited_skeleton_edges:
                continue

            # Trace a segment
            current_path_skel_ids = [start_node_skel_id, neighbor_skel_id]
            visited_skeleton_edges.add(edge_skel)
            
            curr_skel_id = neighbor_skel_id
            prev_skel_id = start_node_skel_id

            while G_skeleton.degree(curr_skel_id) == 2:
                # Find the next node in the path (not the one we just came from)
                next_node_found = False
                for next_neighbor_skel_id in G_skeleton.neighbors(curr_skel_id):
                    if next_neighbor_skel_id != prev_skel_id:
                        edge_skel = tuple(sorted((curr_skel_id, next_neighbor_skel_id)))
                        if edge_skel in visited_skeleton_edges: # Should not happen if logic is correct
                            logger.warning(f"Re-visiting edge {edge_skel} during segment trace. Breaking.")
                            break # Safety break
                        visited_skeleton_edges.add(edge_skel)
                        
                        current_path_skel_ids.append(next_neighbor_skel_id)
                        prev_skel_id = curr_skel_id
                        curr_skel_id = next_neighbor_skel_id
                        next_node_found = True
                        break
                if not next_node_found: # Path ends unexpectedly or error
                    logger.warning(f"Segment trace broke at degree-2 node {curr_skel_id}. Path so far: {current_path_skel_ids}")
                    break 
            
            # Segment traced. current_path_skel_ids contains the node IDs from G_skeleton
            if len(current_path_skel_ids) < 2:
                logger.debug(f"Skipping very short traced path: {current_path_skel_ids}")
                continue

            segment_id_counter += 1
            segment_start_skel_id = current_path_skel_ids[0]
            segment_end_skel_id = current_path_skel_ids[-1]

            # Collect points and radii for this segment
            points_voxel = np.array([G_skeleton.nodes[n]['pos_voxel'] for n in current_path_skel_ids])
            radii_voxel = np.array([G_skeleton.nodes[n]['radius_voxel_units'] for n in current_path_skel_ids])

            # Smooth if desired (smoothing is usually better on a per-segment basis)
            if smoothing_factor > 0:
                points_voxel, radii_voxel = smooth_centerline(points_voxel, radii_voxel, smoothing_factor)

            # Transform to world coordinates
            points_world, radii_world = transform_voxel_to_world(points_voxel, radii_voxel, reoriented_affine)
            
            # Calculate segment length
            segment_length_world = 0.0
            if len(points_world) >= 2:
                segment_length_world = np.sum(np.sqrt(np.sum(np.diff(points_world, axis=0)**2, axis=1)))
            
            avg_radius_world = np.mean(radii_world) if len(radii_world) > 0 else 0.0

            segments_data.append({
                'id': segment_id_counter,
                'points_world': points_world,
                'radii_world': radii_world,
                'start_node_id_skel': segment_start_skel_id, # Original skeleton graph node ID
                'end_node_id_skel': segment_end_skel_id,     # Original skeleton graph node ID
                'avg_radius_world': avg_radius_world,
                'length_world': segment_length_world,
            })
            
            # Update junction info
            if segment_start_skel_id in junction_info_world:
                junction_info_world[segment_start_skel_id]['connected_segment_ids'].append(segment_id_counter)
            if segment_end_skel_id in junction_info_world:
                junction_info_world[segment_end_skel_id]['connected_segment_ids'].append(segment_id_counter)

    logger.info(f"Extracted {len(segments_data)} unique centerline segments.")
    return segments_data, junction_info_world


# --- NEW FUNCTION: Save Centerline Network as VTP ---


def save_centerline_network_vtp(
    segments_data: List[Dict],    # Each dict has 'id', 'points_world', 'radii_world', 'avg_radius_world', 'length_world', 'start_node_id_skel', 'end_node_id_skel'
    junction_info: Dict[int, Dict], # skel_node_id -> {'pos_world': list, 'connected_segment_ids': list}
    output_path: str
):
    """
    Save centerline network as VTP with point and cell data.
    Ensures "Radius" point data is correctly associated.
    """
    if not segments_data and not junction_info:
        logger.warning("No segments or junction data to save to VTP. Output will be empty.")
        # Create and save an empty polydata to avoid downstream file not found errors
        emptyPolyData = vtk.vtkPolyData()
        writer = vtk.vtkXMLPolyDataWriter()
        writer.SetFileName(output_path)
        writer.SetInputData(emptyPolyData)
        writer.Write()
        return

    polyData = vtk.vtkPolyData()
    vtp_points = vtk.vtkPoints() # Changed name for clarity
    vtp_lines = vtk.vtkCellArray()

    # --- Point Data Arrays ---
    # These will be populated as points are added to vtp_points
    point_radius_data = vtk.vtkDoubleArray()
    point_radius_data.SetName("radius") # Crucial Name!
    point_radius_data.SetNumberOfComponents(1)

    point_type_data = vtk.vtkIntArray() # 0 for segment point, 1 for junction
    point_type_data.SetName("PointType")
    point_type_data.SetNumberOfComponents(1)
    
    # Optional: If you want to store which original segment a point belongs to (for non-junction points)
    # point_orig_segment_id_data = vtk.vtkIntArray()
    # point_orig_segment_id_data.SetName("OriginalSegmentID")
    # point_orig_segment_id_data.SetNumberOfComponents(1)


    # --- Cell Data Arrays ---
    cell_segment_id_data = vtk.vtkIntArray()
    cell_segment_id_data.SetName("SegmentID") # ID of the segment this cell represents
    cell_segment_id_data.SetNumberOfComponents(1)
    
    cell_avg_radius_data = vtk.vtkDoubleArray()
    cell_avg_radius_data.SetName("AverageRadiusWorld")
    cell_avg_radius_data.SetNumberOfComponents(1)

    cell_length_data = vtk.vtkDoubleArray()
    cell_length_data.SetName("LengthWorld")
    cell_length_data.SetNumberOfComponents(1)

    # --- Logic ---
    # Map skeleton node ID (from junctions) to the VTP point ID created for it
    skel_junction_to_vtp_point_id: Dict[int, int] = {}
    current_vtp_point_id_counter = 0

    # 1. Add all unique junction points first and store their VTP IDs
    for skel_node_id, j_info in junction_info.items():
        pos_w = np.asarray(j_info['pos_world'], dtype=np.float64)
        if not (pos_w.shape == (3,) and np.all(np.isfinite(pos_w))):
            logger.warning(f"Junction skel_node {skel_node_id} has invalid position {pos_w}, skipping.")
            continue
        
        vtp_points.InsertNextPoint(pos_w[0], pos_w[1], pos_w[2])
        # Junctions themselves don't have a defined radius from distance transform.
        # Assign a nominal small radius or average of connected segments later if needed.
        # For now, let's assign a placeholder or an average of what it connects to.
        # For simplicity, using a small default or 0.
        # A better approach might be to calculate average radius of connected segments after all segments are processed.
        # For now, let's use a small default for junctions.
        junction_radius = j_info.get('radius_world_avg_connected', 0.01) # Add this to junction_info if possible
        point_radius_data.InsertNextValue(junction_radius)
        point_type_data.InsertNextValue(1) # 1 for junction
        # point_orig_segment_id_data.InsertNextValue(-1) # -1 to indicate not part of a single segment path

        skel_junction_to_vtp_point_id[skel_node_id] = current_vtp_point_id_counter
        current_vtp_point_id_counter += 1
    
    logger.info(f"Added {len(skel_junction_to_vtp_point_id)} junction points to VTP.")

    # 2. Add points from segments and create lines (polylines)
    for seg_dict in segments_data:
        seg_id = seg_dict['id']
        points_world_seg = np.asarray(seg_dict['points_world'], dtype=np.float64)
        radii_world_seg = np.asarray(seg_dict['radii_world'], dtype=np.float64)

        if points_world_seg.ndim != 2 or points_world_seg.shape[1] != 3 or len(points_world_seg) < 2:
            logger.warning(f"Segment {seg_id} has invalid points shape {points_world_seg.shape}, skipping.")
            continue
        if radii_world_seg.ndim != 1 or len(radii_world_seg) != len(points_world_seg):
            logger.warning(f"Segment {seg_id} radii array shape mismatch with points shape, skipping or using default.")
            # Fallback: create a default radii array for this segment
            radii_world_seg = np.full(len(points_world_seg), np.mean(radii_world_seg) if np.any(np.isfinite(radii_world_seg)) else 0.01)


        polyLine = vtk.vtkPolyLine()
        num_points_in_segment = len(points_world_seg)
        polyLine.GetPointIds().SetNumberOfIds(num_points_in_segment)

        for i_pt_in_seg in range(num_points_in_segment):
            is_endpoint = (i_pt_in_seg == 0) or (i_pt_in_seg == num_points_in_segment - 1)
            vtp_id_for_this_point = -1

            if i_pt_in_seg == 0 and seg_dict['start_node_id_skel'] in skel_junction_to_vtp_point_id:
                vtp_id_for_this_point = skel_junction_to_vtp_point_id[seg_dict['start_node_id_skel']]
            elif i_pt_in_seg == num_points_in_segment - 1 and seg_dict['end_node_id_skel'] in skel_junction_to_vtp_point_id:
                vtp_id_for_this_point = skel_junction_to_vtp_point_id[seg_dict['end_node_id_skel']]
            
            if vtp_id_for_this_point != -1: # Point is an existing junction
                polyLine.GetPointIds().SetId(i_pt_in_seg, vtp_id_for_this_point)
                # Radius for junction points is already set. We could average/update it here if desired.
            else: # Point is an intermediate segment point, add it now
                pos_w = points_world_seg[i_pt_in_seg]
                rad_w = radii_world_seg[i_pt_in_seg]

                vtp_points.InsertNextPoint(pos_w[0], pos_w[1], pos_w[2])
                point_radius_data.InsertNextValue(rad_w if np.isfinite(rad_w) else 0.01)
                point_type_data.InsertNextValue(0) # 0 for segment point
                # point_orig_segment_id_data.InsertNextValue(seg_id)
                
                polyLine.GetPointIds().SetId(i_pt_in_seg, current_vtp_point_id_counter)
                current_vtp_point_id_counter += 1
        
        vtp_lines.InsertNextCell(polyLine)
        
        # Add cell data for this segment
        cell_segment_id_data.InsertNextValue(seg_id)
        cell_avg_radius_data.InsertNextValue(float(seg_dict.get('avg_radius_world', np.mean(radii_world_seg))))
        cell_length_data.InsertNextValue(float(seg_dict.get('length_world', 0.0))) # Calculate if not present

    polyData.SetPoints(vtp_points)
    if vtp_lines.GetNumberOfCells() > 0:
        polyData.SetLines(vtp_lines)
        polyData.GetCellData().AddArray(cell_segment_id_data)
        polyData.GetCellData().AddArray(cell_avg_radius_data)
        polyData.GetCellData().AddArray(cell_length_data)
        # polyData.GetCellData().SetActiveScalars("SegmentID") # Optional

    # Add point data arrays - crucial step
    # Ensure the number of tuples in each array matches the number of points in vtp_points
    if vtp_points.GetNumberOfPoints() == point_radius_data.GetNumberOfTuples():
        polyData.GetPointData().AddArray(point_radius_data)
    else:
        logger.error(f"Mismatch: VTP points ({vtp_points.GetNumberOfPoints()}) vs Radius data ({point_radius_data.GetNumberOfTuples()}). Radius array not added.")

    if vtp_points.GetNumberOfPoints() == point_type_data.GetNumberOfTuples():
        polyData.GetPointData().AddArray(point_type_data)
    else:
        logger.error(f"Mismatch: VTP points ({vtp_points.GetNumberOfPoints()}) vs PointType data ({point_type_data.GetNumberOfTuples()}). PointType array not added.")

    # if vtp_points.GetNumberOfPoints() == point_orig_segment_id_data.GetNumberOfTuples():
    #     polyData.GetPointData().AddArray(point_orig_segment_id_data)
    # else:
    #     logger.error("Mismatch for OriginalSegmentID point data.")

    polyData.GetPointData().SetActiveScalars("radius") # Set "Radius" as the default active scalar for points

    # Write file
    try:
        writer = vtk.vtkXMLPolyDataWriter()
        writer.SetFileName(output_path)
        writer.SetInputData(polyData)
        writer.SetDataModeToAscii() # Easier to debug VTP content
        writer.SetCompressorTypeToNone() # No compression for easier debugging
        result = writer.Write()
        
        if result == 1:
            logger.info(f"Successfully saved centerline network VTP: {output_path}")
            logger.info(f"  Total VTP Points: {vtp_points.GetNumberOfPoints()}, Total Lines/Cells: {vtp_lines.GetNumberOfCells()}")
            logger.info(f"  Point Data 'Radius' tuples: {point_radius_data.GetNumberOfTuples()}")
            logger.info(f"  Point Data 'PointType' tuples: {point_type_data.GetNumberOfTuples()}")
            logger.info(f"  Cell Data 'SegmentID' tuples: {cell_segment_id_data.GetNumberOfTuples()}")
        else:
            logger.error(f"vtkXMLPolyDataWriter failed to write VTP file (Write() returned 0): {output_path}")
            
    except Exception as e:
        logger.error(f"Exception during vtkXMLPolyDataWriter.Write(): {e}", exc_info=True)

# --- MODIFIED Main Centerline Extraction Workflow ---
def extract_and_save_centerlines(
    primary_mask_path: str,
    output_dir: str,
    min_branch_length: int = 10,
    smoothing_factor: float = 0.1, # Smoothing for network segments
    min_component_size: int = 1000
) -> Tuple[Union[Dict, None], Union[np.ndarray, None]]:
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"--- Starting Centerline Extraction for: {primary_mask_path} ---")
    logger.info(f"Target orientation for all outputs: {TARGET_ORIENTATION_AXCODES}")

    reoriented_mask_data, reoriented_affine = load_and_reorient_mask(primary_mask_path, TARGET_ORIENTATION_AXCODES)
    if reoriented_mask_data is None or reoriented_affine is None:
        logger.error(f"Failed to load or reorient primary mask: {primary_mask_path}. Aborting.")
        return None, None
        
    base, ext = os.path.splitext(os.path.basename(primary_mask_path))
    if ext == ".gz": base, ext2 = os.path.splitext(base); ext = ext2 + ext
    reoriented_mask_filename = f"{base}_reoriented_{''.join(TARGET_ORIENTATION_AXCODES)}{ext if ext.startswith('.') else '.nii.gz'}"
    reoriented_mask_output_path = os.path.join(output_dir, reoriented_mask_filename)
    save_mask_as_nifti(reoriented_mask_data, reoriented_affine, reoriented_mask_output_path)

    logger.info("Removing disconnected components from reoriented mask...")
    mask_cleaned = remove_small_components(reoriented_mask_data, min_component_size)
    if not np.any(mask_cleaned):
        logger.error("Reoriented mask is empty after removing small components.")
        return None, reoriented_affine

    debug_mask_cleaned_path = os.path.join(output_dir, "debug_mask_cleaned_before_skeleton_preprocessing.nii.gz")
    save_mask_as_nifti(mask_cleaned.astype(np.uint8), reoriented_affine, debug_mask_cleaned_path)

    logger.info("Extracting skeleton from reoriented mask...")
    skeleton_data, distance_map_data = extract_skeleton_3d(mask_cleaned, output_dir, reoriented_affine)
    if not np.any(skeleton_data):
        logger.error("Skeleton is empty.")
        return None, reoriented_affine

    debug_skeleton_path = os.path.join(output_dir, "debug_skeleton_output_after_processing.nii.gz")
    save_mask_as_nifti(skeleton_data.astype(np.uint8), reoriented_affine, debug_skeleton_path)

    logger.info("Converting skeleton to graph...")
    try:
        G_skeleton = skeleton_to_graph(skeleton_data, distance_map_data)
    except ValueError as e:
        logger.error(f"Error converting skeleton to graph: {e}")
        return None, reoriented_affine
        
    logger.info(f"Initial skeleton graph: {len(G_skeleton.nodes())} N, {len(G_skeleton.edges())} E")
    if len(G_skeleton.nodes()) == 0: return None, reoriented_affine

    logger.info("Pruning skeleton graph...")
    G_pruned_skeleton = prune_skeleton_graph(G_skeleton, min_branch_length)
    logger.info(f"Pruned skeleton graph: {len(G_pruned_skeleton.nodes())} N, {len(G_pruned_skeleton.edges())} E")
    if len(G_pruned_skeleton.nodes()) == 0:
        logger.error("Skeleton graph empty after pruning.")
        return None, reoriented_affine

    # --- EXTRACT NETWORK INSTEAD OF PATHS ---
    centerline_network_segments, junction_info = extract_centerline_network_from_graph(
        G_pruned_skeleton,
        reoriented_affine,
        smoothing_factor # Pass smoothing factor here
    )

    if not centerline_network_segments:
        logger.warning("No centerline network segments extracted.")
        # Proceed to save empty/minimal data, or return None
    
    # --- SAVE NETWORK ---
    logger.info("Saving centerline network (VTP, TXT will represent unique segments)...")
    save_centerline_network_vtp(centerline_network_segments, junction_info, os.path.join(output_dir, "centerline_NETWORK_world.vtp"))
    
    # Save a simple TXT representation of segments (can be enhanced)
    with open(os.path.join(output_dir, "centerline_NETWORK_segments_world.txt"), 'w') as f:
        f.write(f"# Centerline Network Segments in WORLD coordinates (Target Orientation: {TARGET_ORIENTATION_AXCODES})\n")
        f.write("# SegmentID AvgRadiusWorld LengthWorld NumPoints StartSkelNodeIDSkel EndSkelNodeIDSkel\n")
        for seg in centerline_network_segments:
            f.write(f"{seg['id']} {seg['avg_radius_world']:.6f} {seg['length_world']:.6f} {len(seg['points_world'])} "
                    f"{seg['start_node_id_skel']} {seg['end_node_id_skel']}\n")
        f.write("\n# Junction Info (Skeleton Node ID -> World Pos X, Y, Z, ConnectedSegmentIDs)\n")
        for j_id, j_data in junction_info.items():
            pos_w_str = ", ".join(map(lambda x: f"{x:.6f}", j_data['pos_world']))
            f.write(f"JunctionSkelID_{j_id}: PosW({pos_w_str}), SegIDs({j_data['connected_segment_ids']})\n")

    # Save detailed data package
    network_data_package = {
        'network_segments_world': centerline_network_segments, # List of segment dicts
        'junction_info_world_via_skel_id': junction_info,     # Dict mapping skel_id to junction data
        'reoriented_affine_matrix': reoriented_affine,
        'target_orientation': TARGET_ORIENTATION_AXCODES,
        'skeleton_graph_pruned_node_count': len(G_pruned_skeleton.nodes()) # For reference
    }
    np.save(os.path.join(output_dir, "centerline_NETWORK_data_package.npy"),
            network_data_package, allow_pickle=True)

    logger.info(f"Centerline network extraction finished. Results in: {output_dir}")
    return network_data_package, reoriented_affine

# --- Main Execution ---
if __name__ == "__main__":
    # --- Define Inputs ---
    primary_mask_for_centerlines = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/mida_processed_masks/mida_arteries_mask.nii.gz" # Example
    
    # List of other masks you want to reorient to the same TARGET_ORIENTATION
    other_mask_paths = [
         "/Users/c3495249/Coding/Gemini_Pro_Vasculature/mida_processed_masks/mida_total_csf_mask.nii.gz",
         "/Users/c3495249/Coding/Gemini_Pro_Vasculature/mida_processed_masks/mida_total_gm_mask.nii.gz",
         "/Users/c3495249/Coding/Gemini_Pro_Vasculature/mida_processed_masks/mida_total_wm_mask.nii.gz",
        # Add more paths here if needed
    ]
    
    # --- Define Output Location ---
    # CHANGED: Added _NETWORK_ to distinguish from previous path-based outputs
    base_output_directory = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/data/mida_reoriented_NETWORK_v1" 

    # --- Setup Output Directories ---
    primary_mask_name_no_ext = os.path.basename(primary_mask_for_centerlines)
    # Robustly remove .nii or .nii.gz or .gz extensions
    while any(primary_mask_name_no_ext.endswith(ext) for ext in ['.nii', '.gz']): 
        primary_mask_name_no_ext = os.path.splitext(primary_mask_name_no_ext)[0]
    
    centerline_output_dir = os.path.join(base_output_directory, f"{primary_mask_name_no_ext}_centerlines_TARGET_{''.join(TARGET_ORIENTATION_AXCODES)}")
    other_masks_output_dir = os.path.join(base_output_directory, f"other_reoriented_masks_TARGET_{''.join(TARGET_ORIENTATION_AXCODES)}")

    os.makedirs(centerline_output_dir, exist_ok=True)
    os.makedirs(other_masks_output_dir, exist_ok=True)

    # --- Start Logging ---
    logger.info(f"--- STARTING FULL PIPELINE (NETWORK EXTRACTION) ---")
    logger.info(f"Target orientation for all outputs: {TARGET_ORIENTATION_AXCODES}")
    logger.info(f"Primary mask for centerlines: {primary_mask_for_centerlines}")
    logger.info(f"Centerline network outputs will be in: {centerline_output_dir}")
    if other_mask_paths:
        logger.info(f"Other masks to reorient: {len(other_mask_paths)}")
        logger.info(f"Reoriented 'other' masks will be saved in: {other_masks_output_dir}")
    else:
        logger.info("No 'other_mask_paths' provided for reorientation.")

    # --- 1. Extract Centerline Network from the Primary Mask ---
    final_reoriented_affine_for_primary = None # Initialize
    try:
        # Note: extract_and_save_centerlines now performs network extraction internally
        network_results_package, final_reoriented_affine_for_primary = extract_and_save_centerlines(
            primary_mask_path=primary_mask_for_centerlines,
            output_dir=centerline_output_dir,
            min_branch_length=3,    # Pruning for skeleton graph before network extraction.
                                    # Can be less aggressive if network extraction is robust.
            smoothing_factor=0.2,   # Smoothing now applied per-segment in network extraction if desired there.
                                    # Set to 0.0 if no smoothing is wanted for network segments.
            min_component_size=500  # For cleaning the initial mask
        )
        if network_results_package:
            logger.info(f"Centerline network extraction for '{primary_mask_for_centerlines}' process completed.")
            # Check if actual segments were found
            if not network_results_package.get('network_segments_world') and \
               not network_results_package.get('centerlines_voxel_space'): # Check old key too for compatibility
                logger.warning(f"Network package for '{primary_mask_for_centerlines}' was created but contains no centerline segments.")
            else:
                num_segments = len(network_results_package.get('network_segments_world', []))
                logger.info(f"Successfully extracted {num_segments} network segments.")

        else:
            logger.error(f"Centerline network extraction for '{primary_mask_for_centerlines}' failed or produced no output package.")

    except Exception as e: # Catch any unexpected error from the main workflow
        logger.error(f"Unhandled error during centerline network extraction pipeline for '{primary_mask_for_centerlines}': {e}", exc_info=True)

    # --- 2. Reorient and Save Other Specified Masks ---
    if other_mask_paths:
        logger.info(f"\n--- Processing Other Masks for Reorientation ---")
        for other_mask_path in other_mask_paths:
            if not os.path.exists(other_mask_path):
                logger.warning(f"Other mask file not found, skipping: {other_mask_path}")
                continue
            try:
                logger.info(f"Processing other mask: {other_mask_path}")
                # Use the globally defined TARGET_ORIENTATION_AXCODES
                reoriented_data, reoriented_affine = load_and_reorient_mask(other_mask_path, TARGET_ORIENTATION_AXCODES)
                
                if reoriented_data is not None and reoriented_affine is not None:
                    # Create a descriptive output name for the reoriented other mask
                    other_mask_basename = os.path.basename(other_mask_path)
                    other_mask_name_no_ext_temp = other_mask_basename
                    while any(other_mask_name_no_ext_temp.endswith(ext) for ext in ['.nii', '.gz']):
                        other_mask_name_no_ext_temp = os.path.splitext(other_mask_name_no_ext_temp)[0]
                    
                    # Determine original extension for saving correctly (.nii or .nii.gz)
                    save_ext = ".nii.gz" if other_mask_basename.endswith(".nii.gz") else \
                               ".nii" if other_mask_basename.endswith(".nii") else \
                               ".nii.gz" # Fallback
                        
                    output_filename = f"{other_mask_name_no_ext_temp}_reoriented_{''.join(TARGET_ORIENTATION_AXCODES)}{save_ext}"
                    full_output_path = os.path.join(other_masks_output_dir, output_filename)
                    
                    save_mask_as_nifti(reoriented_data, reoriented_affine, full_output_path)
                    # logger.info(f"Successfully reoriented and saved: {other_mask_path} -> {full_output_path}") # save_mask_as_nifti logs this
                else:
                    logger.error(f"Skipping saving of other mask {other_mask_path} due to load/reorientation failure.")

            except Exception as e:
                logger.error(f"Failed to process other mask {other_mask_path}: {e}", exc_info=True)
    
    logger.info(f"--- FULL PIPELINE FINISHED ---")import numpy as np
import nibabel as nib
from scipy.ndimage import gaussian_filter

# --- Load MIDA segmentation (for shape/affine)
mida_img = nib.load('/Users/c3495249/Coding/Gemini_Pro_Vasculature/data/MIDA_v1.nii')
shape = mida_img.shape
affine = mida_img.affine

# --- Parameters
# tumour_centre = np.array([240, 350, 110])  # voxel coordinates (choose based on anatomy)
# --- Tumour parameters
centre = np.array([240, 350, 110])  # centre in voxel coordinates
radii = np.array([15, 20, 25])     # radii along x, y, z axes
noise_scale = 2               # 0 = no irregularity, 0.2–0.4 = realistic
blur_sigma = 3                    # smoothness of shape deformation

# --- Create coordinate grid
X, Y, Z = np.meshgrid(
    np.arange(shape[0]),
    np.arange(shape[1]),
    np.arange(shape[2]),
    indexing='ij'
)

coords = np.stack((X, Y, Z), axis=-1)
relative = coords - centre

# --- Ellipsoid distance
ellipsoid_dist = (
    (relative[..., 0] / radii[0]) ** 2 +
    (relative[..., 1] / radii[1]) ** 2 +
    (relative[..., 2] / radii[2]) ** 2
)

# --- Add smoothed noise
np.random.seed(42)
raw_noise = np.random.rand(*shape)
smooth_noise = gaussian_filter(raw_noise, sigma=blur_sigma)
smooth_noise = (smooth_noise - np.min(smooth_noise)) / (np.max(smooth_noise) - np.min(smooth_noise))

# --- Irregular shape via thresholded noise-modulated ellipsoid
threshold = 1.0  # inside ellipsoid if ellipsoid_dist < 1.0
deformed_threshold = threshold * (1 + noise_scale * (smooth_noise - 0.5))
tumour_mask = ellipsoid_dist < deformed_threshold

# --- Save as NIfTI
tumour_img = nib.Nifti1Image(tumour_mask.astype(np.uint8), affine)
nib.save(tumour_img, 'tumour_seg_irregular_ellipsoid.nii.gz')import nibabel as nib
import numpy as np
from skimage import measure
import random
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# --- Parameters ---
input_nii = '/Users/c3495249/Coding/Gemini_Pro_Vasculature/data/mida_reoriented_NETWORK_v1/other_reoriented_masks_TARGET_RAS/brain_domain_mask_solid_parenchyma.nii.gz'
n_points = 30
inward_shift_range_mm = (0.5, 2.0)  # shift range in mm
radius_range_mm = (0.2, 2.0)  # initial radius range in mm

# --- Load the NIFTI image ---
img = nib.load(input_nii)
data = img.get_fdata()
affine = img.affine
inv_affine = np.linalg.inv(affine)

# --- Extract surface with marching cubes ---
verts, faces, normals, values = measure.marching_cubes(data, level=0.5)

# --- Convert vertices and normals to world coordinates ---
verts_world = nib.affines.apply_affine(affine, verts)

# Transform normals to world space direction vectors
normals_hom = np.concatenate([normals, np.zeros((normals.shape[0], 1))], axis=1)  # add 0 for homogeneous transform
normals_world = (affine @ normals_hom.T).T[:, :3]

# Normalize normals in world space
normals_world_unit = normals_world / np.linalg.norm(normals_world, axis=1)[:, np.newaxis]

# --- Randomly select N surface points with safety check ---
seed_points = []
tries = 0
max_tries = 1000  # to avoid infinite loops in case of very thin masks

while len(seed_points) < n_points and tries < max_tries:
    idx = random.choice(range(len(verts_world)))
    shift_mm = random.uniform(*inward_shift_range_mm)
    candidate_point = verts_world[idx] - shift_mm * normals_world_unit[idx]

    # Convert candidate point to voxel space
    candidate_voxel = nib.affines.apply_affine(inv_affine, candidate_point)
    candidate_voxel = np.round(candidate_voxel).astype(int)

    # Check if voxel is still inside the mask
    if (0 <= candidate_voxel[0] < data.shape[0] and
        0 <= candidate_voxel[1] < data.shape[1] and
        0 <= candidate_voxel[2] < data.shape[2]):
        if data[tuple(candidate_voxel)] > 0:
            initial_radius = random.uniform(*radius_range_mm)
            seed_points.append((candidate_point, initial_radius))
    
    tries += 1

if tries >= max_tries:
    print("Warning: Maximum tries reached. Only {} valid seed points were found.".format(len(seed_points)))

# --- Output seed points in requested format ---
print("seed_points: # Optional: Define seed points if no initial_arterial_centerlines are provided")
for i, (point, radius) in enumerate(seed_points):
    print(f"  - id: \"seed_{i+1}\"\n    position: [{point[0]:.3f}, {point[1]:.3f}, {point[2]:.3f}]\n    initial_radius: {radius:.3f}  # mm")

# --- 3D Visualization ---
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Plot surface mesh (downsampled for speed)
sample_surface = verts_world[::100]  # plot every 100th vertex for clarity
ax.scatter(sample_surface[:, 0], sample_surface[:, 1], sample_surface[:, 2], color='lightgrey', alpha=0.3, s=1, label='Surface')

# Plot seed points
if len(seed_points) > 0:
    seed_positions = np.array([p for p, r in seed_points])
    ax.scatter(seed_positions[:, 0], seed_positions[:, 1], seed_positions[:, 2], color='red', s=50, label='Seed Points')

ax.set_title("Surface and Inward-shifted Seed Points")
ax.legend()
plt.show()
# investigate_vtp.py
import pyvista as pv
import numpy as np
import os

# --- Configuration ---
VTP_FILE_PATH = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/data/arteries_mida/centerlines.vtp" # Your VTP file
OUTPUT_DIR = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/output/vtp_investigation" # For any saved outputs
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Load the Mesh ---
if not os.path.exists(VTP_FILE_PATH):
    print(f"ERROR: VTP file not found at {VTP_FILE_PATH}")
    exit()

try:
    mesh = pv.read(VTP_FILE_PATH)
    print(f"Successfully loaded VTP: {VTP_FILE_PATH}")
except Exception as e:
    print(f"ERROR loading VTP: {e}")
    exit()

# --- 1. Basic Mesh Information ---
print("\n--- Basic Mesh Information ---")
print(f"Number of Points: {mesh.n_points}")
print(f"Number of Cells: {mesh.n_cells}")
print(f"Bounds (X_min, X_max, Y_min, Y_max, Z_min, Z_max): {mesh.bounds}")

# --- 2. Point Data Analysis ---
print("\n--- Point Data Arrays ---")
if not mesh.point_data:
    print("No point data arrays found.")
else:
    print(f"Available point data array names: {list(mesh.point_data.keys())}")
    for name, array in mesh.point_data.items():
        print(f"  Array Name: '{name}'")
        print(f"    Number of tuples: {array.shape[0] if isinstance(array, np.ndarray) else 'N/A (not NumPy)'}")
        print(f"    Data type: {array.dtype if isinstance(array, np.ndarray) else type(array)}")
        if isinstance(array, np.ndarray) and array.ndim == 1: # Scalar data
            print(f"    Min value: {np.min(array) if array.size > 0 else 'N/A'}")
            print(f"    Max value: {np.max(array) if array.size > 0 else 'N/A'}")
            print(f"    Mean value: {np.mean(array) if array.size > 0 else 'N/A'}")
            if np.issubdtype(array.dtype, np.floating):
                print(f"    Contains NaNs: {np.isnan(array).any()}")
                print(f"    Contains Infs: {np.isinf(array).any()}")
        elif isinstance(array, np.ndarray):
             print(f"    Shape: {array.shape}")
        print(f"    First 5 values: {array[:5]}")


# --- 3. Cell Data Analysis ---
print("\n--- Cell Data Arrays ---")
if not mesh.cell_data:
    print("No cell data arrays found.")
else:
    print(f"Available cell data array names: {list(mesh.cell_data.keys())}")
    for name, array in mesh.cell_data.items():
        print(f"  Array Name: '{name}'")
        print(f"    Number of tuples: {array.shape[0] if isinstance(array, np.ndarray) else 'N/A'}")
        print(f"    Data type: {array.dtype if isinstance(array, np.ndarray) else type(array)}")
        print(f"    First 5 values: {array[:5]}")

# --- 4. Cell Structure Analysis (Polylines vs. Lines) ---
print("\n--- Cell Structure Analysis ---")
if mesh.n_cells > 0:
    actual_cell_types_codes = np.array([mesh.get_cell(i).type for i in range(mesh.n_cells)])
    unique_type_codes, counts = np.unique(actual_cell_types_codes, return_counts=True)
    
    print(f"Cell types found (VTK codes and names):")
    cell_type_names_map = {
        pv.CellType.LINE: "LINE",
        pv.CellType.POLY_LINE: "POLY_LINE",
        pv.CellType.TRIANGLE: "TRIANGLE",
        pv.CellType.QUAD: "QUAD",
        pv.CellType.POLYGON: "POLYGON",
        # Add other pv.CellType members as needed
    }
    for utype_code, count in zip(unique_type_codes, counts):
        type_name = cell_type_names_map.get(pv.CellType(utype_code), f"UnknownVTKType({utype_code})") # Convert int to Enum
        print(f"  VTK Type Code: {utype_code} ({type_name}), Count: {count}")

    print("Connectivity of first 5 cells (point indices):")
    for i in range(min(5, mesh.n_cells)):
        cell = mesh.get_cell(i)
        cell_type_code = cell.type
        type_name = cell_type_names_map.get(pv.CellType(cell_type_code), f"UnknownVTKType({cell_type_code})")
        print(f"  Cell {i} (type code {cell_type_code} - {type_name}): Points {cell.point_ids} (Number of points: {len(cell.point_ids)})")

    num_simple_lines = np.sum(actual_cell_types_codes == pv.CellType.LINE.value) # Compare with enum's value
    num_polylines = np.sum(actual_cell_types_codes == pv.CellType.POLY_LINE.value)
    print(f"Number of simple LINE cells (2 points by definition): {num_simple_lines}")
    print(f"Number of POLY_LINE cells (often >2 points): {num_polylines}")

    if num_polylines > 0 and num_polylines == mesh.n_cells:
        print("This VTP consists entirely of POLY_LINE cells.")
    elif num_simple_lines == mesh.n_cells:
        print("This VTP seems to contain only simple 2-point LINE segments.")
    elif num_polylines > 0 :
        print("This VTP contains POLY_LINE cells. Your main.py parser needs to handle these.")
    else:
        print("Cell types are primarily simple LINEs or other types.")

# --- 5. Visual Inspection (Optional) ---
# ... (plotting code remains the same) ...
# print("\n--- Visual Inspection (Plotting) ---")
# plotter = pv.Plotter()
# if 'Radius' in mesh.point_data:
#     plotter.add_mesh(mesh, scalars='Radius', cmap='viridis', line_width=3, render_lines_as_tubes=True,
#                      scalar_bar_args={'title': 'Radius'})
#     print("Plotting with 'Radius' (uppercase R) scalars.")
# elif 'radius' in mesh.point_data:
#     plotter.add_mesh(mesh, scalars='radius', cmap='viridis', line_width=3, render_lines_as_tubes=True,
#                      scalar_bar_args={'title': 'radius'})
#     print("Plotting with 'radius' (lowercase r) scalars.")
# elif mesh.point_data.active_scalars_name and mesh.point_data.active_scalars is not None:
#     active_scalars_name = mesh.point_data.active_scalars_name
#     plotter.add_mesh(mesh, scalars=active_scalars_name, cmap='viridis', line_width=3, render_lines_as_tubes=True,
#                      scalar_bar_args={'title': active_scalars_name})
#     print(f"Plotting with active scalar: '{active_scalars_name}'.")
# else:
#     plotter.add_mesh(mesh, color='lightblue', line_width=3, render_lines_as_tubes=True)
#     print("Plotting with default color (no recognized radius scalar for coloring).")
# plotter.show_grid()
# plotter.show_axes()
# plotter.camera_position = 'xy'
# plotter.show()

print(f"\nInvestigation complete. Check output above.")import numpy as np
import nibabel as nib
import nibabel.orientations as nio
from skimage import measure, morphology
from scipy import ndimage
import networkx as nx
from scipy.spatial import KDTree
import os
from typing import Union, Tuple, List, Dict
from tqdm import tqdm
import logging

# --- Configuration ---
TARGET_ORIENTATION_AXCODES = ('R', 'A', 'S') # Target canonical orientation (Right, Anterior, Superior)
EPSILON = 1e-9

# --- Logger Setup ---
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Utility Functions ---

def load_and_reorient_mask(mask_path: str, target_axcodes: Tuple[str, str, str] = TARGET_ORIENTATION_AXCODES) -> Union[Tuple[np.ndarray, np.ndarray], Tuple[None, None]]:
    """
    Load a mask, reorient its data and affine to the target axis codes.
    Returns the reoriented mask data (numpy array) and its new affine, or (None, None) on failure.
    """
    if not (mask_path.endswith(('.nii', '.nii.gz'))):
        if mask_path.endswith('.npy'):
            logger.warning(f"Loading .npy file: {mask_path}. Assuming identity affine and no reorientation needed/possible.")
            try:
                data = (np.load(mask_path) > 0).astype(np.uint8)
                return data, np.eye(4)
            except Exception as e:
                logger.error(f"Failed to load .npy file {mask_path}: {e}")
                return None, None
        else:
            logger.error(f"Unsupported format for reorientation: {mask_path}. Only .nii, .nii.gz supported.")
            return None, None

    try:
        img_original = nib.load(mask_path)
    except Exception as e:
        logger.error(f"nibabel failed to load image {mask_path}: {e}")
        return None, None

    original_axcodes = nib.aff2axcodes(img_original.affine)
    logger.info(f"Loading mask: {mask_path}")
    logger.info(f"Original orientation: {original_axcodes}, Original shape: {img_original.shape}")

    if img_original.get_data_dtype() == object or not hasattr(img_original, 'get_fdata'):
        logger.error(f"Image {mask_path} has an unusual data type or is not a standard spatial image. Cannot get float data.")
        return None, None

    if original_axcodes == target_axcodes:
        logger.info(f"Mask is already in target orientation {target_axcodes}.")
        try:
            data_reoriented = (img_original.get_fdata(dtype=np.float32) > 0).astype(np.uint8)
            affine_reoriented = img_original.affine
        except Exception as e:
            logger.error(f"Failed to get data from already oriented image {mask_path}: {e}")
            return None, None
    else:
        logger.info(f"Reorienting mask to {target_axcodes}...")
        try:
            # 1. Get current orientation in 'ornt' format
            current_ornt = nio.io_orientation(img_original.affine)
            
            # 2. Get target orientation in 'ornt' format
            target_ornt = nio.axcodes2ornt(target_axcodes)

            # 3. Calculate the transformation from current to target
            transform_ornt = nio.ornt_transform(current_ornt, target_ornt)

            # 4. Apply this orientation transform to the data array
            original_data = np.asanyarray(img_original.get_fdata(dtype=np.float32)) # Use get_fdata
            data_reoriented_numpy = nio.apply_orientation(original_data, transform_ornt)
            data_reoriented = (data_reoriented_numpy > 0).astype(np.uint8)

            # 5. Calculate the new affine for the reoriented data
            affine_reoriented = img_original.affine @ nio.inv_ornt_aff(transform_ornt, img_original.shape)
            
            final_axcodes = nib.aff2axcodes(affine_reoriented)
            logger.info(f"New orientation: {final_axcodes}, New shape: {data_reoriented.shape}")
            if final_axcodes != target_axcodes:
                 # This can be complex. Forcing a perfectly canonical affine might be an option
                 # but needs careful handling of voxel sizes and origin.
                 # For now, we trust the calculated reoriented affine.
                logger.warning(f"Achieved axcodes {final_axcodes} after reorientation differ slightly from target {target_axcodes}. "
                               f"This can happen with complex initial affines (shears/rotations). The world space should still be {target_axcodes} oriented.")

        except IndexError as ie: # Catch the specific error you encountered
            logger.error(f"IndexError during nibabel reorientation for {mask_path}: {ie}. This might indicate an issue with how nibabel handles this specific affine or data shape.")
            return None, None
        except Exception as e:
            logger.error(f"Failed during reorientation process for {mask_path}: {e}")
            return None, None

    return data_reoriented, affine_reoriented

def save_mask_as_nifti(data: np.ndarray, affine: np.ndarray, output_path: str):
    """Saves a numpy array mask as a NIFTI file with the given affine."""
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # Ensure data is in a format NIFTI can handle, e.g., uint8 for masks
        nifti_img = nib.Nifti1Image(data.astype(np.uint8), affine)
        nib.save(nifti_img, output_path)
        logger.info(f"Saved reoriented mask to: {output_path}")
    except Exception as e:
        logger.error(f"Failed to save NIFTI file {output_path}: {e}")


# --- Centerline Extraction Functions (Modified to work with reoriented data and affine) ---
# (These functions: remove_small_components, extract_skeleton_3d, skeleton_to_graph,
#  prune_skeleton_graph, extract_centerline_paths_from_graph, smooth_centerline,
#  transform_voxel_to_world, save_centerlines_vtk, save_centerlines_txt
#  remain THE SAME as in the previous version you approved, as they operate on numpy arrays
#  and the reoriented_affine. For brevity, I will not repeat them here but assume they are present.)

# Placeholder for the centerline extraction functions from your script
def remove_small_components(mask_data: np.ndarray, min_size=1000) -> np.ndarray:
    labeled_mask, num_features = ndimage.label(mask_data)
    if num_features == 0:
        logger.warning("No connected components found in mask_data during remove_small_components.")
        return np.zeros_like(mask_data, dtype=bool)
    component_sizes = np.bincount(labeled_mask.ravel())
    if len(component_sizes) > 1:
        largest_component_label = component_sizes[1:].argmax() + 1
        largest_component_area = component_sizes[largest_component_label]
        if largest_component_area < min_size:
            logger.warning(f"Largest component ({largest_component_area} voxels) is smaller than min_size ({min_size}). Returning empty mask.")
            return np.zeros_like(mask_data, dtype=bool)
        clean_mask_data = (labeled_mask == largest_component_label)
        logger.info(f"Kept largest component with {largest_component_area} voxels from mask_data.")
    else:
        clean_mask_data = np.zeros_like(mask_data, dtype=bool)
        logger.info("No foreground components found in mask_data.")
    return clean_mask_data.astype(mask_data.dtype)

def extract_skeleton_3d(mask_data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    mask_bool = (mask_data > 0)
    distance = ndimage.distance_transform_edt(mask_bool)
    skeleton = morphology.skeletonize(mask_bool)
    return skeleton, distance

def skeleton_to_graph(skeleton_data: np.ndarray, distance_map_data: np.ndarray) -> nx.Graph:
    points_voxel = np.argwhere(skeleton_data > 0)
    if len(points_voxel) == 0:
        raise ValueError("No skeleton points found in skeleton_data.")
    logger.info(f"Found {len(points_voxel)} skeleton points.")
    tree = KDTree(points_voxel)
    G = nx.Graph()
    for i, point_vox in enumerate(points_voxel):
        radius_vox_units = distance_map_data[tuple(point_vox)]
        G.add_node(i, pos_voxel=point_vox, radius_voxel_units=radius_vox_units)
    for i, point_vox in enumerate(points_voxel):
        neighbors_indices = tree.query_ball_point(point_vox.reshape(1, -1), r=1.733)[0]
        for j in neighbors_indices:
            if j > i: G.add_edge(i, j)
    return G

def prune_skeleton_graph(G: nx.Graph, min_branch_length: int = 5) -> nx.Graph:
    # (Using the pruning logic from your original script - assuming it's okay)
    logger.info(f"Starting pruning with min_branch_length={min_branch_length}")
    G_pruned = G.copy() # Work on a copy
    # Iteratively remove short endpoint branches
    while True:
        endpoints = [n for n in G_pruned.nodes() if G_pruned.degree(n) == 1]
        if not endpoints: break
        
        removed_in_pass_nodes = set()
        for endpoint in endpoints: # No tqdm here for simplicity, add if needed
            if endpoint in removed_in_pass_nodes: continue # Already processed or marked
            
            branch = [endpoint]
            current = endpoint
            is_short_branch_to_prune = False
            
            # Trace the branch
            for _ in range(min_branch_length): # Trace up to min_branch_length + 1 nodes
                # Neighbors not already in branch or marked for removal in this pass
                neighbors = [n for n in G_pruned.neighbors(current) if n not in branch and n not in removed_in_pass_nodes]
                if len(neighbors) != 1: # Reached junction, another endpoint, or already pruned part
                    # If it's short AND doesn't end in a junction node, it's a short spur
                    if len(branch) < min_branch_length and (not neighbors or all(G_pruned.degree(n) <= 2 for n in neighbors)):
                         is_short_branch_to_prune = True
                    break 
                current = neighbors[0]
                branch.append(current)
            else: # Path is longer than min_branch_length or ends, check if last node is junction
                if len(branch) < min_branch_length and G_pruned.degree(current) <= 2 : # still short and not ending at a significant junction
                    is_short_branch_to_prune = True


            if is_short_branch_to_prune:
                # Mark entire branch for removal if it's short and doesn't connect to a major junction
                # The condition for pruning needs to be robust: a short branch that isn't essential
                last_node_in_branch = branch[-1]
                # If the branch is short and its end is not a junction (degree > 2)
                if len(branch) < min_branch_length:
                    # If the branch connects to a junction, remove up to the junction
                    if G_pruned.degree(last_node_in_branch) > 2 and len(branch) > 1 :
                        removed_in_pass_nodes.update(branch[:-1])
                    # Otherwise, if it's a short isolated segment or ends in another endpoint, remove all
                    else:
                        removed_in_pass_nodes.update(branch)
        
        if not removed_in_pass_nodes: break
        G_pruned.remove_nodes_from(list(removed_in_pass_nodes))
        # logger.debug(f"Pruning pass removed {len(removed_in_pass_nodes)} nodes.")

    logger.info(f"Removed {len(G.nodes()) - len(G_pruned.nodes())} nodes from short branches.")
    
    isolated = list(nx.isolates(G_pruned))
    G_pruned.remove_nodes_from(isolated)
    logger.info(f"Removed {len(isolated)} isolated nodes after pruning.")
    
    if len(G_pruned) > 0:
        components = list(nx.connected_components(G_pruned))
        if len(components) > 1:
            largest_cc = max(components, key=len)
            nodes_to_keep = set(largest_cc)
            nodes_to_remove_cc = set(G_pruned.nodes()) - nodes_to_keep
            G_pruned.remove_nodes_from(list(nodes_to_remove_cc))
            logger.info(f"Kept largest connected component ({len(largest_cc)} nodes), removed {len(nodes_to_remove_cc)} other nodes.")
    return G_pruned

def extract_centerline_paths_from_graph(G_pruned: nx.Graph) -> Tuple[List[Dict], Union[int, None], List[int]]:
    if len(G_pruned) == 0: raise ValueError("Graph is empty after pruning, cannot extract paths.")
    
    endpoints = [n for n in G_pruned.nodes() if G_pruned.degree(n) == 1]
    logger.info(f"Found {len(endpoints)} endpoints in pruned graph for path extraction.")
    
    inlet_node_idx = None
    outlet_node_indices = []

    if len(endpoints) < 2:
        if len(G_pruned.nodes()) < 2:
             logger.warning("Not enough nodes for centerline extraction even with fallback. Returning empty.")
             return [], None, []
        logger.warning("Less than 2 endpoints. Using most distant nodes as effective endpoints.")
        nodes = list(G_pruned.nodes())
        positions_voxel = np.array([G_pruned.nodes[n]['pos_voxel'] for n in nodes])
        from scipy.spatial.distance import cdist # More direct for pair-wise
        dist_matrix = cdist(positions_voxel, positions_voxel)
        np.fill_diagonal(dist_matrix, -1) # Avoid picking same node
        i_dm, j_dm = np.unravel_index(dist_matrix.argmax(), dist_matrix.shape)
        endpoints = [nodes[i_dm], nodes[j_dm]]
        logger.info(f"Using {len(endpoints)} most distant nodes as effective endpoints: {endpoints}")

    if endpoints:
        inlet_node_idx = max(endpoints, key=lambda n: G_pruned.nodes[n]['radius_voxel_units'])
        outlet_node_indices = [e for e in endpoints if e != inlet_node_idx]
        if not outlet_node_indices and len(endpoints) == 1: # Only one endpoint total
            outlet_node_indices = endpoints # Treat it as an outlet too for a single path
        elif not outlet_node_indices and len(endpoints) > 1: # Should not happen if inlet is one of them
             logger.warning("No distinct outlets found even with multiple endpoints. Using first non-inlet endpoint.")
             outlet_node_indices = [ep for ep in endpoints if ep != inlet_node_idx][:1] # Fallback
    else: # No endpoints after fallback (e.g. graph is a cycle or too small)
        logger.warning("No endpoints identified. Cannot define inlet/outlets conventionally.")
        return [], None, []


    centerlines_voxel_data = []
    for outlet_idx in outlet_node_indices:
        try:
            path_node_ids = nx.shortest_path(G_pruned, inlet_node_idx, outlet_idx)
            centerline_points_voxel = np.array([G_pruned.nodes[n]['pos_voxel'] for n in path_node_ids])
            centerline_radii_voxel_units = np.array([G_pruned.nodes[n]['radius_voxel_units'] for n in path_node_ids])
            centerlines_voxel_data.append({
                'points_voxel': centerline_points_voxel, # These are indices into the reoriented mask
                'radii_voxel_units': centerline_radii_voxel_units
            })
        except nx.NetworkXNoPath:
            logger.warning(f"No path found from inlet {inlet_node_idx} to outlet {outlet_idx}")
            
    if not centerlines_voxel_data and inlet_node_idx is not None and not outlet_node_indices and len(G_pruned.nodes) > 1:
        # Case: Graph exists, inlet found, but no distinct outlets (e.g. single long segment treated as endpoint)
        # Try to form a path from inlet to furthest node from inlet (if it's not itself an endpoint)
        logger.info("Attempting to find longest path from single identified inlet/endpoint.")
        path_node_ids_lengths = nx.single_source_shortest_path_length(G_pruned, inlet_node_idx) # Corrected variable name
        if len(path_node_ids_lengths) > 1:
            furthest_node = max(path_node_ids_lengths, key=path_node_ids_lengths.get)
            try:
                actual_path = nx.shortest_path(G_pruned, inlet_node_idx, furthest_node)
                centerline_points_voxel = np.array([G_pruned.nodes[n]['pos_voxel'] for n in actual_path])
                centerline_radii_voxel_units = np.array([G_pruned.nodes[n]['radius_voxel_units'] for n in actual_path])
                centerlines_voxel_data.append({
                    'points_voxel': centerline_points_voxel,
                    'radii_voxel_units': centerline_radii_voxel_units
                })
                outlet_node_indices.append(furthest_node) # Update outlet list
            except nx.NetworkXNoPath:
                 logger.warning(f"Fallback: No path found from inlet {inlet_node_idx} to furthest node {furthest_node}")


    return centerlines_voxel_data, inlet_node_idx, outlet_node_indices

def smooth_centerline(points_voxel: np.ndarray, radii_voxel_units: np.ndarray, smoothing_factor: float = 0.1):
    from scipy.interpolate import UnivariateSpline # Keep import local if not used elsewhere often
    if len(points_voxel) < 4: return points_voxel, radii_voxel_units
    
    diffs = np.diff(points_voxel, axis=0)
    distances = np.sqrt(np.sum(diffs**2, axis=1))
    arc_length = np.concatenate([[0], np.cumsum(distances)])
    
    smoothed_points_voxel = np.zeros_like(points_voxel, dtype=float) # Use float for smoothed
    for i in range(points_voxel.shape[1]):
        try:
            spline = UnivariateSpline(arc_length, points_voxel[:, i], s=smoothing_factor * len(points_voxel))
            smoothed_points_voxel[:, i] = spline(arc_length)
        except Exception as e:
            logger.warning(f"Spline smoothing failed for coordinate {i}, keeping original. Error: {e}")
            smoothed_points_voxel[:, i] = points_voxel[:, i]
    
    try:
        radius_spline = UnivariateSpline(arc_length, radii_voxel_units, s=smoothing_factor * len(radii_voxel_units))
        smoothed_radii_voxel_units = radius_spline(arc_length)
        smoothed_radii_voxel_units[smoothed_radii_voxel_units < 0] = 0
    except Exception as e:
        logger.warning(f"Spline smoothing failed for radii, keeping original. Error: {e}")
        smoothed_radii_voxel_units = radii_voxel_units
    
    return smoothed_points_voxel, smoothed_radii_voxel_units

def transform_voxel_to_world(points_voxel: np.ndarray, radii_voxel_units: Union[np.ndarray, float], reoriented_affine: np.ndarray) -> Tuple[np.ndarray, Union[np.ndarray, float]]:
    """
    Transforms voxel coordinates (indices into reoriented_mask_data) to world coordinates
    using the reoriented_affine. Scales radii to physical units.
    """
    points_voxel_arr = np.asarray(points_voxel)
    if points_voxel_arr.ndim == 1:
        points_voxel_arr = points_voxel_arr.reshape(1, -1)

    homogeneous_coords = np.hstack((points_voxel_arr, np.ones((points_voxel_arr.shape[0], 1))))
    # The reoriented_affine maps the reoriented voxel indices to the target world space (e.g., RAS)
    world_coords_homogeneous = homogeneous_coords @ reoriented_affine.T
    points_world = world_coords_homogeneous[:, :3]

    voxel_spacing = np.sqrt(np.sum(reoriented_affine[:3, :3]**2, axis=0)) # More robust for rotated affines
    valid_spacings = voxel_spacing[voxel_spacing > EPSILON]
    mean_spacing = np.mean(valid_spacings) if len(valid_spacings) > 0 else 1.0
    if mean_spacing == 1.0 and len(valid_spacings) == 0:
        logger.warning("All voxel spacings are zero or near zero. Using mean_spacing=1.0 for radius conversion.")

    radii_world = np.asarray(radii_voxel_units) * mean_spacing
    
    if world_coords_homogeneous.shape[0] == 1: # Single point input
        return points_world[0], radii_world.item() if radii_world.ndim == 0 else radii_world[0]
    return points_world, radii_world

def save_centerlines_vtk(centerlines_world_data: List[Dict], output_path: str):
    """Save centerlines in VTK format. Expects 'points_world' and 'radii_world' in dictionaries."""
    try:
        import vtk
    except ImportError:
        logger.error("VTK Python library not available, skipping VTK export.")
        return

    polyData = vtk.vtkPolyData()
    points_vtk = vtk.vtkPoints()
    lines_vtk = vtk.vtkCellArray()
    radius_vtk_array = vtk.vtkDoubleArray()
    radius_vtk_array.SetName("Radius")
    radius_vtk_array.SetNumberOfComponents(1)

    point_id_counter = 0
    for cl_dict in centerlines_world_data:
        points_world = cl_dict['points_world']
        radii_world = cl_dict['radii_world']

        line = vtk.vtkPolyLine()
        line.GetPointIds().SetNumberOfIds(len(points_world))
        
        for j, (point_w, radius_w) in enumerate(zip(points_world, radii_world)):
            points_vtk.InsertNextPoint(point_w)
            radius_vtk_array.InsertNextTuple1(radius_w)
            line.GetPointIds().SetId(j, point_id_counter)
            point_id_counter += 1
        
        lines_vtk.InsertNextCell(line)
    
    polyData.SetPoints(points_vtk)
    polyData.SetLines(lines_vtk)
    polyData.GetPointData().AddArray(radius_vtk_array)
    polyData.GetPointData().SetActiveScalars("Radius")
    
    writer = vtk.vtkXMLPolyDataWriter()
    writer.SetFileName(output_path)
    writer.SetInputData(polyData)
    writer.Write()
    logger.info(f"Saved VTK file with world coordinates: {output_path}")

def save_centerlines_txt(centerlines_world_data: List[Dict], output_path: str):
    """Save centerlines in simple text format. Expects 'points_world' and 'radii_world'."""
    with open(output_path, 'w') as f:
        f.write(f"# Centerline data in WORLD coordinates (Target Orientation: {TARGET_ORIENTATION_AXCODES})\n")
        f.write("# Format: centerline_id point_id x_world y_world z_world radius_world\n")
        
        for cl_id, cl_dict in enumerate(centerlines_world_data):
            points_world = cl_dict['points_world']
            radii_world = cl_dict['radii_world']
            f.write(f"\n# Centerline {cl_id}\n")
            for pt_id, (point_w, radius_w) in enumerate(zip(points_world, radii_world)):
                f.write(f"{cl_id} {pt_id} {point_w[0]:.6f} {point_w[1]:.6f} "
                       f"{point_w[2]:.6f} {radius_w:.6f}\n")
    logger.info(f"Saved TXT file with world coordinates: {output_path}")


# --- Main Centerline Extraction Workflow ---
def extract_and_save_centerlines(
    primary_mask_path: str,
    output_dir: str,
    min_branch_length: int = 10,
    smoothing_factor: float = 0.1,
    min_component_size: int = 1000
) -> Tuple[Union[Dict, None], Union[np.ndarray, None]]: # Adjusted return annotation
    """
    Full workflow: Load primary mask, reorient, extract centerlines, save.
    Returns centerline data package and the reoriented affine, or (None, None) on failure.
    """
    os.makedirs(output_dir, exist_ok=True)

    logger.info(f"--- Starting Centerline Extraction for: {primary_mask_path} ---")
    logger.info(f"Target orientation for all outputs: {TARGET_ORIENTATION_AXCODES}")

    # 1. Load and reorient the primary mask
    reoriented_mask_data, reoriented_affine = load_and_reorient_mask(primary_mask_path, TARGET_ORIENTATION_AXCODES)
    if reoriented_mask_data is None or reoriented_affine is None:
        logger.error(f"Failed to load or reorient primary mask: {primary_mask_path}. Aborting centerline extraction.")
        return None, None # Return two Nones as expected by caller
        
    # Save the reoriented primary mask for reference
    base, ext = os.path.splitext(os.path.basename(primary_mask_path))
    if ext == ".gz": base, ext2 = os.path.splitext(base); ext = ext2 + ext
    reoriented_mask_filename = f"{base}_reoriented_{''.join(TARGET_ORIENTATION_AXCODES)}{ext if ext.startswith('.') else '.nii.gz'}"
    reoriented_mask_output_path = os.path.join(output_dir, reoriented_mask_filename)
    save_mask_as_nifti(reoriented_mask_data, reoriented_affine, reoriented_mask_output_path)


    # 2. Process reoriented mask data
    logger.info("Removing disconnected components from reoriented mask...")
    mask_cleaned = remove_small_components(reoriented_mask_data, min_component_size)
    if not np.any(mask_cleaned):
        logger.error("Reoriented mask is empty after removing small components.")
        return None, reoriented_affine # Return affine for context, package is None

    logger.info("Extracting skeleton from reoriented mask...")
    skeleton_data, distance_map_data = extract_skeleton_3d(mask_cleaned)
    if not np.any(skeleton_data):
        logger.error("Skeleton is empty. Check reoriented mask or skeletonization parameters.")
        return None, reoriented_affine

    logger.info("Converting skeleton to graph...")
    try:
        G = skeleton_to_graph(skeleton_data, distance_map_data)
    except ValueError as e: # Catch if skeleton_to_graph raises ValueError (e.g. no skeleton points)
        logger.error(f"Error converting skeleton to graph: {e}")
        return None, reoriented_affine
        
    logger.info(f"Initial graph from reoriented skeleton: {len(G.nodes())} N, {len(G.edges())} E")
    if len(G.nodes()) == 0:
        logger.error("Graph from reoriented skeleton is empty.")
        return None, reoriented_affine

    logger.info("Pruning small branches...")
    G_pruned = prune_skeleton_graph(G, min_branch_length)
    logger.info(f"Pruned graph: {len(G_pruned.nodes())} N, {len(G_pruned.edges())} E")
    if len(G_pruned.nodes()) == 0:
        logger.error("Graph empty after pruning.")
        return None, reoriented_affine

    logger.info("Extracting centerline paths (voxel coords w.r.t. reoriented mask)...")
    try:
        centerlines_voxel_coords, inlet_node_idx, outlet_node_indices = extract_centerline_paths_from_graph(G_pruned)
    except ValueError as e: # Catch if graph is empty after pruning
        logger.error(f"Error extracting centerline paths: {e}")
        return None, reoriented_affine

    if not centerlines_voxel_coords:
        logger.error("No centerlines extracted from graph.")
        return None, reoriented_affine

    # 3. Smooth centerlines (still in voxel space of reoriented_mask_data)
    if smoothing_factor > 0:
        logger.info("Smoothing centerlines (voxel coords)...")
        for cl_dict in centerlines_voxel_coords:
            cl_dict['points_voxel'], cl_dict['radii_voxel_units'] = smooth_centerline(
                cl_dict['points_voxel'], cl_dict['radii_voxel_units'], smoothing_factor)

    # 4. Prepare data for saving (convert to world coordinates, gather terminal info)
    centerlines_world_data = []
    for cl_vox_dict in centerlines_voxel_coords:
        points_w, radii_w = transform_voxel_to_world(
            cl_vox_dict['points_voxel'],
            cl_vox_dict['radii_voxel_units'],
            reoriented_affine
        )
        centerlines_world_data.append({'points_world': points_w, 'radii_world': radii_w})

    terminals_data_voxel = {}
    terminals_data_world = {}
    if inlet_node_idx is not None and inlet_node_idx in G_pruned:
        terminals_data_voxel['inlet_pos_voxel'] = G_pruned.nodes[inlet_node_idx]['pos_voxel']
        terminals_data_voxel['inlet_radius_voxel_units'] = G_pruned.nodes[inlet_node_idx]['radius_voxel_units']
        iw_pos, iw_rad = transform_voxel_to_world(terminals_data_voxel['inlet_pos_voxel'], terminals_data_voxel['inlet_radius_voxel_units'], reoriented_affine)
        terminals_data_world['inlet_pos_world'] = iw_pos
        terminals_data_world['inlet_radius_world'] = iw_rad
    
    terminals_data_voxel['outlets_pos_voxel'] = [G_pruned.nodes[n]['pos_voxel'] for n in outlet_node_indices if n in G_pruned]
    terminals_data_voxel['outlets_radii_voxel_units'] = [G_pruned.nodes[n]['radius_voxel_units'] for n in outlet_node_indices if n in G_pruned]
    
    terminals_data_world['outlets_pos_world'] = []
    terminals_data_world['outlets_radii_world'] = []
    for ov_pos, ov_rad in zip(terminals_data_voxel.get('outlets_pos_voxel', []), terminals_data_voxel.get('outlets_radii_voxel_units', [])):
        ow_pos, ow_rad = transform_voxel_to_world(ov_pos, ov_rad, reoriented_affine)
        terminals_data_world['outlets_pos_world'].append(ow_pos)
        terminals_data_world['outlets_radii_world'].append(ow_rad)

    # 5. Save results
    logger.info("Saving centerline results (VTP, TXT will have world coordinates)...")
    save_centerlines_vtk(centerlines_world_data, os.path.join(output_dir, "centerlines_world.vtp"))
    save_centerlines_txt(centerlines_world_data, os.path.join(output_dir, "centerlines_world.txt"))

    centerline_save_data_package = {
        'centerlines_voxel_space': centerlines_voxel_coords,
        'centerlines_world_space': centerlines_world_data,
        'terminals_voxel_space': terminals_data_voxel,
        'terminals_world_space': terminals_data_world,
        'reoriented_affine_matrix': reoriented_affine,
        'target_orientation': TARGET_ORIENTATION_AXCODES
    }
    np.save(os.path.join(output_dir, "centerline_data_package.npy"),
            centerline_save_data_package, allow_pickle=True)

    with open(os.path.join(output_dir, "terminals_world.txt"), 'w') as f:
        f.write(f"# Terminal data in WORLD coordinates (Target Orientation: {TARGET_ORIENTATION_AXCODES})\n")
        f.write("# Format: Type X_world Y_world Z_world Radius_world\n")
        if 'inlet_pos_world' in terminals_data_world:
            ipw, irw = terminals_data_world['inlet_pos_world'], terminals_data_world['inlet_radius_world']
            f.write(f"Inlet: {ipw[0]:.6f}, {ipw[1]:.6f}, {ipw[2]:.6f}, radius: {irw:.6f}\n")
        for i, (opw, orw) in enumerate(zip(terminals_data_world.get('outlets_pos_world',[]), terminals_data_world.get('outlets_radii_world',[]))):
            f.write(f"Outlet_{i+1}: {opw[0]:.6f}, {opw[1]:.6f}, {opw[2]:.6f}, radius: {orw:.6f}\n")

    logger.info(f"Centerline extraction finished. Results in: {output_dir}")
    return centerline_save_data_package, reoriented_affine


# --- Main Execution ---
if __name__ == "__main__":
    primary_mask_for_centerlines = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/mida_processed_masks/mida_arteries_mask.nii.gz"
    
    other_mask_paths = [
         "/Users/c3495249/Coding/Gemini_Pro_Vasculature/mida_processed_masks/mida_total_csf_mask.nii.gz",
         "/Users/c3495249/Coding/Gemini_Pro_Vasculature/mida_processed_masks/mida_total_gm_mask.nii.gz",
         "/Users/c3495249/Coding/Gemini_Pro_Vasculature/mida_processed_masks/mida_total_wm_mask.nii.gz",
    ]
    
    base_output_directory = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/data/mida_reoriented_processing_v2"

    primary_mask_name_no_ext = os.path.basename(primary_mask_for_centerlines)
    while any(primary_mask_name_no_ext.endswith(ext) for ext in ['.nii', '.gz']): # Robustly remove .nii or .nii.gz
        primary_mask_name_no_ext = os.path.splitext(primary_mask_name_no_ext)[0]
    
    centerline_output_dir = os.path.join(base_output_directory, f"{primary_mask_name_no_ext}_centerlines_TARGET_{''.join(TARGET_ORIENTATION_AXCODES)}")
    other_masks_output_dir = os.path.join(base_output_directory, f"other_reoriented_masks_TARGET_{''.join(TARGET_ORIENTATION_AXCODES)}")

    os.makedirs(centerline_output_dir, exist_ok=True)
    os.makedirs(other_masks_output_dir, exist_ok=True)

    logger.info(f"--- STARTING FULL PIPELINE ---")
    logger.info(f"Target orientation for all outputs: {TARGET_ORIENTATION_AXCODES}")
    logger.info(f"Primary mask for centerlines: {primary_mask_for_centerlines}")
    logger.info(f"Centerline outputs will be in: {centerline_output_dir}")
    if other_mask_paths:
        logger.info(f"Other masks to reorient: {len(other_mask_paths)}")
        logger.info(f"Reoriented 'other' masks will be saved in: {other_masks_output_dir}")

    # 1. Extract centerlines from the primary mask
    final_reoriented_affine_for_primary = None # Initialize
    try:
        centerline_results_package, final_reoriented_affine_for_primary = extract_and_save_centerlines(
            primary_mask_path=primary_mask_for_centerlines,
            output_dir=centerline_output_dir,
            min_branch_length=5,
            smoothing_factor=0.05,
            min_component_size=500
        )
        if centerline_results_package:
            logger.info(f"Centerline extraction for '{primary_mask_for_centerlines}' completed successfully.")
        else:
            logger.error(f"Centerline extraction for '{primary_mask_for_centerlines}' failed or produced no output package.")

    except Exception as e: # Catch any unexpected error from the main workflow
        logger.error(f"Unhandled error during centerline extraction pipeline for '{primary_mask_for_centerlines}': {e}", exc_info=True)

    # 2. Reorient and save other specified masks
    if other_mask_paths:
        logger.info(f"\n--- Processing Other Masks ---")
        for other_mask_path in other_mask_paths:
            if not os.path.exists(other_mask_path):
                logger.warning(f"Other mask file not found, skipping: {other_mask_path}")
                continue
            try:
                logger.info(f"Processing other mask: {other_mask_path}")
                # Use the globally defined TARGET_ORIENTATION_AXCODES
                reoriented_data, reoriented_affine = load_and_reorient_mask(other_mask_path, TARGET_ORIENTATION_AXCODES)
                
                if reoriented_data is not None and reoriented_affine is not None:
                    other_mask_name_no_ext = os.path.basename(other_mask_path)
                    while any(other_mask_name_no_ext.endswith(ext) for ext in ['.nii', '.gz']):
                        other_mask_name_no_ext = os.path.splitext(other_mask_name_no_ext)[0]
                    
                    # Get original extension for saving
                    original_basename = os.path.basename(other_mask_path)
                    if original_basename.endswith(".nii.gz"):
                        save_ext = ".nii.gz"
                    elif original_basename.endswith(".nii"):
                        save_ext = ".nii"
                    else: # Fallback, though load_and_reorient_mask should prevent this for .nii/.nii.gz
                        save_ext = ".nii.gz" 
                        
                    output_filename = f"{other_mask_name_no_ext}_reoriented_{''.join(TARGET_ORIENTATION_AXCODES)}{save_ext}"
                    full_output_path = os.path.join(other_masks_output_dir, output_filename)
                    
                    save_mask_as_nifti(reoriented_data, reoriented_affine, full_output_path)
                    logger.info(f"Successfully reoriented and saved: {other_mask_path} -> {full_output_path}")
                else:
                    logger.error(f"Skipping saving of other mask {other_mask_path} due to load/reorientation failure.")

            except Exception as e:
                logger.error(f"Failed to process other mask {other_mask_path}: {e}", exc_info=True)
    else:
        logger.info("No 'other_mask_paths' provided for reorientation.")
        
    logger.info(f"--- FULL PIPELINE FINISHED ---")# extract_mida_masks.py
import nibabel as nib
import numpy as np
import os

MIDA_LABEL_FILE = "/Users/c3495249/Coding/Gemini_Pro_Vasculature/data/MIDA_v1.nii" # Path to your MIDA label map
OUTPUT_DIR = "mida_processed_masks"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Define label values from your text file
LABELS = {
    "GM_Brain": 10,
    "WM_Brain": 12,
    "GM_Cerebellum": 2,
    "WM_Cerebellum": 9,
    "CSF_General": 32,
    "CSF_Ventricles": 6,
    "Arteries": 24
}

# --- Function to create and save a binary mask ---
def create_save_mask(label_data, affine, header, label_value, output_filename_base):
    mask_data = (label_data == label_value).astype(np.uint8)
    if np.sum(mask_data) == 0:
        print(f"Warning: No voxels found for label {label_value} ({output_filename_base}). Mask will be empty.")
    
    mask_img = nib.Nifti1Image(mask_data, affine, header)
    nib.save(mask_img, os.path.join(OUTPUT_DIR, f"{output_filename_base}.nii.gz"))
    print(f"Saved {output_filename_base}.nii.gz ({np.sum(mask_data)} voxels)")

# --- Function to combine masks (e.g., GM_Brain + GM_Cerebellum) ---
def combine_save_masks(label_data, affine, header, label_values_list, output_filename_base):
    combined_mask_data = np.zeros(label_data.shape, dtype=np.uint8)
    for label_value in label_values_list:
        combined_mask_data[label_data == label_value] = 1
    
    if np.sum(combined_mask_data) == 0:
        print(f"Warning: No voxels found for combined labels {label_values_list} ({output_filename_base}). Mask will be empty.")

    mask_img = nib.Nifti1Image(combined_mask_data, affine, header)
    nib.save(mask_img, os.path.join(OUTPUT_DIR, f"{output_filename_base}.nii.gz"))
    print(f"Saved combined {output_filename_base}.nii.gz ({np.sum(combined_mask_data)} voxels)")

if __name__ == "__main__":
    print(f"Loading MIDA label file: {MIDA_LABEL_FILE}")
    try:
        img = nib.load(MIDA_LABEL_FILE)
        # Ensure data is integer for label comparison
        # MIDA data type was float64, but values are integer labels.
        label_data_float = img.get_fdata()
        if not np.all(np.mod(label_data_float, 1) == 0):
            print("Warning: Loaded data contains non-integer values, but expecting a label map.")
        label_data_int = label_data_float.astype(np.int32) # Use a robust integer type

        affine = img.affine
        header = img.header # Preserve header for consistent geometry

        print(f"Successfully loaded. Shape: {label_data_int.shape}, Affine:\n{affine}")

        # Create individual masks
        create_save_mask(label_data_int, affine, header, LABELS["Arteries"], "mida_arteries_mask")
        
        # Create combined masks for GM and WM
        # GM = Brain GM + Cerebellum GM
        combine_save_masks(label_data_int, affine, header, 
                           [LABELS["GM_Brain"], LABELS["GM_Cerebellum"]], 
                           "mida_total_gm_mask")
        
        # WM = Brain WM + Cerebellum WM
        combine_save_masks(label_data_int, affine, header, 
                           [LABELS["WM_Brain"], LABELS["WM_Cerebellum"]], 
                           "mida_total_wm_mask")

        # CSF = General CSF + Ventricles CSF
        combine_save_masks(label_data_int, affine, header,
                           [LABELS["CSF_General"], LABELS["CSF_Ventricles"]],
                           "mida_total_csf_mask")

        print(f"Mask extraction complete. Files saved in {OUTPUT_DIR}")

    except FileNotFoundError:
        print(f"Error: MIDA file not found at {MIDA_LABEL_FILE}")
    except Exception as e:
        print(f"An error occurred during mask extraction: {e}")# src/perfusion_solver.py
from __future__ import annotations
import numpy as np
import networkx as nx
import logging
from typing import Dict, Tuple, Optional, List

from src import constants, config_manager, utils

logger = logging.getLogger(__name__)

def calculate_segment_resistance(length: float, radius: float, viscosity: float) -> float:
    """Calculates hydraulic resistance of a cylindrical segment using Poiseuille's law.
    R = (8 * mu * L) / (pi * r^4)
    """
    if radius < constants.EPSILON: # Avoid division by zero or extremely small radius
        return np.inf
    if length < constants.EPSILON: # Segment with negligible length
        # Return a very small resistance to ensure conductance is high but finite
        return constants.EPSILON
    return (8.0 * viscosity * length) / (constants.PI * (radius**4))

def solve_1d_poiseuille_flow(
    graph: nx.DiGraph,
    config: dict,
    root_pressure_val: Optional[float] = None,
    terminal_flows_val: Optional[Dict[str, float]] = None
) -> nx.DiGraph:
    """Solves for nodal pressures and segmental flows in a vascular graph.

    Modifies the input graph in-place by adding/updating:
    - 'pressure' attribute to nodes.
    - 'flow_solver' attribute to edges.

    Assumptions on input graph:
    - Nodes:
        - Must have a unique ID.
        - Pressure inlet nodes must have `is_flow_root=True`.
        - Flow sink nodes (terminals) must have a 'Q_flow' attribute (target demand/outflow)
          and be identifiable (e.g., `type='synthetic_terminal'` and `graph.out_degree(node_id) == 0`,
          or an explicit `is_flow_terminal=True`).
    - Edges:
        - Must have 'length' and 'radius' attributes. The 'radius' attribute on an edge
          typically represents the radius of the upstream segment/node.
    """
    if graph.number_of_nodes() == 0:
        logger.warning("Flow solver: Graph is empty. Nothing to solve.")
        return graph

    logger.info(f"Starting 1D Poiseuille flow solution for graph with {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges.")

    viscosity = config_manager.get_param(config, "vascular_properties.blood_viscosity", constants.DEFAULT_BLOOD_VISCOSITY)
    if root_pressure_val is None:
        inlet_pressure = config_manager.get_param(config, "perfusion_solver.inlet_pressure", 10000.0) # Pa
    else:
        inlet_pressure = root_pressure_val

    node_list = list(graph.nodes())
    node_to_idx = {node_id: i for i, node_id in enumerate(node_list)}
    num_nodes = len(node_list)

    # Initialize graph attributes in case of early exit or errors
    for node_id_init in node_list:
        graph.nodes[node_id_init]['pressure'] = np.nan
    for u_init, v_init, data_init in graph.edges(data=True):
        data_init['flow_solver'] = np.nan

    if num_nodes == 0: # Should have been caught, but as a safeguard
        return graph

    A_matrix = np.zeros((num_nodes, num_nodes), dtype=float)
    B_vector = np.zeros(num_nodes, dtype=float)
    num_defined_pressure_bcs = 0
    num_defined_flow_sinks = 0

    logger.debug("--- Flow Solver: Assembling System Matrix A and Vector B ---")
    for i, node_id_i in enumerate(node_list):
        node_i_data = graph.nodes[node_id_i]

        # --- Set up equation for node i ---
        if node_i_data.get('is_flow_root', False):
            # Pressure Boundary Condition: P_i = inlet_pressure
            A_matrix[i, :] = 0.0
            A_matrix[i, i] = 1.0
            B_vector[i] = inlet_pressure
            num_defined_pressure_bcs += 1
            logger.debug(f"  BC Set (Pressure): Node {node_id_i} (type: {node_i_data.get('type')}) is flow_root. Row {i}: A[{i},{i}]=1, B[{i}]={inlet_pressure:.2f}")
            continue # Move to the next node

        # Flow Conservation Equation for non-root nodes: sum(G_ij * (P_j - P_i)) = Q_source_sink_i
        # Rearranged: sum(G_ij * P_j) - (sum(G_ij)) * P_i = Q_source_sink_i
        # So, A[i,i] = -sum(G_ij) and A[i,j] = G_ij.
        # Or, (sum(G_ij)) * P_i - sum(G_ij * P_j) = -Q_source_sink_i
        # So, A[i,i] = sum_over_j(G_ij) and A[i,j] = -G_ij
        # The current implementation uses the second form for A_matrix terms.

        sum_conductances_at_i = 0.0
        # Consider all physical neighbors (connected by an edge, regardless of direction in DiGraph)
        physical_neighbors = set(graph.predecessors(node_id_i)) | set(graph.successors(node_id_i))
        current_row_terms_log = []

        for neighbor_id_j in physical_neighbors:
            if neighbor_id_j not in node_to_idx:
                logger.error(f"  Error: Neighbor {neighbor_id_j} of {node_id_i} not in node_to_idx map. Skipping.")
                continue
            j = node_to_idx[neighbor_id_j]

            edge_data = None
            if graph.has_edge(node_id_i, neighbor_id_j):
                edge_data = graph.edges[node_id_i, neighbor_id_j]
            elif graph.has_edge(neighbor_id_j, node_id_i):
                edge_data = graph.edges[neighbor_id_j, node_id_i]

            if edge_data:
                length = edge_data.get('length', 0.0)
                # Radius for resistance calculation is taken from the edge attribute
                radius = edge_data.get('radius', constants.MIN_VESSEL_RADIUS_MM)
                radius = max(radius, constants.MIN_VESSEL_RADIUS_MM * 0.01) # Ensure tiny positive
                if length < constants.EPSILON:
                    length = constants.EPSILON # Use tiny length for zero-length segments

                resistance = calculate_segment_resistance(length, radius, viscosity)
                conductance = 0.0
                if resistance < np.inf and resistance > constants.EPSILON:
                    conductance = 1.0 / resistance
                elif resistance <= constants.EPSILON: # Effectively zero resistance
                    conductance = 1.0 / (constants.EPSILON * 10) # Very high conductance
                    logger.debug(f"  Edge involving {node_id_i}-{neighbor_id_j}: R~0, using high G={conductance:.1e}")

                if conductance > 0:
                    A_matrix[i, j] -= conductance # Off-diagonal term: -G_ij
                    sum_conductances_at_i += conductance
                    current_row_terms_log.append(f"G_({node_id_i}-{neighbor_id_j})={conductance:.2e} (to P_{neighbor_id_j})")
            else:
                 logger.warning(f"  No edge data found between physically connected {node_id_i} and {neighbor_id_j}.")

        A_matrix[i, i] = sum_conductances_at_i # Diagonal term: sum_over_j(G_ij)

        # Handle flow sinks (outflow boundary condition)
        # A terminal for flow sink is a node with no outgoing segments AND it's not a pressure root.
        is_graph_terminal_for_flow = (graph.out_degree(node_id_i) == 0 and not node_i_data.get('is_flow_root', False))

        if (node_i_data.get('type') == 'synthetic_terminal' and is_graph_terminal_for_flow) or \
           node_i_data.get('is_flow_terminal', False): # Explicitly marked as a flow terminal
            outflow_demand = 0.0
            if terminal_flows_val and node_id_i in terminal_flows_val: # Prioritize explicitly passed terminal flows
                outflow_demand = terminal_flows_val[node_id_i]
            elif 'Q_flow' in node_i_data: # Fallback to Q_flow attribute on the node
                outflow_demand = node_i_data['Q_flow']
            else:
                logger.warning(f"  Terminal node {node_id_i} (type: {node_i_data.get('type')}) has no Q_flow or provided outflow. Assuming zero demand.")

            B_vector[i] -= outflow_demand # Flow leaving node i is -Q on RHS (sum(G_ij*P_j) - sum(G_ij)*P_i = -Q_out)
            num_defined_flow_sinks +=1
            logger.debug(f"  BC Set (Flow Sink): Node {node_id_i}. B[{i}] -= {outflow_demand:.2e} (Total B[{i}]={B_vector[i]:.2e})")
        elif B_vector[i] == 0.0 and not is_graph_terminal_for_flow and not node_i_data.get('is_flow_root', False): # Internal node with no explicit source/sink
            logger.debug(f"  Matrix Row {i} ({node_id_i}, type: {node_i_data.get('type')}): Internal node. B[{i}]=0.")
        
        logger.debug(f"  Matrix Row {i} ({node_id_i}, type: {node_i_data.get('type')}): Diag A[{i},{i}]={A_matrix[i,i]:.2e}. Off-diag terms: {', '.join(current_row_terms_log) if current_row_terms_log else 'None'}. B[{i}]={B_vector[i]:.2e}")


    logger.debug(f"--- Flow Solver: System Matrix A (shape {A_matrix.shape}):\n{A_matrix if num_nodes < 10 else 'Too large to print'}")
    logger.debug(f"--- Flow Solver: System Vector B (shape {B_vector.shape}):\n{B_vector if num_nodes < 10 else 'Too large to print'}")
    logger.info(f"Flow Solver: Total defined pressure BCs (is_flow_root): {num_defined_pressure_bcs}")
    logger.info(f"Flow Solver: Total defined flow sinks (terminals with Q_flow): {num_defined_flow_sinks}")

    if num_defined_pressure_bcs == 0 and num_nodes > 0:
        logger.error("Flow solver: No pressure boundary conditions (is_flow_root=True) defined. Cannot solve. Pressures and flows will be NaN.")
        return graph # Graph attributes already initialized to NaN

    node_pressures_vec = np.full(num_nodes, np.nan) # Default to NaN

    try:
        rank_A = 0
        if num_nodes > 0:
            rank_A = np.linalg.matrix_rank(A_matrix)
        logger.debug(f"Flow Solver: Matrix A Rank = {rank_A} for {num_nodes} nodes.")

        if num_nodes > 0 and rank_A < num_nodes:
            logger.error(f"Flow solver: Matrix A is singular or rank-deficient (rank {rank_A} for {num_nodes} nodes). Cannot solve. "
                         "Check for disconnected graph components not attached to a pressure BC, or insufficient/inconsistent outflow conditions. "
                         "Pressures and flows will be NaN.")
            # Graph attributes already initialized to NaN
        else:
            node_pressures_vec = np.linalg.solve(A_matrix, B_vector)
            logger.info("Flow solver: Successfully solved for nodal pressures.")
            # logger.debug(f"Solved Node Pressures Vector:\n{node_pressures_vec}")

    except np.linalg.LinAlgError as e:
        logger.error(f"Flow solver: Linear algebra error during pressure solution: {e}", exc_info=False)
        # Log condition number only if matrix is likely well-formed enough for it
        if num_nodes > 0 and rank_A == num_nodes and not np.all(A_matrix == 0):
             logger.error(f"Matrix A Condition Number: {np.linalg.cond(A_matrix)}")
        # Graph attributes already initialized to NaN

    # Assign solved pressures to graph nodes
    for i, node_id in enumerate(node_list):
        graph.nodes[node_id]['pressure'] = node_pressures_vec[i] # Will be NaN if solve failed or matrix was singular

    # Calculate flows on edges based on solved pressures
    for u_id, v_id, data in graph.edges(data=True):
        P_u = graph.nodes[u_id]['pressure']
        P_v = graph.nodes[v_id]['pressure']

        if np.isnan(P_u) or np.isnan(P_v): # If pressures couldn't be solved, flow is also unknown
            data['flow_solver'] = np.nan
            continue

        length = data.get('length', 0.0)
        radius = data.get('radius', constants.MIN_VESSEL_RADIUS_MM)
        radius = max(radius, constants.MIN_VESSEL_RADIUS_MM * 0.01)
        if length < constants.EPSILON:
            length = constants.EPSILON

        resistance = calculate_segment_resistance(length, radius, viscosity)
        flow_on_edge = 0.0
        if resistance < np.inf and resistance > constants.EPSILON:
            flow_on_edge = (P_u - P_v) / resistance
        elif resistance <= constants.EPSILON: # Near zero resistance
            # If pressures are different, flow could be very large.
            # This case implies P_u should be very close to P_v.
            if not np.isclose(P_u, P_v, atol=1e-3): # Allow small tolerance for numerical error
                 logger.warning(f"Edge {u_id}->{v_id} has R~0 ({resistance:.1e}) but P_diff {(P_u-P_v):.2e}. Flow may be very large/ill-defined.")
                 # Use a very high conductance to estimate flow, but this situation is often problematic.
                 flow_on_edge = (P_u - P_v) / (constants.EPSILON * 10)
            # else, if P_u is close to P_v, flow is near zero, which is fine.
        elif resistance == np.inf: # Infinite resistance, zero flow
            flow_on_edge = 0.0


        data['flow_solver'] = flow_on_edge
        if logger.isEnabledFor(logging.DEBUG):
             logger.debug(f"Edge {u_id}->{v_id}: P_u={P_u:.2f}, P_v={P_v:.2f}, L={length:.3f}, R_edge={radius:.4f}, Res={resistance:.2e}, Q_solver={flow_on_edge:.2e}")

    logger.info("Flow solver: Finished annotating graph with pressures and flows.")
    return graph# src/config_manager.py
import yaml
import os
from typing import Any, Dict
import logging

logger = logging.getLogger(__name__)

def load_config(config_path: str) -> Dict[str, Any]:
    """
    Loads a YAML configuration file.

    Args:
        config_path (str): Path to the YAML configuration file.

    Returns:
        Dict[str, Any]: A dictionary containing the configuration parameters.
    
    Raises:
        FileNotFoundError: If the config file is not found.
        yaml.YAMLError: If there's an error parsing the YAML file.
    """
    if not os.path.exists(config_path):
        logger.error(f"Configuration file not found: {config_path}")
        raise FileNotFoundError(f"Configuration file not found: {config_path}")
    
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        logger.info(f"Successfully loaded configuration from: {config_path}")
        return config
    except yaml.YAMLError as e:
        logger.error(f"Error parsing YAML configuration file {config_path}: {e}")
        raise

def get_param(config: Dict[str, Any], key_path: str, default: Any = None) -> Any:
    """
    Retrieves a parameter from the config dictionary using a dot-separated key path.
    Example: get_param(config, "simulation.gbo.max_iterations")

    Args:
        config (Dict[str, Any]): The configuration dictionary.
        key_path (str): Dot-separated path to the key (e.g., "parent.child.key").
        default (Any, optional): Default value to return if key is not found. Defaults to None.

    Returns:
        Any: The parameter value or the default value.
    """
    keys = key_path.split('.')
    value = config
    try:
        for key in keys:
            value = value[key]
        return value
    except (KeyError, TypeError):
        logger.warning(f"Parameter '{key_path}' not found in config. Using default: {default}")
        return default

def create_default_config(config_path: str = "config.yaml"):
    """
    Creates a default configuration file if it doesn't exist.
    """
    from src import constants # To access default values

    default_config_content = {
        "paths": {
            "output_dir": "output/simulation_results",
            "wm_nifti": "data/sample_brain_wm.nii.gz",
            "gm_nifti": "data/sample_brain_gm.nii.gz",
            "csf_nifti": "data/sample_brain_csf.nii.gz", # Optional
            "tumor_nifti": "data/sample_tumor.nii.gz", # Optional
            "arterial_centerlines": "data/sample_arteries.vtp" # or .txt
        },
        "simulation": {
            "random_seed": 42,
            "log_level": "INFO", # DEBUG, INFO, WARNING, ERROR
            "units": { # Define units used for consistency, e.g. 'mm' for length
                "length": "mm",
                "pressure": "Pa", # Pascal
                "flow_rate": "mm^3/s"
            }
        },
        "tissue_properties": {
            "metabolic_rates": { # in 1/s (ml_blood / s / ml_tissue)
                "gm": constants.Q_MET_GM_PER_ML,
                "wm": constants.Q_MET_WM_PER_ML,
                "tumor_rim": constants.Q_MET_TUMOR_RIM_PER_ML,
                "tumor_core": constants.Q_MET_TUMOR_CORE_PER_ML,
                "csf": constants.Q_MET_CSF_PER_ML
            },
            "permeability": { # in mm^2 (if length unit is mm)
                "gm": constants.DEFAULT_TISSUE_PERMEABILITY_GM,
                "wm": constants.DEFAULT_TISSUE_PERMEABILITY_WM,
                # Tumor permeability can be higher
                "tumor": 5e-7 # mm^2
            }
        },
        "vascular_properties": {
            "blood_viscosity": constants.DEFAULT_BLOOD_VISCOSITY, # Pa.s
            "murray_law_exponent": constants.MURRAY_LAW_EXPONENT,
            "initial_terminal_flow": constants.INITIAL_TERMINAL_FLOW_Q, # mm^3/s (if units are mm)
            "min_segment_length": 0.1, # mm
            "max_segment_length": 2.0, # mm
            "min_radius": 0.005 # mm (e.g. 5 microns)
        },
        "gbo_growth": {
            "max_iterations": 100,
            "energy_coefficient_C_met": constants.DEFAULT_C_MET_VESSEL_WALL, # W/m^3 or equivalent in chosen units
            "target_perfusion_level": 0.9, # Target fraction of demand to be met
            "branching_threshold_radius": 0.2, # mm (example: branch if terminal radius exceeds this)
            "bifurcation_angle_search_steps": 5, # Number of angles to test
            "bifurcation_length_factor": 0.5, # New segments are this factor of parent segment length (heuristic)
            "stop_criteria": {
                "max_radius_factor_measured": 1.0 # Stop if synth. radius = 1.0 * measured_terminal_radius
            }
        },
        "tumor_angiogenesis": {
            "enabled": True,
            "sprouting_vessel_min_radius": 0.1, # mm
            "growth_bias_strength": 0.5, # For biased random walk
            "max_tumor_vessels": 500,
            "segment_length_mean_tumor": 0.3, # mm
            "segment_length_std_tumor": 0.1, # mm
            "tortuosity_factor": 1.5 # How much more tortuous tumor vessels are
        },
        "perfusion_solver": {
            "enabled": True,
            "inlet_pressure": 10000, # Pa (approx 75 mmHg)
            "terminal_outlet_pressure": 2000, # Pa (approx 15 mmHg, for network solver if applicable)
            "coupling_beta": constants.DEFAULT_COUPLING_BETA, # mm^3 / (s*Pa)
            "use_flow_sinks_for_terminals": True, # Example, if we add options later
            "max_iterations_coupling": 20,
            "convergence_tolerance_coupling": 1e-4,
            "darcy_solver_max_iter": 1000,
            "darcy_solver_tolerance": 1e-6
        },
        "visualization":{
            "save_intermediate_steps": False,
            "plot_slice_axis": "axial", # axial, sagittal, coronal
            "plot_slice_index": -1 # -1 for middle slice
        }
    }
    if not os.path.exists(config_path):
        with open(config_path, 'w') as f:
            yaml.dump(default_config_content, f, sort_keys=False, indent=4)
        logger.info(f"Created default configuration file: {config_path}")
    else:
        logger.info(f"Configuration file already exists: {config_path}")

if __name__ == '__main__':
    # Example usage:
    logging.basicConfig(level=logging.INFO)
    
    # Create a dummy config.yaml if it doesn't exist in the script's directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    dummy_config_path = os.path.join(script_dir, "..", "config.yaml") # Assumes script is in src/
    
    if not os.path.exists(dummy_config_path):
        os.makedirs(os.path.dirname(dummy_config_path), exist_ok=True)
        create_default_config(dummy_config_path)

    try:
        config = load_config(dummy_config_path)
        print("Config loaded successfully.")
        print("Random seed:", get_param(config, "simulation.random_seed", 0))
        print("GM metabolic rate:", get_param(config, "tissue_properties.metabolic_rates.gm"))
        print("Non-existent param:", get_param(config, "foo.bar.baz", "default_val"))
        
        # Test output directory creation from config
        output_dir = get_param(config, "paths.output_dir", "output/default_sim")
        os.makedirs(output_dir, exist_ok=True)
        print(f"Ensured output directory exists: {output_dir}")

    except Exception as e:
        print(f"Error in example usage: {e}")# src/vascular_growth.py
from __future__ import annotations # Must be first line for postponed evaluation of annotations

import numpy as np
import networkx as nx
import logging
import os
from scipy.spatial import KDTree
from typing import Tuple, List, Dict, Set, Optional

from src import utils, data_structures, constants, config_manager
from src import energy_model
from src import perfusion_solver
from src import io_utils # Added missing import

logger = logging.getLogger(__name__)

class GBOIterationData:
    def __init__(self, terminal_id: str, pos: np.ndarray, radius: float, flow: float,
                 is_synthetic: bool = True, original_measured_radius: Optional[float] = None,
                 parent_id: Optional[str] = None, parent_measured_terminal_id: Optional[str] = None):
        self.id: str = terminal_id
        self.pos: np.ndarray = np.array(pos, dtype=float)
        self.radius: float = float(radius)
        self.flow: float = float(flow)
        self.parent_id: Optional[str] = parent_id
        self.parent_measured_terminal_id: Optional[str] = parent_measured_terminal_id
        self.original_measured_terminal_radius: Optional[float] = original_measured_radius
        self.length_from_parent: float = 0.0
        self.is_synthetic: bool = is_synthetic
        self.stop_growth: bool = False
        self.current_territory_voxel_indices_flat: List[int] = []
        self.current_territory_demand: float = 0.0


def initialize_perfused_territory_and_terminals(
    config: dict,
    initial_graph: Optional[nx.DiGraph], # Parsed VTP graph from main.py
    tissue_data: dict # Contains various masks and affine
) -> Tuple[np.ndarray, List[GBOIterationData], int, nx.DiGraph]:
    logger.info("Initializing perfused territory and active GBO terminals...")
    perfused_tissue_mask = np.zeros(tissue_data['shape'], dtype=bool)
    active_terminals: List[GBOIterationData] = []

    initial_synthetic_radius_default = config_manager.get_param(config, "vascular_properties.min_radius", 0.005)
    k_murray_factor = config_manager.get_param(config, "vascular_properties.k_murray_scaling_factor", 0.5)
    murray_exponent = config_manager.get_param(config, "vascular_properties.murray_law_exponent", 3.0)
    default_initial_flow = config_manager.get_param(config, "vascular_properties.initial_terminal_flow", constants.INITIAL_TERMINAL_FLOW_Q)

    next_synthetic_node_id = 0

    gbo_graph = initial_graph.copy() if initial_graph and initial_graph.number_of_nodes() > 0 else data_structures.create_empty_vascular_graph()
    if initial_graph is None or initial_graph.number_of_nodes() == 0:
        logger.info("No initial VTP graph provided or it's empty. GBO will rely on config seeds or fallback.")

    # Mark VTP roots from input graph as flow roots for the solver
    if initial_graph:
        for node_id, data in initial_graph.nodes(data=True):
            if data.get('type') == 'measured_root':
                if gbo_graph.has_node(node_id):
                    gbo_graph.nodes[node_id]['is_flow_root'] = True
                    gbo_graph.nodes[node_id]['Q_flow'] = 0.0 # Pressure BC
                    logger.info(f"Marked VTP input node {node_id} (type: measured_root) as is_flow_root=True in gbo_graph.")

    processed_from_vtp_terminals = False
    if initial_graph:
        vtp_terminals_in_anatomical_domain = []
        # This is the GM+WM mask, where GBO growth is allowed
        gbo_growth_domain_mask = tissue_data.get('gbo_growth_domain_mask') 
        affine = tissue_data.get('affine')

        if gbo_growth_domain_mask is None or affine is None:
            logger.error("GBO growth domain mask or affine missing in tissue_data. Cannot reliably sprout from VTP terminals.")
        else:
            for node_id, data in initial_graph.nodes(data=True):
                # In main.py, VTP terminals within GM/WM/CSF were typed as 'measured_terminal_in_anatomical_domain'
                if data.get('type') == 'measured_terminal_in_anatomical_domain':
                    # Now, further check if this terminal is within the GBO growth domain (GM/WM)
                    pos_world = data['pos']
                    pos_vox_int = np.round(utils.world_to_voxel(pos_world, affine)).astype(int)

                    if utils.is_voxel_in_bounds(pos_vox_int, gbo_growth_domain_mask.shape) and \
                       gbo_growth_domain_mask[tuple(pos_vox_int)]:
                        vtp_terminals_in_anatomical_domain.append(node_id)
                    else:
                        logger.info(f"VTP terminal {node_id} (in anatomical domain) is outside GBO growth domain (GM/WM). Not sprouting GBO from it.")
                        # Optionally change its type in gbo_graph if it won't be a sprouting point
                        if gbo_graph.has_node(node_id):
                            gbo_graph.nodes[node_id]['type'] = 'measured_terminal_non_parenchymal' # e.g. ends in CSF

        if vtp_terminals_in_anatomical_domain:
            logger.info(f"Found {len(vtp_terminals_in_anatomical_domain)} VTP terminals within GBO growth domain (GM/WM) to sprout from.")
            for measured_terminal_id in vtp_terminals_in_anatomical_domain:
                measured_data = initial_graph.nodes[measured_terminal_id] # Data from original VTP graph
                measured_pos = np.array(measured_data['pos'], dtype=float)
                original_measured_radius = measured_data.get('radius', initial_synthetic_radius_default) 

                gbo_sprout_id = f"s_{next_synthetic_node_id}"; next_synthetic_node_id += 1

                term_gbo_data_obj = GBOIterationData(
                    terminal_id=gbo_sprout_id, pos=measured_pos,
                    radius=initial_synthetic_radius_default, flow=default_initial_flow,
                    original_measured_radius=original_measured_radius,
                    parent_id=measured_terminal_id,
                    parent_measured_terminal_id=measured_terminal_id
                )
                active_terminals.append(term_gbo_data_obj)

                node_attrs_for_s_node = vars(term_gbo_data_obj).copy()
                node_attrs_for_s_node['type'] = 'synthetic_terminal'
                node_attrs_for_s_node['is_flow_root'] = False
                node_attrs_for_s_node['Q_flow'] = term_gbo_data_obj.flow # Initial demand for solver
                node_attrs_for_s_node.pop('current_territory_voxel_indices_flat', None)
                node_attrs_for_s_node.pop('current_territory_demand', None)
                data_structures.add_node_to_graph(gbo_graph, gbo_sprout_id, **node_attrs_for_s_node)

                if gbo_graph.has_node(measured_terminal_id):
                     gbo_graph.nodes[measured_terminal_id]['type'] = 'measured_to_synthetic_junction'
                     if 'Q_flow' in gbo_graph.nodes[measured_terminal_id]: # Remove if it was typed as sink by main.py
                         del gbo_graph.nodes[measured_terminal_id]['Q_flow'] 
                
                edge_length_vtp_to_gbo = config_manager.get_param(config, "gbo_growth.vtp_sprout_connection_length", constants.EPSILON)
                data_structures.add_edge_to_graph(
                    gbo_graph, measured_terminal_id, gbo_sprout_id, 
                    length=edge_length_vtp_to_gbo, 
                    radius=initial_synthetic_radius_default, # Edge takes radius of the new GBO sprout
                    type='synthetic_sprout_from_measured'
                )
                logger.info(f"GBO Init: Sprouted synthetic_terminal '{gbo_sprout_id}' from VTP node '{measured_terminal_id}' "
                            f"(VTP R={original_measured_radius:.4f}, Sprout R_init={initial_synthetic_radius_default:.4f}).")
                processed_from_vtp_terminals = True
        
        if not processed_from_vtp_terminals and initial_graph.number_of_nodes() > 0 :
             if any(data.get('type') == 'measured_terminal_in_anatomical_domain' for _, data in initial_graph.nodes(data=True)):
                logger.warning("VTP terminals were found in anatomical domain, but none were within GBO growth domain (GM/WM). Checking config seeds.")
             else: # No 'measured_terminal_in_anatomical_domain' types were found at all
                logger.warning("Initial graph (VTP) provided but no 'measured_terminal_in_anatomical_domain' nodes found/processed by GBO init. Checking config seeds.")


    if not processed_from_vtp_terminals:
        seed_points_config = config_manager.get_param(config, "gbo_growth.seed_points", [])
        if seed_points_config and isinstance(seed_points_config, list):
            logger.info(f"No VTP terminals processed for GBO, or none valid. Using {len(seed_points_config)} GBO seed points from configuration.")
            for seed_info in seed_points_config:
                seed_id_base = seed_info.get('id', f"cfg_seed_{next_synthetic_node_id}")
                next_synthetic_node_id +=1

                seed_pos = np.array(seed_info.get('position'), dtype=float)
                seed_initial_radius = float(seed_info.get('initial_radius', initial_synthetic_radius_default))
                # Flow for config seeds is for radius calc, actual Q_flow on node for solver is 0 if is_flow_root
                seed_flow_for_radius = (seed_initial_radius / k_murray_factor) ** murray_exponent \
                                    if seed_initial_radius > constants.EPSILON and k_murray_factor > constants.EPSILON \
                                    else default_initial_flow
                term_gbo_data_obj = GBOIterationData(
                    terminal_id=seed_id_base, pos=seed_pos, radius=seed_initial_radius, flow=seed_flow_for_radius
                )
                active_terminals.append(term_gbo_data_obj)

                node_attrs_seed = vars(term_gbo_data_obj).copy()
                node_attrs_seed['is_flow_root'] = True 
                node_attrs_seed['type'] = 'synthetic_root_terminal'
                node_attrs_seed['Q_flow'] = 0.0 # Pressure BC for solver
                node_attrs_seed.pop('current_territory_voxel_indices_flat', None)
                node_attrs_seed.pop('current_territory_demand', None)
                data_structures.add_node_to_graph(gbo_graph, term_gbo_data_obj.id, **node_attrs_seed)
                logger.info(f"Initialized GBO seed terminal from config: {term_gbo_data_obj.id} at {np.round(seed_pos,2)} "
                            f"with R={seed_initial_radius:.4f}. Marked as is_flow_root.")
        elif not processed_from_vtp_terminals: # Only log if no VTP terminals AND no config seeds
             logger.info("No VTP terminals processed and no GBO seed points found in configuration. Checking fallback.")

    if not active_terminals: # Fallback seed
        logger.warning("No GBO starting points from VTP or config seeds. Attempting one fallback seed.")
        # Ensure 'domain_mask' for fallback is gbo_growth_domain_mask (GM/WM)
        fallback_domain_mask = tissue_data.get('gbo_growth_domain_mask', tissue_data.get('domain_mask'))
        if fallback_domain_mask is not None and np.any(fallback_domain_mask):
            seed_point_world = utils.get_random_point_in_mask(fallback_domain_mask, tissue_data['affine'])
            if seed_point_world is not None:
                fallback_id = f"s_fallback_{next_synthetic_node_id}"; next_synthetic_node_id +=1
                term_gbo_data_obj = GBOIterationData(fallback_id, seed_point_world, initial_synthetic_radius_default, default_initial_flow)
                active_terminals.append(term_gbo_data_obj)
                node_attrs_fallback = vars(term_gbo_data_obj).copy()
                node_attrs_fallback['is_flow_root'] = True; node_attrs_fallback['type'] = 'synthetic_root_terminal'
                node_attrs_fallback['Q_flow'] = 0.0 # Pressure BC
                node_attrs_fallback.pop('current_territory_voxel_indices_flat', None); node_attrs_fallback.pop('current_territory_demand', None)
                data_structures.add_node_to_graph(gbo_graph, fallback_id, **node_attrs_fallback)
                logger.info(f"Initialized fallback GBO seed terminal {fallback_id} at {np.round(seed_point_world,2)}.")
            else: logger.error("Cannot find a valid random seed point within GBO growth domain for fallback.")
        else: logger.error("No GBO growth domain_mask available for fallback seeding.")

    if not active_terminals:
        logger.error("CRITICAL: No GBO terminals to initialize growth. Aborting GBO.")
        return perfused_tissue_mask, [], next_synthetic_node_id, gbo_graph

    # Initial Territory Assignment uses 'world_coords_flat' derived from 'gbo_growth_domain_mask'
    if tissue_data.get('world_coords_flat') is None or tissue_data['world_coords_flat'].size == 0:
        logger.error("tissue_data['world_coords_flat'] (from GBO growth domain) is empty. Cannot initialize GBO territories.")
        for term_data in active_terminals: term_data.stop_growth = True
        return perfused_tissue_mask, active_terminals, next_synthetic_node_id, gbo_graph
        
    kdtree_gbo_domain_voxels = KDTree(tissue_data['world_coords_flat']) # KDTree of GM/WM voxels
    initial_territory_radius_search = config_manager.get_param(config, "gbo_growth.initial_territory_radius", 0.2) 

    for term_gbo_obj in active_terminals: # These are GBOIterationData objects (s_XXX)
        # Query against GM/WM kdtree
        nearby_flat_indices_in_gbo_domain = kdtree_gbo_domain_voxels.query_ball_point(term_gbo_obj.pos, r=initial_territory_radius_search)
        actual_demand_init = 0.0
        voxels_for_term_init_flat: List[int] = [] # These will be indices into tissue_data['world_coords_flat']

        if nearby_flat_indices_in_gbo_domain: # Check if any nearby voxels were found
            for local_kdtree_idx in nearby_flat_indices_in_gbo_domain:
                # The index from query_ball_point is an index into the points used to build kdtree_gbo_domain_voxels
                # which are tissue_data['world_coords_flat']. So, local_kdtree_idx IS the global_flat_idx.
                global_flat_idx = local_kdtree_idx
                
                v_3d_idx_tuple = tuple(tissue_data['voxel_indices_flat'][global_flat_idx]) # Get 3D index from global flat index
                
                if not perfused_tissue_mask[v_3d_idx_tuple]: 
                    perfused_tissue_mask[v_3d_idx_tuple] = True
                    actual_demand_init += tissue_data['metabolic_demand_map'][v_3d_idx_tuple] 
                    voxels_for_term_init_flat.append(global_flat_idx)
        
        term_gbo_obj.current_territory_voxel_indices_flat = voxels_for_term_init_flat
        term_gbo_obj.current_territory_demand = actual_demand_init
        
        if actual_demand_init > constants.EPSILON:
            term_gbo_obj.flow = actual_demand_init
            new_r = k_murray_factor * (term_gbo_obj.flow ** (1.0 / murray_exponent))
            term_gbo_obj.radius = max(initial_synthetic_radius_default, new_r) 
        else: 
            term_gbo_obj.flow = default_initial_flow 
            term_gbo_obj.radius = initial_synthetic_radius_default

        if gbo_graph.has_node(term_gbo_obj.id):
            gbo_node_data = gbo_graph.nodes[term_gbo_obj.id]
            if gbo_node_data.get('is_flow_root'): gbo_node_data['Q_flow'] = 0.0
            else: gbo_node_data['Q_flow'] = term_gbo_obj.flow
            gbo_node_data['radius'] = term_gbo_obj.radius
            
            if term_gbo_obj.current_territory_voxel_indices_flat:
                valid_indices_centroid = [idx for idx in term_gbo_obj.current_territory_voxel_indices_flat 
                                          if idx < tissue_data['world_coords_flat'].shape[0]]
                if valid_indices_centroid:
                    initial_coords = tissue_data['world_coords_flat'][valid_indices_centroid]
                    if initial_coords.shape[0] > 0:
                        new_pos_centroid = np.mean(initial_coords, axis=0)
                        if utils.distance_squared(term_gbo_obj.pos, new_pos_centroid) > constants.EPSILON**2 : # Only move if significant
                            old_pos_log = term_gbo_obj.pos.copy()
                            term_gbo_obj.pos = new_pos_centroid 
                            gbo_node_data['pos'] = new_pos_centroid 
                            logger.debug(f"GBO Terminal {term_gbo_obj.id} (Ω_init): Moved from {np.round(old_pos_log,3)} to centroid {np.round(new_pos_centroid,3)}.")
                            if term_gbo_obj.parent_id and gbo_graph.has_edge(term_gbo_obj.parent_id, term_gbo_obj.id):
                                parent_pos = gbo_graph.nodes[term_gbo_obj.parent_id]['pos']
                                new_len = utils.distance(parent_pos, term_gbo_obj.pos)
                                gbo_graph.edges[term_gbo_obj.parent_id, term_gbo_obj.id]['length'] = new_len
                                term_gbo_obj.length_from_parent = new_len
        else:
            logger.error(f"GBO terminal {term_gbo_obj.id} from GBOIterationData not found in gbo_graph during territory init.")

        logger.debug(f"GBO Terminal {term_gbo_obj.id} (Ω_init final): Pos={np.round(term_gbo_obj.pos,3)}, Claimed {len(voxels_for_term_init_flat)} voxels, "
                     f"Demand={term_gbo_obj.current_territory_demand:.2e}, Target Flow={term_gbo_obj.flow:.2e}, Radius={term_gbo_obj.radius:.4f}")

    logger.info(f"GBO Initialization complete. Perfused {np.sum(perfused_tissue_mask)} initial voxels within GBO domain. "
                f"{len(active_terminals)} active GBO terminals.")
    return perfused_tissue_mask, active_terminals, next_synthetic_node_id, gbo_graph


def find_growth_frontier_voxels(
    terminal_gbo_data: GBOIterationData,
    kdtree_unperfused_domain_voxels: Optional[KDTree],
    unperfused_global_flat_indices: np.ndarray,
    tissue_data: dict,
    config: dict
) -> np.ndarray:
    logger.debug(f"Terminal {terminal_gbo_data.id}: Entering find_growth_frontier_voxels. "
                 f"Pos: {np.round(terminal_gbo_data.pos,3)}, Radius: {terminal_gbo_data.radius:.4f}")

    if kdtree_unperfused_domain_voxels is None or kdtree_unperfused_domain_voxels.n == 0:
        logger.debug(f"Terminal {terminal_gbo_data.id}: KDTree of unperfused voxels is empty or None. No frontier.")
        return np.array([], dtype=int)

    radius_factor = config_manager.get_param(config, "gbo_growth.frontier_search_radius_factor", 3.0)
    fixed_radius = config_manager.get_param(config, "gbo_growth.frontier_search_radius_fixed", 0.25)
    voxel_dim = tissue_data['voxel_volume']**(1/3.0)
    search_r = max(radius_factor * terminal_gbo_data.radius, fixed_radius, voxel_dim * 1.5)

    logger.debug(f"Terminal {terminal_gbo_data.id}: Searching for frontier with effective radius {search_r:.3f}mm. "
                 f"(voxel_dim ~{voxel_dim:.3f}, term_radius_component ~{radius_factor * terminal_gbo_data.radius:.3f})")

    try:
        local_indices_in_kdtree = kdtree_unperfused_domain_voxels.query_ball_point(terminal_gbo_data.pos, r=search_r)
    except Exception as e:
        logger.error(f"Terminal {terminal_gbo_data.id}: KDTree query_ball_point failed: {e}", exc_info=True)
        return np.array([], dtype=int)

    logger.debug(f"Terminal {terminal_gbo_data.id}: KDTree query_ball_point found {len(local_indices_in_kdtree)} local indices "
                 f"within {search_r:.3f}mm.")

    if not local_indices_in_kdtree: return np.array([], dtype=int)
    if unperfused_global_flat_indices.shape[0] == 0:
        logger.warning(f"Terminal {terminal_gbo_data.id}: unperfused_global_flat_indices is empty.")
        return np.array([], dtype=int)

    valid_kdtree_indices = [idx for idx in local_indices_in_kdtree if idx < len(unperfused_global_flat_indices)]
    if len(valid_kdtree_indices) != len(local_indices_in_kdtree):
        logger.warning(f"Terminal {terminal_gbo_data.id}: Some KDTree indices were out of bounds for unperfused_global_flat_indices. "
                       f"Original: {len(local_indices_in_kdtree)}, Valid: {len(valid_kdtree_indices)}")
    if not valid_kdtree_indices: return np.array([], dtype=int)

    frontier_voxels_global_flat_indices_initial = unperfused_global_flat_indices[valid_kdtree_indices]
    logger.debug(f"Terminal {terminal_gbo_data.id}: Initial frontier (after mapping) "
                 f"contains {len(frontier_voxels_global_flat_indices_initial)} voxels.")

    max_voxels_in_Rip = config_manager.get_param(config, "gbo_growth.max_voxels_for_Rip", 50)
    final_frontier_voxels_global_flat_indices = frontier_voxels_global_flat_indices_initial

    if len(frontier_voxels_global_flat_indices_initial) > max_voxels_in_Rip:
        logger.debug(f"Terminal {terminal_gbo_data.id}: Initial frontier ({len(frontier_voxels_global_flat_indices_initial)}) "
                     f"> max_voxels_for_Rip ({max_voxels_in_Rip}). Selecting closest.")
        try:
            k_val = min(max_voxels_in_Rip, kdtree_unperfused_domain_voxels.n)
            if k_val > 0 :
                _, local_indices_k_closest = kdtree_unperfused_domain_voxels.query(terminal_gbo_data.pos, k=k_val)
                if isinstance(local_indices_k_closest, (int, np.integer)):
                    local_indices_k_closest = np.array([local_indices_k_closest])

                if len(local_indices_k_closest) > 0:
                    valid_k_closest_indices = [idx for idx in local_indices_k_closest if idx < len(unperfused_global_flat_indices)]
                    if len(valid_k_closest_indices) != len(local_indices_k_closest):
                         logger.warning(f"Terminal {terminal_gbo_data.id}: Some k-closest KDTree indices out of bounds.")
                    if valid_k_closest_indices:
                        final_frontier_voxels_global_flat_indices = unperfused_global_flat_indices[valid_k_closest_indices]
                    else: final_frontier_voxels_global_flat_indices = np.array([], dtype=int)
                else: final_frontier_voxels_global_flat_indices = np.array([], dtype=int)
            else: final_frontier_voxels_global_flat_indices = np.array([], dtype=int)
        except Exception as e_kquery:
            logger.error(f"Terminal {terminal_gbo_data.id}: KDTree k-closest query failed: {e_kquery}. Using initial ball query if valid.", exc_info=True)
            if np.any(np.array(valid_kdtree_indices) >= len(unperfused_global_flat_indices)):
                 final_frontier_voxels_global_flat_indices = np.array([], dtype=int)
        logger.debug(f"Terminal {terminal_gbo_data.id}: Frontier limited to {len(final_frontier_voxels_global_flat_indices)} voxels.")

    logger.info(f"Terminal {terminal_gbo_data.id} identified {len(final_frontier_voxels_global_flat_indices)} "
                 f"final frontier voxels (Ri,p).")
    return final_frontier_voxels_global_flat_indices


# In src/vascular_growth.py

import numpy as np
import networkx as nx
import logging
import os
from scipy.spatial import KDTree
from typing import Tuple, List, Dict, Optional 

from src import utils, data_structures, constants, config_manager
from src import energy_model
from src import perfusion_solver
from src import io_utils # For saving intermediate VTPs/NIfTIs

# Assuming GBOIterationData class and initialize_perfused_territory_and_terminals,
# find_growth_frontier_voxels are defined above this function in the same file or imported.

# If GBOIterationData is not defined elsewhere in this file, it should be:
# class GBOIterationData:
#     def __init__(self, terminal_id: str, pos: np.ndarray, radius: float, flow: float,
#                  is_synthetic: bool = True, original_measured_radius: Optional[float] = None,
#                  parent_id: Optional[str] = None, parent_measured_terminal_id: Optional[str] = None):
#         self.id: str = terminal_id
#         self.pos: np.ndarray = np.array(pos, dtype=float)
#         self.radius: float = float(radius)
#         self.flow: float = float(flow)
#         self.parent_id: Optional[str] = parent_id
#         self.parent_measured_terminal_id: Optional[str] = parent_measured_terminal_id
#         self.original_measured_terminal_radius: Optional[float] = original_measured_radius
#         self.length_from_parent: float = 0.0
#         self.is_synthetic: bool = is_synthetic
#         self.stop_growth: bool = False
#         self.current_territory_voxel_indices_flat: List[int] = []
#         self.current_territory_demand: float = 0.0

# Assume initialize_perfused_territory_and_terminals and find_growth_frontier_voxels
# are correctly defined in this file (as provided in previous responses).

logger = logging.getLogger(__name__)


def grow_healthy_vasculature(config: dict,
                             tissue_data: dict,
                             initial_graph: Optional[nx.DiGraph],
                             output_dir: str) -> Optional[nx.DiGraph]:
    logger.info("Starting GBO healthy vascular growth (Tissue-Led Model v2 with Flow Solver Integration)...")

    perfused_tissue_mask, current_active_terminals, next_node_id, gbo_graph = \
        initialize_perfused_territory_and_terminals(config, initial_graph, tissue_data)

    if not current_active_terminals:
        logger.error("GBO Aborted: No active GBO terminals after initialization.")
        return gbo_graph

    # --- Load parameters used throughout the main loop ---
    max_iterations = config_manager.get_param(config, "gbo_growth.max_iterations", 100)
    min_radius = config_manager.get_param(config, "vascular_properties.min_radius", constants.MIN_VESSEL_RADIUS_MM)
    k_murray = config_manager.get_param(config, "vascular_properties.k_murray_scaling_factor", 0.5)
    murray_exp = config_manager.get_param(config, "vascular_properties.murray_law_exponent", 3.0)
    branch_radius_factor_thresh = config_manager.get_param(config, "gbo_growth.branch_radius_increase_threshold", 1.1)
    max_flow_single_term = config_manager.get_param(config, "gbo_growth.max_flow_single_terminal", 0.005)
    min_iters_no_growth_stop = config_manager.get_param(config, "gbo_growth.min_iterations_before_no_growth_stop", 10)
    min_demand_rip_bif_factor = config_manager.get_param(config, "gbo_growth.min_frontier_demand_factor_for_bifurcation", 0.3)
    default_initial_flow = config_manager.get_param(config, "vascular_properties.initial_terminal_flow", constants.INITIAL_TERMINAL_FLOW_Q)
    flow_solver_interval = config_manager.get_param(config, "gbo_growth.flow_solver_interval", 1)
    max_segment_length_gbo = config_manager.get_param(config, "vascular_properties.max_segment_length", 2.0)


    total_voxels_in_domain = 0
    # Use healthy domain_mask for GBO total voxels count
    if tissue_data.get('domain_mask') is not None and np.any(tissue_data['domain_mask']):
        total_voxels_in_domain = np.sum(tissue_data['domain_mask'])
    
    if total_voxels_in_domain == 0:
        logger.warning("Healthy GBO: Domain mask for healthy tissue is empty. Growth might be limited or rely on fallback.")

    map_3d_to_flat_idx = -np.ones(tissue_data['shape'], dtype=np.int64)
    if tissue_data.get('voxel_indices_flat') is not None and tissue_data['voxel_indices_flat'].size > 0:
        valid_indices = tissue_data['voxel_indices_flat']
        valid_mask = (valid_indices[:,0] < tissue_data['shape'][0]) & \
                     (valid_indices[:,1] < tissue_data['shape'][1]) & \
                     (valid_indices[:,2] < tissue_data['shape'][2]) & \
                     (valid_indices[:,0] >= 0) & \
                     (valid_indices[:,1] >= 0) & \
                     (valid_indices[:,2] >= 0)
        valid_indices = valid_indices[valid_mask]
        if valid_indices.size > 0: # Check after filtering
            map_3d_to_flat_idx[valid_indices[:,0], valid_indices[:,1], valid_indices[:,2]] = np.arange(valid_indices.shape[0])

    # --- Main Iteration Loop ---
    for iteration in range(max_iterations):
        logger.info(f"--- GBO Iteration {iteration + 1} / {max_iterations} ---")

        terminals_for_growth_attempt = [t for t in current_active_terminals if not t.stop_growth]
        if not terminals_for_growth_attempt:
            logger.info("GBO: No active GBO terminals for growth at start of iteration.")
            break

        current_perfused_count = np.sum(perfused_tissue_mask)
        if total_voxels_in_domain > 0:
            perf_percentage = (current_perfused_count / total_voxels_in_domain) * 100
            logger.info(f"Active GBO terminals: {len(terminals_for_growth_attempt)}. Perfused voxels: {current_perfused_count}/{total_voxels_in_domain} ({perf_percentage:.1f}%)")
        else:
            logger.info(f"Active GBO terminals: {len(terminals_for_growth_attempt)}. Perfused voxels: {current_perfused_count} (Total domain voxels is 0).")

        # --- Build KDTree of Unperfused Voxels (within healthy GBO domain) ---
        unperfused_mask_3d = tissue_data.get('domain_mask', np.zeros(tissue_data['shape'], dtype=bool)) & (~perfused_tissue_mask)
        unperfused_voxels_3d_indices = np.array(np.where(unperfused_mask_3d)).T
        kdtree_unperfused: Optional[KDTree] = None
        # unperfused_kdtree_global_flat_indices maps indices from kdtree_unperfused back to global flat indices
        unperfused_kdtree_global_flat_indices: np.ndarray = np.array([], dtype=int)

        if unperfused_voxels_3d_indices.shape[0] > 0:
            unperfused_voxels_world_coords_for_kdt_build = utils.voxel_to_world(unperfused_voxels_3d_indices, tissue_data['affine'])
            
            # Get the global flat indices corresponding to these unperfused 3D voxel indices
            # These are indices into tissue_data['voxel_indices_flat'] / tissue_data['world_coords_flat']
            temp_flat_indices = map_3d_to_flat_idx[unperfused_voxels_3d_indices[:,0],
                                                   unperfused_voxels_3d_indices[:,1],
                                                   unperfused_voxels_3d_indices[:,2]]
            valid_for_kdtree_mask = (temp_flat_indices != -1)
            
            if np.any(valid_for_kdtree_mask):
                unperfused_kdtree_global_flat_indices = temp_flat_indices[valid_for_kdtree_mask]
                unperfused_voxels_world_coords_for_kdt_build = unperfused_voxels_world_coords_for_kdt_build[valid_for_kdtree_mask]
                if unperfused_voxels_world_coords_for_kdt_build.shape[0] > 0:
                    kdtree_unperfused = KDTree(unperfused_voxels_world_coords_for_kdt_build)
            else:
                logger.debug("No unperfused voxels mapped to valid flat indices for KDTree.")
        
        if kdtree_unperfused is None or kdtree_unperfused.n == 0:
            logger.info("GBO: No unperfused domain voxels for KDTree this iteration.")

        next_iter_terminals_manager: List[GBOIterationData] = []
        newly_perfused_in_iter_mask = np.zeros_like(perfused_tissue_mask)

        # --- Growth/Branching/Extension Phase for each GBO terminal ---
        for term_p_gbo_data in terminals_for_growth_attempt:
            logger.debug(f"Processing GBO terminal {term_p_gbo_data.id}. Pos: {np.round(term_p_gbo_data.pos,3)}, "
                         f"R: {term_p_gbo_data.radius:.4f}, Current Q_target: {term_p_gbo_data.flow:.2e}")

            # find_growth_frontier_voxels returns indices LOCAL to the kdtree_unperfused
            local_indices_in_kdt = find_growth_frontier_voxels(
                term_p_gbo_data, kdtree_unperfused, 
                np.arange(kdtree_unperfused.n if kdtree_unperfused else 0), # Pass local indices 0..N-1
                tissue_data, config
            )

            if kdtree_unperfused is None or kdtree_unperfused.n == 0 or local_indices_in_kdt.size == 0:
                logger.debug(f"Terminal {term_p_gbo_data.id} found no frontier voxels.")
                next_iter_terminals_manager.append(term_p_gbo_data)
                continue
            
            # Map these local KDTree indices back to the global flat indices
            current_frontier_global_flat_indices = unperfused_kdtree_global_flat_indices[local_indices_in_kdt]
            unique_frontier_global_flat_indices = np.unique(current_frontier_global_flat_indices) # Ensure unique

            if unique_frontier_global_flat_indices.size == 0:
                next_iter_terminals_manager.append(term_p_gbo_data)
                continue

            demand_map_3d_indices_frontier = tissue_data['voxel_indices_flat'][unique_frontier_global_flat_indices]
            demand_of_frontier_voxels = tissue_data['metabolic_demand_map'][
                demand_map_3d_indices_frontier[:,0], demand_map_3d_indices_frontier[:,1],
                demand_map_3d_indices_frontier[:,2]] # Demand map already has * dV
            demand_Rip = np.sum(demand_of_frontier_voxels)

            if demand_Rip < constants.EPSILON:
                next_iter_terminals_manager.append(term_p_gbo_data)
                continue
            
            potential_total_flow_if_extended = term_p_gbo_data.flow + demand_Rip
            potential_radius_if_extended = k_murray * (potential_total_flow_if_extended ** (1.0 / murray_exp))
            attempt_branching = False
            if term_p_gbo_data.radius > constants.EPSILON and \
               potential_radius_if_extended > term_p_gbo_data.radius * branch_radius_factor_thresh : attempt_branching = True
            if potential_total_flow_if_extended > max_flow_single_term: attempt_branching = True
            if demand_Rip > term_p_gbo_data.flow * min_demand_rip_bif_factor and term_p_gbo_data.flow > constants.EPSILON: attempt_branching = True

            if attempt_branching and unique_frontier_global_flat_indices.size > 0:
                logger.debug(f"Terminal {term_p_gbo_data.id} evaluating branching. New frontier demand: {demand_Rip:.2e}.")
                old_territory_indices_flat = np.array(term_p_gbo_data.current_territory_voxel_indices_flat, dtype=int)
                
                combined_territory_indices_flat = np.unique(np.concatenate((old_territory_indices_flat, unique_frontier_global_flat_indices))) \
                                             if old_territory_indices_flat.size > 0 else unique_frontier_global_flat_indices

                if len(combined_territory_indices_flat) < 2:
                    logger.debug(f"Combined territory for {term_p_gbo_data.id} too small. Attempting extension.")
                    attempt_branching = False
                else:
                    bifurcation_result = energy_model.find_optimal_bifurcation_for_combined_territory(
                        term_p_gbo_data, combined_territory_indices_flat, tissue_data, config, k_murray, murray_exp
                    )
                    if bifurcation_result:
                        c1_pos, c1_rad, c1_total_flow, c2_pos, c2_rad, c2_total_flow, _ = bifurcation_result
                        new_parent_total_flow = c1_total_flow + c2_total_flow
                        new_parent_radius = max(min_radius, k_murray * (new_parent_total_flow ** (1.0 / murray_exp)))
                        
                        parent_node_gbo_graph_data = gbo_graph.nodes[term_p_gbo_data.id]
                        parent_is_flow_root = parent_node_gbo_graph_data.get('is_flow_root', False)
                        parent_node_gbo_graph_data.update(
                            type='synthetic_bifurcation', radius=new_parent_radius,
                            Q_flow=new_parent_total_flow if not parent_is_flow_root else 0.0,
                            is_flow_root=parent_is_flow_root
                        )
                        logger.debug(f"Bifurcating node {term_p_gbo_data.id}: is_flow_root remains {parent_is_flow_root}")

                        for _, (child_pos, child_rad, child_flow) in enumerate([(c1_pos, c1_rad, c1_total_flow), (c2_pos, c2_rad, c2_total_flow)]):
                            child_id = f"s_{next_node_id}"; next_node_id += 1
                            child_gbo_obj = GBOIterationData(
                                child_id, child_pos, child_rad, child_flow,
                                original_measured_radius=term_p_gbo_data.original_measured_terminal_radius,
                                parent_id=term_p_gbo_data.id, parent_measured_terminal_id=term_p_gbo_data.parent_measured_terminal_id
                            )
                            child_gbo_obj.length_from_parent = utils.distance(parent_node_gbo_graph_data['pos'], child_pos)
                            next_iter_terminals_manager.append(child_gbo_obj)

                            child_attrs = vars(child_gbo_obj).copy()
                            child_attrs['type'] = 'synthetic_terminal'; child_attrs['is_flow_root'] = False
                            child_attrs['Q_flow'] = child_gbo_obj.flow
                            child_attrs.pop('current_territory_voxel_indices_flat', None); child_attrs.pop('current_territory_demand', None)
                            data_structures.add_node_to_graph(gbo_graph, child_id, **child_attrs)
                            data_structures.add_edge_to_graph(gbo_graph, term_p_gbo_data.id, child_id,
                                                              length=child_gbo_obj.length_from_parent, radius=new_parent_radius,
                                                              type='synthetic_segment')
                        
                        for v_idx_flat in unique_frontier_global_flat_indices:
                            v_3d_idx = tuple(tissue_data['voxel_indices_flat'][v_idx_flat])
                            newly_perfused_in_iter_mask[v_3d_idx] = True
                        # Parent GBOIterationData is done, children are in next_iter_terminals_manager
                        term_p_gbo_data.stop_growth = True # Mark parent GBO object as stopped
                        logger.info(f"Terminal {term_p_gbo_data.id} branched. Children target flows: {c1_total_flow:.2e}, {c2_total_flow:.2e}.")
                    else: attempt_branching = False 
            
            if not attempt_branching: # Extend
                old_pos_ext = term_p_gbo_data.pos.copy()
                logger.debug(f"Terminal {term_p_gbo_data.id} extending for Ri,p (demand {demand_Rip:.2e}).")
                term_p_gbo_data.flow += demand_Rip 
                term_p_gbo_data.radius = max(min_radius, k_murray * (term_p_gbo_data.flow ** (1.0 / murray_exp)))
                
                if unique_frontier_global_flat_indices.size > 0:
                    current_territory_coords_list = []
                    if term_p_gbo_data.current_territory_voxel_indices_flat:
                         valid_curr_idx_ext = [idx for idx in term_p_gbo_data.current_territory_voxel_indices_flat if idx < tissue_data['world_coords_flat'].shape[0]]
                         if valid_curr_idx_ext: current_territory_coords_list.append(tissue_data['world_coords_flat'][valid_curr_idx_ext])
                    
                    newly_acquired_coords = tissue_data['world_coords_flat'][unique_frontier_global_flat_indices]
                    current_territory_coords_list.append(newly_acquired_coords)
                    
                    all_supplied_coords = np.vstack(current_territory_coords_list)
                    if all_supplied_coords.shape[0] > 0:
                        new_target_pos = np.mean(all_supplied_coords, axis=0)
                        extension_vector = new_target_pos - old_pos_ext
                        extension_length = np.linalg.norm(extension_vector)

                        if extension_length > constants.EPSILON:
                            move_dist = min(extension_length, max_segment_length_gbo)
                            term_p_gbo_data.pos = old_pos_ext + extension_vector * (move_dist / extension_length)
                            if term_p_gbo_data.parent_id and gbo_graph.has_edge(term_p_gbo_data.parent_id, term_p_gbo_data.id):
                                parent_pos = gbo_graph.nodes[term_p_gbo_data.parent_id]['pos']
                                new_len = utils.distance(parent_pos, term_p_gbo_data.pos)
                                gbo_graph.edges[term_p_gbo_data.parent_id, term_p_gbo_data.id]['length'] = new_len
                                if gbo_graph.has_node(term_p_gbo_data.parent_id):
                                    gbo_graph.edges[term_p_gbo_data.parent_id, term_p_gbo_data.id]['radius'] = gbo_graph.nodes[term_p_gbo_data.parent_id]['radius']
                                term_p_gbo_data.length_from_parent = new_len
                
                if gbo_graph.has_node(term_p_gbo_data.id):
                    gbo_graph.nodes[term_p_gbo_data.id].update(pos=term_p_gbo_data.pos, 
                                                               Q_flow=term_p_gbo_data.flow if not gbo_graph.nodes[term_p_gbo_data.id].get('is_flow_root') else 0.0, 
                                                               radius=term_p_gbo_data.radius)
                
                for v_idx_flat in unique_frontier_global_flat_indices:
                    v_3d_idx = tuple(tissue_data['voxel_indices_flat'][v_idx_flat])
                    newly_perfused_in_iter_mask[v_3d_idx] = True
                term_p_gbo_data.current_territory_voxel_indices_flat.extend(list(unique_frontier_global_flat_indices))
                next_iter_terminals_manager.append(term_p_gbo_data)
        # --- End of loop for terminals_for_growth_attempt ---

        current_active_terminals = next_iter_terminals_manager # Update active terminals list
        perfused_tissue_mask = perfused_tissue_mask | newly_perfused_in_iter_mask
        num_newly_perfused_this_iter = np.sum(newly_perfused_in_iter_mask)
        logger.info(f"Perfused {num_newly_perfused_this_iter} new voxels in this GBO iteration's growth/branching phase.")
        
        # --- Global Adaptation Phase ---
        if current_active_terminals and np.any(perfused_tissue_mask):
            live_terminals_for_adaptation = [t for t in current_active_terminals if not t.stop_growth]
            if live_terminals_for_adaptation:
                # 1. Voronoi Refinement
                perfused_3d_indices_vor = np.array(np.where(perfused_tissue_mask)).T
                if perfused_3d_indices_vor.shape[0] > 0:
                    perfused_global_flat_indices_vor = map_3d_to_flat_idx[perfused_3d_indices_vor[:,0], perfused_3d_indices_vor[:,1], perfused_3d_indices_vor[:,2]]
                    valid_flat_mask_for_perf_vor = perfused_global_flat_indices_vor != -1
                    perfused_global_flat_indices_vor = perfused_global_flat_indices_vor[valid_flat_mask_for_perf_vor]
                    
                    if perfused_global_flat_indices_vor.size > 0 :
                        perfused_world_coords_for_voronoi = tissue_data['world_coords_flat'][perfused_global_flat_indices_vor]
                        term_positions_vor = np.array([t.pos for t in live_terminals_for_adaptation])
                        term_flows_capacity_vor = np.array([t.radius**murray_exp if t.radius > constants.EPSILON else default_initial_flow for t in live_terminals_for_adaptation]) # Weight by capacity (r^gamma)
                        
                        assigned_local_term_indices = np.full(perfused_world_coords_for_voronoi.shape[0], -1, dtype=int)
                        for i_pvox, p_vox_wc in enumerate(perfused_world_coords_for_voronoi):
                            distances_sq = np.sum((term_positions_vor - p_vox_wc)**2, axis=1)
                            weighted_distances = distances_sq / (term_flows_capacity_vor + constants.EPSILON) 
                            assigned_local_term_indices[i_pvox] = np.argmin(weighted_distances)
                        
                        for t_data_vor in live_terminals_for_adaptation: t_data_vor.current_territory_voxel_indices_flat, t_data_vor.current_territory_demand = [], 0.0
                        for i_pvox, local_term_idx in enumerate(assigned_local_term_indices):
                            if local_term_idx != -1 and local_term_idx < len(live_terminals_for_adaptation):
                                term_obj_vor = live_terminals_for_adaptation[local_term_idx]
                                global_flat_v_idx_vor = perfused_global_flat_indices_vor[i_pvox]
                                term_obj_vor.current_territory_voxel_indices_flat.append(global_flat_v_idx_vor)
                                v_3d_idx_for_demand_vor = tuple(tissue_data['voxel_indices_flat'][global_flat_v_idx_vor])
                                term_obj_vor.current_territory_demand += tissue_data['metabolic_demand_map'][v_3d_idx_for_demand_vor]
                        
                        for t_data_vor in live_terminals_for_adaptation: 
                            t_data_vor.flow = t_data_vor.current_territory_demand if t_data_vor.current_territory_demand > constants.EPSILON else default_initial_flow
                            if not ((iteration + 1) % flow_solver_interval == 0 or iteration == max_iterations - 1):
                                new_r_vor = k_murray * (t_data_vor.flow ** (1.0 / murray_exp))
                                t_data_vor.radius = max(min_radius, new_r_vor)
                                if gbo_graph.has_node(t_data_vor.id): 
                                    node_to_update = gbo_graph.nodes[t_data_vor.id]
                                    node_to_update['radius'] = t_data_vor.radius
                                    if not node_to_update.get('is_flow_root'):
                                        node_to_update['Q_flow'] = t_data_vor.flow
                            logger.debug(f"Term {t_data_vor.id} (Voronoi Refined): Target Q_demand={t_data_vor.flow:.2e}, Current R_after_voronoi_adapt={t_data_vor.radius:.4f}")
                    logger.info("Completed Voronoi refinement for active GBO terminals.")

                # 2. Solve Network Flow & Global Radius Adaptation
                run_flow_solver_this_iteration = ((iteration + 1) % flow_solver_interval == 0) or \
                                                 (iteration == max_iterations - 1 and iteration > 0)

                if run_flow_solver_this_iteration and gbo_graph.number_of_nodes() > 0 :
                    logger.info(f"Running 1D network flow solver for GBO iteration {iteration + 1}...")
                    for term_obj_flow_set in live_terminals_for_adaptation: # Update demands for sinks
                        if gbo_graph.has_node(term_obj_flow_set.id):
                            node_data_fs = gbo_graph.nodes[term_obj_flow_set.id]
                            if not node_data_fs.get('is_flow_root', False):
                                node_data_fs['Q_flow'] = term_obj_flow_set.flow
                            else: # Roots are pressure BCs, their Q_flow for B_vector is 0
                                node_data_fs['Q_flow'] = 0.0
                    
                    temp_graph_for_solver = gbo_graph.copy()
                    gbo_graph_with_flow = perfusion_solver.solve_1d_poiseuille_flow(temp_graph_for_solver, config, None, None) # Use Q_flow on nodes
                    
                    if gbo_graph_with_flow:
                        gbo_graph = gbo_graph_with_flow
                        logger.info("Flow solution obtained. Starting global radius adaptation for GBO tree...")
                        # Adapt radii of synthetic GBO nodes (and potentially measured junctions if desired)
                        nodes_to_adapt_gbo = [n for n, data_gbo_adapt in gbo_graph.nodes(data=True)
                                              if data_gbo_adapt.get('type','').startswith('synthetic_') or \
                                                 data_gbo_adapt.get('type') == 'measured_to_synthetic_junction'] # Adapt junctions too

                        for node_id_adapt in nodes_to_adapt_gbo:
                            node_data_adapt = gbo_graph.nodes[node_id_adapt]
                            actual_node_flow = 0.0
                            original_radius_before_adapt = node_data_adapt.get('radius', min_radius)
                            node_type_for_adapt = node_data_adapt.get('type')

                            is_sink_node = gbo_graph.out_degree(node_id_adapt) == 0 and gbo_graph.in_degree(node_id_adapt) > 0
                            is_source_like_node = gbo_graph.out_degree(node_id_adapt) > 0 # Roots, Bifs

                            if is_sink_node: # e.g. synthetic_terminal
                                for _, _, edge_data_in in gbo_graph.in_edges(node_id_adapt, data=True):
                                    solved_in_flow = edge_data_in.get('flow_solver')
                                    if solved_in_flow is not None and np.isfinite(solved_in_flow):
                                        actual_node_flow += abs(solved_in_flow) # Sum of magnitudes of incoming flows
                            elif is_source_like_node: # e.g. synthetic_bifurcation, synthetic_root_terminal, measured_to_synthetic_junction
                                for _, _, edge_data_out in gbo_graph.out_edges(node_id_adapt, data=True):
                                    solved_out_flow = edge_data_out.get('flow_solver')
                                    if solved_out_flow is not None and np.isfinite(solved_out_flow):
                                        actual_node_flow += abs(solved_out_flow) # Sum of magnitudes of outgoing flows
                            
                            if node_data_adapt.get('is_flow_root'): # Don't adapt radius of fixed pressure roots based on this flow sum
                                logger.debug(f"GlobalAdapt GBO: Node {node_id_adapt} is flow_root. Actual flow sum (for info): {actual_node_flow:.2e}. Radius not adapted by flow.")
                                continue # Skip radius adaptation for roots based on summed child flows

                            if abs(actual_node_flow) > constants.EPSILON:
                                new_radius_adapted = k_murray * (abs(actual_node_flow) ** (1.0 / murray_exp))
                                new_radius_adapted = max(min_radius, new_radius_adapted)
                                if not np.isclose(original_radius_before_adapt, new_radius_adapted, rtol=1e-2, atol=1e-5):
                                    logger.info(f"GlobalAdapt GBO: Node {node_id_adapt} ({node_type_for_adapt}) R: {original_radius_before_adapt:.4f} -> {new_radius_adapted:.4f} (Q_actual_sum={actual_node_flow:.2e})")
                                node_data_adapt['radius'] = new_radius_adapted
                            elif original_radius_before_adapt > min_radius + constants.EPSILON :
                                logger.info(f"GlobalAdapt GBO: Node {node_id_adapt} ({node_type_for_adapt}) Q_actual_sum near zero. Shrinking R from {original_radius_before_adapt:.4f} to {min_radius:.4f}.")
                                node_data_adapt['radius'] = min_radius
                        
                        # Sync radii from gbo_graph back to GBOIterationData objects
                        for term_obj_sync in live_terminals_for_adaptation: # These are GBOIterationData
                            if gbo_graph.has_node(term_obj_sync.id): # s_XXX nodes
                                term_obj_sync.radius = gbo_graph.nodes[term_obj_sync.id]['radius']
                                # Also update GBOIterationData flow if it was derived from solved edge flows (e.g. for terminals)
                                # For now, term_obj_sync.flow is primarily driven by Voronoi demand.
                        logger.info("Global radius adaptation for GBO tree complete.")
                    else: logger.error("Flow solver did not return a graph. Skipping GBO radius adaptation.")
                else: logger.info(f"Skipping GBO flow solver and global radius adaptation for iteration {iteration + 1}.")

        # --- Update Stop Flags (includes the new criterion) ---
        logger.info("Updating stop flags for GBO terminals...")
        active_terminals_still_growing = 0
        for term_data_stop_check in current_active_terminals:
            if term_data_stop_check.stop_growth:
                if gbo_graph.has_node(term_data_stop_check.id): gbo_graph.nodes[term_data_stop_check.id]['stop_growth'] = True
                continue

            if term_data_stop_check.radius < (min_radius + constants.EPSILON) : 
                term_data_stop_check.stop_growth = True
                logger.info(f"Terminal {term_data_stop_check.id} stopped: minRadius (R={term_data_stop_check.radius:.4f} <= {min_radius:.4f})")
            
            if not term_data_stop_check.stop_growth and term_data_stop_check.original_measured_terminal_radius is not None:
                stop_radius_factor_measured = config_manager.get_param(config, "gbo_growth.stop_criteria.radius_match_factor_measured", 0.95)
                stop_radius_factor_measured = np.clip(stop_radius_factor_measured, 0.1, 2.0)
                target_stop_radius = term_data_stop_check.original_measured_terminal_radius * stop_radius_factor_measured
                if term_data_stop_check.radius >= target_stop_radius:
                    term_data_stop_check.stop_growth = True
                    logger.info(f"Terminal {term_data_stop_check.id} (from VTP {term_data_stop_check.parent_measured_terminal_id}) "
                                f"stopped: GBO R {term_data_stop_check.radius:.4f} reached target {target_stop_radius:.4f} "
                                f"(orig_measured_R={term_data_stop_check.original_measured_terminal_radius:.4f}, factor={stop_radius_factor_measured:.2f}).")
            
            if not term_data_stop_check.stop_growth and \
               (not term_data_stop_check.current_territory_voxel_indices_flat and \
                term_data_stop_check.current_territory_demand < constants.EPSILON):
                term_data_stop_check.stop_growth = True
                logger.info(f"Terminal {term_data_stop_check.id} stopped: no territory/demand.")

            if gbo_graph.has_node(term_data_stop_check.id):
                gbo_graph.nodes[term_data_stop_check.id]['stop_growth'] = term_data_stop_check.stop_growth
            
            if not term_data_stop_check.stop_growth: 
                active_terminals_still_growing += 1
        
        logger.info(f"End of GBO iteration {iteration + 1}: {active_terminals_still_growing} GBO terminals still active.")

        # --- Intermediate Save & Global Stop Conditions ---
        stop_due_to_target_perfusion = False
        if total_voxels_in_domain > 0:
             stop_due_to_target_perfusion = (np.sum(perfused_tissue_mask) >= total_voxels_in_domain * 
                                        config_manager.get_param(config, "gbo_growth.target_domain_perfusion_fraction", 0.99))
        stop_due_to_no_active_terminals = (active_terminals_still_growing == 0 and iteration > 0)
        stop_due_to_no_new_growth = (num_newly_perfused_this_iter == 0 and iteration >= min_iters_no_growth_stop)

        if config_manager.get_param(config, "visualization.save_intermediate_steps", False):
            interval = config_manager.get_param(config, "visualization.intermediate_step_interval", 1)
            if ((iteration + 1) % interval == 0) or (iteration == max_iterations - 1) or \
               stop_due_to_no_new_growth or stop_due_to_target_perfusion or stop_due_to_no_active_terminals:
                logger.info(f"Saving intermediate GBO results for iteration {iteration + 1}...")
                io_utils.save_vascular_tree_vtp(gbo_graph, os.path.join(output_dir, f"gbo_graph_iter_{iteration+1}.vtp"))
                if np.any(perfused_tissue_mask):
                    io_utils.save_nifti_image(perfused_tissue_mask.astype(np.uint8), tissue_data['affine'],
                                              os.path.join(output_dir, f"gbo_perfused_mask_iter_{iteration+1}.nii.gz"))

        if stop_due_to_target_perfusion: logger.info(f"GBO Stopping after iteration {iteration + 1}: Target perfusion reached."); break
        if stop_due_to_no_active_terminals: logger.info(f"GBO Stopping after iteration {iteration + 1}: No active GBO terminals."); break
        if stop_due_to_no_new_growth: logger.info(f"GBO Stopping after iteration {iteration + 1}: No new growth for specified iters."); break
            
    logger.info(f"GBO healthy vascular growth finished. Final GBO tree component(s) added to graph. Total graph: {gbo_graph.number_of_nodes()} nodes, {gbo_graph.number_of_edges()} edges.")
    return gbo_graph# src/constants.py
import numpy as np

# Physical Constants
PI = np.pi
# Blood viscosity (e.g., in Pa.s). Typical human blood viscosity is 3-4 cP (0.003-0.004 Pa.s).
# Ensure consistency with pressure (Pa) and length (m or mm) units.
# If length in mm, pressure in Pa, Q in mm^3/s, then viscosity in Pa.s.
# Example: 0.0035 Pa.s
DEFAULT_BLOOD_VISCOSITY = 3.5e-3  # Pa.s

# Default metabolic rates
# These are example values. Units should be carefully considered for flow calculation.
# Let's assume units of: ml_blood / s / ml_tissue (volume flow rate per unit volume of tissue)
# Conversion from common literature values (e.g., ml_blood / min / 100g_tissue):
# Assume tissue density ~ 1 g/ml. So 100g_tissue ~ 100ml_tissue.
# Example: GM 60 ml/min/100g = 60 ml/min/100ml = 1 ml/min/ml_tissue = (1/60) ml/s/ml_tissue ~= 0.0167 s^-1
Q_MET_GM_PER_ML = 0.0167  # 1/s (equivalent to ml_blood / s / ml_tissue)
Q_MET_WM_PER_ML = 0.0056  # 1/s (approx 20-25 ml/min/100g)
Q_MET_TUMOR_RIM_PER_ML = 0.0334 # 1/s (e.g., 2x GM)
Q_MET_TUMOR_CORE_PER_ML = 0.0083 # 1/s (can be lower due to necrosis)
Q_MET_CSF_PER_ML = 0.0 # No metabolic demand for CSF

# GBO Parameters
MURRAY_LAW_EXPONENT = 3.0 # For r_parent^gamma = sum(r_child_i^gamma)
# C_met for metabolic maintenance cost: E_metabolic = C_met * PI * r^2 * L
# Units must be consistent with E_flow. E_flow is in Joules if Q is m^3/s, P in Pa, L in m, r in m.
# E_flow = (Pressure_drop * Q) * time_implicit = ( (8 * mu * L * Q) / (PI * r^4) ) * Q
# So E_flow has units of Power (Energy/time). For the sum, it's more like total power dissipation.
# E_metabolic should also be in units of Power.
# C_met * PI * r^2 * L => C_met units = Power / Length^3 = Power / Volume
# (e.g., W/m^3). This represents volumetric metabolic power density of vessel wall.
DEFAULT_C_MET_VESSEL_WALL = 1.0e5 # W/m^3 (Placeholder value, needs calibration from literature)

# Initial small flow for new terminals to seed Voronoi calculation (e.g., mm^3/s or m^3/s)
# Must be > 0. Let's use mm^3/s if coordinates are in mm.
INITIAL_TERMINAL_FLOW_Q = 1e-6 # mm^3/s (if using mm)

# Perfusion Model Parameters
# Tissue permeability K (e.g., in mm^2 or m^2). This is for Darcy's Law: v = -(K/mu) * grad(P)
# For brain tissue, K is very low. Values can range from 10^-12 to 10^-18 m^2.
# Let's use mm, so if K is in mm^2:
DEFAULT_TISSUE_PERMEABILITY_GM = 1e-7 # mm^2 (Placeholder, highly dependent on tissue type)
DEFAULT_TISSUE_PERMEABILITY_WM = 5e-8  # mm^2 (Placeholder)
# Coupling coefficient beta for Q_terminal = beta * (P_vessel_terminal - P_tissue_at_terminal)
# Units: (mm^3/s) / Pa if P is in Pa and Q in mm^3/s.
DEFAULT_COUPLING_BETA = 1e-7 # mm^3 / (s * Pa) (Placeholder)

# Simulation control
DEFAULT_VOXEL_SIZE_MM = 1.0 # Default isotropic voxel size in mm, if not from NIfTI

# Small epsilon for numerical stability
EPSILON = 1e-9


MIN_VESSEL_RADIUS_MM = 0.005 # example, 5 microns in mm
INITIAL_TERMINAL_FLOW_Q = 1e-6 # mm^3/s# src/energy_model.py
import numpy as np
import networkx as nx # Not directly used in functions yet, but good for context if needed later
import logging
from typing import Tuple, List, Optional # For type hinting

# Attempt to import sklearn for KMeans, but make it optional
try:
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    KMeans = None # Placeholder if not available

from src import constants, config_manager, utils

logger = logging.getLogger(__name__)

def calculate_segment_flow_energy(length: float, radius: float, flow: float, viscosity: float) -> float:
    """ 
    Calculates viscous energy dissipation (power) for a vessel segment.
    E_flow = (8 * mu * L * Q^2) / (pi * r^4) 
    Units: If mu (Pa.s), L (m), Q (m^3/s), r (m), then E_flow is in Watts (J/s).
           If mu (Pa.s), L (mm), Q (mm^3/s), r (mm):
           Pa.s * mm * (mm^3/s)^2 / mm^4 = (N/m^2).s * (1e-3 m) * (1e-9 m^3/s)^2 / (1e-12 m^4)
                                       = N.s/m^2 * 1e-3 m * 1e-18 m^6/s^2 / 1e-12 m^4
                                       = 1e-9 N.m/s = 1e-9 W.
           To get milliWatts (mW), multiply by 1000: 1e-6 mW.
           Users must ensure consistency or apply conversion factors.
    """
    if radius < constants.EPSILON: # Avoid division by zero
        # If there's flow through a zero-radius vessel, cost is infinite
        return np.inf if abs(flow) > constants.EPSILON else 0.0
    if abs(flow) < constants.EPSILON: # If flow is effectively zero, energy dissipation is zero
        return 0.0
    
    # For extremely small radii with non-zero flow, energy can become excessively large.
    # This is physically plausible (high resistance) but can cause numerical issues.
    if radius < 1e-7 and abs(flow) > constants.EPSILON: # e.g., radius < 0.1 micron
        logger.debug(f"Very small radius ({radius:.2e}) with non-zero flow ({flow:.2e}). Flow energy may be extreme.")

    return (8.0 * viscosity * length * (flow**2)) / (constants.PI * (radius**4))

def calculate_segment_metabolic_energy(length: float, radius: float, c_met_coeff: float) -> float:
    """ 
    Calculates metabolic maintenance cost (power) for a vessel segment.
    E_metabolic = C_met_coeff * pi * r^2 * L 
    Units: If C_met_coeff (W/m^3), r (m), L (m), then E_metabolic is in Watts.
           If C_met_coeff (mW/mm^3), r (mm), L (mm), then E_metabolic is in milliWatts.
           The config value for c_met_coeff should be in units consistent with E_flow.
    """
    if radius < constants.EPSILON or length < constants.EPSILON:
        return 0.0
    return c_met_coeff * constants.PI * (radius**2) * length

def calculate_bifurcation_loss(
    parent_pos: np.ndarray, # Position of the bifurcation point itself
    child1_pos: np.ndarray, child1_radius: float, child1_flow: float,
    child2_pos: np.ndarray, child2_radius: float, child2_flow: float,
    config: dict
) -> float:
    """
    Calculates the total loss (E_flow + E_metabolic) for the two new child segments
    originating from parent_pos. The flows are specific to these new child segments.
    """
    viscosity = config_manager.get_param(config, "vascular_properties.blood_viscosity", constants.DEFAULT_BLOOD_VISCOSITY)
    c_met = config_manager.get_param(config, "gbo_growth.energy_coefficient_C_met_vessel_wall", constants.DEFAULT_C_MET_VESSEL_WALL)

    l_c1 = utils.distance(parent_pos, child1_pos)
    l_c2 = utils.distance(parent_pos, child2_pos)

    # Calculate energy for child 1 segment
    if l_c1 < constants.EPSILON or child1_radius < constants.EPSILON:
        e_flow_c1 = np.inf if abs(child1_flow) > constants.EPSILON else 0.0
        e_met_c1 = 0.0
    else:
        e_flow_c1 = calculate_segment_flow_energy(l_c1, child1_radius, child1_flow, viscosity)
        e_met_c1 = calculate_segment_metabolic_energy(l_c1, child1_radius, c_met)

    # Calculate energy for child 2 segment
    if l_c2 < constants.EPSILON or child2_radius < constants.EPSILON:
        e_flow_c2 = np.inf if abs(child2_flow) > constants.EPSILON else 0.0
        e_met_c2 = 0.0
    else:
        e_flow_c2 = calculate_segment_flow_energy(l_c2, child2_radius, child2_flow, viscosity)
        e_met_c2 = calculate_segment_metabolic_energy(l_c2, child2_radius, c_met)
    
    total_loss = e_flow_c1 + e_met_c1 + e_flow_c2 + e_met_c2
    
    # logger.debug(f"Bifurcation candidate: L1={l_c1:.2f}, R1={child1_radius:.4f}, Q1={child1_flow:.2e} -> E_f1={e_flow_c1:.2e}, E_m1={e_met_c1:.2e}")
    # logger.debug(f"                     L2={l_c2:.2f}, R2={child2_radius:.4f}, Q2={child2_flow:.2e} -> E_f2={e_flow_c2:.2e}, E_m2={e_met_c2:.2e}")
    # logger.debug(f"                     Total Loss = {total_loss:.3e}")
    return total_loss



def find_optimal_bifurcation_for_combined_territory(
    parent_terminal_gbo_data: object, # GBOIterationData for the terminal that will bifurcate
    # Combined territory: parent's old territory + new frontier region
    combined_territory_voxel_indices_flat: np.ndarray, 
    tissue_data: dict,
    config: dict,
    k_murray_factor: float,
    murray_exponent: float
) -> Optional[Tuple[np.ndarray, float, float, np.ndarray, float, float, float]]:
    """
    Searches for an optimal bifurcation for the parent_terminal to supply a 
    COMBINED territory (its old territory + a new growth region).
    The children C1 and C2 will share the total demand of this combined_territory.
    
    Args:
        parent_terminal_gbo_data: GBOIterationData of the branching terminal.
        combined_territory_voxel_indices_flat: Flat indices of ALL voxels 
                                               (old territory + new frontier) to be supplied.
        tissue_data: Full tissue data dict.
        config: Simulation config.
        k_murray_factor, murray_exponent: For radius calculation.

    Returns:
        Tuple (child1_pos, child1_radius, child1_total_flow, 
               child2_pos, child2_radius, child2_total_flow, min_loss_for_new_segments) 
        or None if no suitable bifurcation found.
        The child flows are their respective total target flows.
    """
    parent_id = parent_terminal_gbo_data.id
    parent_pos = parent_terminal_gbo_data.pos # This is the bifurcation point
    logger.debug(f"Finding optimal bifurcation for {parent_id} at {np.round(parent_pos,3)} to supply combined "
                 f"territory of {len(combined_territory_voxel_indices_flat)} voxels.")

    if len(combined_territory_voxel_indices_flat) == 0:
        logger.debug(f"Terminal {parent_id}: Combined target territory is empty. No bifurcation.")
        return None

    # Get world coordinates and demand for ALL voxels in the combined territory
    combined_voxels_world_coords = tissue_data['world_coords_flat'][combined_territory_voxel_indices_flat]
    
    demand_map_3d_indices = tissue_data['voxel_indices_flat'][combined_territory_voxel_indices_flat]
    demand_per_combined_voxel_qmet = tissue_data['metabolic_demand_map'][
        demand_map_3d_indices[:,0],
        demand_map_3d_indices[:,1],
        demand_map_3d_indices[:,2]
    ]
    demand_of_combined_voxels_flow = demand_per_combined_voxel_qmet * tissue_data['voxel_volume']
    total_demand_of_combined_territory = np.sum(demand_of_combined_voxels_flow)

    if total_demand_of_combined_territory < constants.EPSILON:
        logger.debug(f"Terminal {parent_id}: Total demand in combined territory is negligible. No bifurcation.")
        return None

    num_candidate_location_sets = config_manager.get_param(config, "gbo_growth.bifurcation_candidate_points", 10)
    min_seg_len = config_manager.get_param(config, "vascular_properties.min_segment_length", 0.1)
    min_radius = config_manager.get_param(config, "vascular_properties.min_radius", constants.MIN_VESSEL_RADIUS_MM)

    best_bifurcation_params = None
    min_loss_found = np.inf

    if len(combined_voxels_world_coords) < 2:
        logger.debug(f"Terminal {parent_id}: Combined territory too small ({len(combined_voxels_world_coords)} voxels) "
                     "for meaningful bifurcation. Consider extension.")
        return None # Bifurcation needs to split demand between two children

    # Candidate child locations should be within or near the combined_territory
    # (KMeans or random sampling on combined_voxels_world_coords)
    for i in range(num_candidate_location_sets):
        c1_pos_candidate, c2_pos_candidate = None, None
        # --- 1. Generate candidate child locations (c1_pos, c2_pos) based on combined_territory ---
        # (Using KMeans as before, but on combined_voxels_world_coords)
        if SKLEARN_AVAILABLE and KMeans is not None:
            try:
                n_clust = min(2, len(combined_voxels_world_coords))
                if n_clust < 2: # Should be caught by len(combined_voxels_world_coords) < 2 above
                    continue 
                kmeans = KMeans(n_clusters=n_clust, random_state=i, n_init='auto').fit(combined_voxels_world_coords)
                c1_pos_candidate = kmeans.cluster_centers_[0]
                c2_pos_candidate = kmeans.cluster_centers_[1]
            except Exception as e_km:
                logger.warning(f"KMeans failed for combined territory (iter {i}): {e_km}. Fallback.")
                indices = np.random.choice(len(combined_voxels_world_coords), 2, replace=False)
                c1_pos_candidate = combined_voxels_world_coords[indices[0]]
                c2_pos_candidate = combined_voxels_world_coords[indices[1]]
        else:
            if i == 0 and not SKLEARN_AVAILABLE : logger.warning("Sklearn KMeans not available for child placement.")
            indices = np.random.choice(len(combined_voxels_world_coords), 2, replace=False)
            c1_pos_candidate = combined_voxels_world_coords[indices[0]]
            c2_pos_candidate = combined_voxels_world_coords[indices[1]]

        if utils.distance(parent_pos, c1_pos_candidate) < min_seg_len or \
           utils.distance(parent_pos, c2_pos_candidate) < min_seg_len or \
           utils.distance(c1_pos_candidate, c2_pos_candidate) < min_seg_len:
            continue

        # --- 2. Assign ALL voxels in combined_territory to c1 or c2 ---
        # This determines Q_C1_total and Q_C2_total for the candidate children.
        q_c1_total_candidate = 0.0
        q_c2_total_candidate = 0.0
        
        for idx_in_combined, voxel_wc in enumerate(combined_voxels_world_coords):
            dist_sq_to_c1 = utils.distance_squared(voxel_wc, c1_pos_candidate)
            dist_sq_to_c2 = utils.distance_squared(voxel_wc, c2_pos_candidate)
            if dist_sq_to_c1 <= dist_sq_to_c2:
                q_c1_total_candidate += demand_of_combined_voxels_flow[idx_in_combined]
            else:
                q_c2_total_candidate += demand_of_combined_voxels_flow[idx_in_combined]
        
        if q_c1_total_candidate < constants.EPSILON or q_c2_total_candidate < constants.EPSILON:
            # This means one child would get (almost) no flow from the entire combined territory.
            # This might be a poor bifurcation unless the other child takes nearly all.
            # Forcing both to have substantial flow might be too restrictive.
            # Let it proceed if total demand is met.
            if abs(q_c1_total_candidate + q_c2_total_candidate - total_demand_of_combined_territory) > constants.EPSILON * total_demand_of_combined_territory:
                 logger.warning(f"Demand conservation issue in child assignment: sum_child_Q={q_c1_total_candidate+q_c2_total_candidate:.2e}, total_demand={total_demand_of_combined_territory:.2e}")
                 continue # Skip if total demand not conserved by split

        # --- 3. Calculate radii for c1, c2 based on their TOTAL flows ---
        r_c1_candidate = k_murray_factor * (q_c1_total_candidate ** (1.0 / murray_exponent)) if q_c1_total_candidate > constants.EPSILON else min_radius
        r_c2_candidate = k_murray_factor * (q_c2_total_candidate ** (1.0 / murray_exponent)) if q_c2_total_candidate > constants.EPSILON else min_radius
        r_c1_candidate = max(min_radius, r_c1_candidate)
        r_c2_candidate = max(min_radius, r_c2_candidate)

        # --- 4. Calculate loss for this candidate bifurcation (for the two new child segments) ---
        # Flows used here are q_c1_total_candidate and q_c2_total_candidate
        current_loss = calculate_bifurcation_loss(
            parent_pos, # Bifurcation point
            c1_pos_candidate, r_c1_candidate, q_c1_total_candidate,
            c2_pos_candidate, r_c2_candidate, q_c2_total_candidate,
            config
        )

        if current_loss < min_loss_found:
            min_loss_found = current_loss
            best_bifurcation_params = (c1_pos_candidate.copy(), r_c1_candidate, q_c1_total_candidate,
                                       c2_pos_candidate.copy(), r_c2_candidate, q_c2_total_candidate,
                                       min_loss_found)

    if best_bifurcation_params:
        logger.info(f"Optimal bifurcation for {parent_id} (supplying combined territory) chosen (Loss {best_bifurcation_params[6]:.3e}): "
                    f"C1 (R={best_bifurcation_params[1]:.4f}, Q_total={best_bifurcation_params[2]:.2e}), "
                    f"C2 (R={best_bifurcation_params[4]:.4f}, Q_total={best_bifurcation_params[5]:.2e})")
        return best_bifurcation_params
    else:
        logger.debug(f"No suitable bifurcation found for terminal {parent_id} to supply combined territory.")
        return None

# The __main__ test block in energy_model.py would need to be updated to call this new function
# and provide mock data for a "combined_territory".


def find_optimal_bifurcation_for_new_region(
    parent_terminal_gbo_data: object, 
    new_growth_region_voxel_indices_flat: np.ndarray, 
    tissue_data: dict,
    config: dict,
    k_murray_factor: float,
    murray_exponent: float
) -> Optional[Tuple[np.ndarray, float, float, np.ndarray, float, float, float]]:
    """
    Searches for an optimal bifurcation for the parent_terminal to supply a *new growth region (Ri,p)*.
    (Full implementation as previously provided)
    """
    parent_id = parent_terminal_gbo_data.id
    parent_pos = parent_terminal_gbo_data.pos
    logger.debug(f"Finding optimal bifurcation for terminal {parent_id} at {parent_pos} to supply a new "
                 f"growth region of {len(new_growth_region_voxel_indices_flat)} voxels.")

    if len(new_growth_region_voxel_indices_flat) == 0:
        logger.debug(f"Terminal {parent_id}: New growth region is empty. No bifurcation.")
        return None

    new_growth_voxels_world_coords = tissue_data['world_coords_flat'][new_growth_region_voxel_indices_flat]
    
    demand_map_3d_indices = tissue_data['voxel_indices_flat'][new_growth_region_voxel_indices_flat]
    demand_per_new_growth_voxel = tissue_data['metabolic_demand_map'][
        demand_map_3d_indices[:,0],
        demand_map_3d_indices[:,1],
        demand_map_3d_indices[:,2]
    ] # This is q_met per voxel, not q_met * dV yet
    
    # Multiply by voxel volume to get demand (flow rate)
    demand_of_new_growth_voxels = demand_per_new_growth_voxel * tissue_data['voxel_volume']
    total_demand_of_new_region = np.sum(demand_of_new_growth_voxels)

    if total_demand_of_new_region < constants.EPSILON:
        logger.debug(f"Terminal {parent_id}: Total demand in the new growth region is negligible. No bifurcation.")
        return None

    num_candidate_location_sets = config_manager.get_param(config, "gbo_growth.bifurcation_candidate_points", 10)
    min_seg_len = config_manager.get_param(config, "vascular_properties.min_segment_length", 0.1)
    min_radius = config_manager.get_param(config, "vascular_properties.min_radius", constants.MIN_VESSEL_RADIUS_MM)

    best_bifurcation_params = None
    min_loss_found = np.inf

    if len(new_growth_voxels_world_coords) < 2 and len(new_growth_voxels_world_coords) > 0: # Handle single voxel new region
         logger.debug(f"New growth region for {parent_id} has only {len(new_growth_voxels_world_coords)} voxel(s). Treating as extension.")
         # This case should ideally be handled by "extension" logic in vascular_growth.py
         # For now, find_optimal_bifurcation will attempt to make one child supply it.
    elif len(new_growth_voxels_world_coords) == 0: # Should be caught earlier
        return None


    for i in range(num_candidate_location_sets):
        c1_pos_candidate, c2_pos_candidate = None, None
        if len(new_growth_voxels_world_coords) >= 2:
            if SKLEARN_AVAILABLE and KMeans is not None:
                try:
                    # Ensure n_clusters is not more than n_samples
                    n_clust = min(2, len(new_growth_voxels_world_coords))
                    if n_clust < 2 : # Not enough for two distinct clusters
                        idx = np.random.choice(len(new_growth_voxels_world_coords), 1)[0]
                        c1_pos_candidate = new_growth_voxels_world_coords[idx]
                        # Create a dummy c2 for calculation, it will get ~0 flow from new region
                        c2_pos_candidate = c1_pos_candidate + utils.normalize_vector(np.random.rand(3)-0.5) * min_seg_len
                    else:
                        kmeans = KMeans(n_clusters=n_clust, random_state=i, n_init='auto').fit(new_growth_voxels_world_coords)
                        c1_pos_candidate = kmeans.cluster_centers_[0]
                        c2_pos_candidate = kmeans.cluster_centers_[1] if n_clust == 2 else kmeans.cluster_centers_[0] + utils.normalize_vector(np.random.rand(3)-0.5) * min_seg_len

                except Exception as e_km: # Catch any Kmeans error
                    logger.warning(f"KMeans clustering failed for bifurcation candidates (iter {i}): {e_km}. Falling back to random points.")
                    # Fallback to random points from the new growth region
                    indices = np.random.choice(len(new_growth_voxels_world_coords), 2, replace=len(new_growth_voxels_world_coords) < 2)
                    c1_pos_candidate = new_growth_voxels_world_coords[indices[0]]
                    c2_pos_candidate = new_growth_voxels_world_coords[indices[1]]

            else: # SKLEARN_AVAILABLE is False or KMeans is None (ImportError)
                if i == 0 : logger.warning("Scikit-learn not found. Using random points for bifurcation candidates.")
                indices = np.random.choice(len(new_growth_voxels_world_coords), 2, replace=len(new_growth_voxels_world_coords) < 2)
                c1_pos_candidate = new_growth_voxels_world_coords[indices[0]]
                c2_pos_candidate = new_growth_voxels_world_coords[indices[1]]
        
        elif len(new_growth_voxels_world_coords) == 1:
            c1_pos_candidate = new_growth_voxels_world_coords[0]
            c2_pos_candidate = c1_pos_candidate + utils.normalize_vector(np.random.rand(3)-0.5) * min_seg_len # Dummy c2

        if c1_pos_candidate is None or c2_pos_candidate is None : continue # Should not happen with fallbacks

        if utils.distance(parent_pos, c1_pos_candidate) < min_seg_len or \
           utils.distance(parent_pos, c2_pos_candidate) < min_seg_len or \
           (utils.distance(c1_pos_candidate, c2_pos_candidate) < min_seg_len and not np.allclose(c1_pos_candidate, c2_pos_candidate)):
            continue

        q_c1_candidate_from_new = 0.0
        q_c2_candidate_from_new = 0.0
        
        for idx_in_new_region, voxel_wc in enumerate(new_growth_voxels_world_coords):
            dist_sq_to_c1 = utils.distance_squared(voxel_wc, c1_pos_candidate)
            dist_sq_to_c2 = utils.distance_squared(voxel_wc, c2_pos_candidate)
            if dist_sq_to_c1 <= dist_sq_to_c2:
                q_c1_candidate_from_new += demand_of_new_growth_voxels[idx_in_new_region]
            else:
                q_c2_candidate_from_new += demand_of_new_growth_voxels[idx_in_new_region]
        
        # Ensure flow is not split into practically zero for both if there's demand
        if (q_c1_candidate_from_new < constants.EPSILON and q_c2_candidate_from_new < constants.EPSILON and total_demand_of_new_region > constants.EPSILON):
            # This might happen if c1_pos and c2_pos are poorly chosen relative to demand distribution.
            # For example, if both are far from all demand points.
            # Or if total_demand_of_new_region is tiny.
            # logger.debug(f"Candidate pair {i} results in zero flow for both children from new region. Skipping.")
            continue

        r_c1_candidate = k_murray_factor * (q_c1_candidate_from_new ** (1.0 / murray_exponent)) if q_c1_candidate_from_new > constants.EPSILON else min_radius
        r_c2_candidate = k_murray_factor * (q_c2_candidate_from_new ** (1.0 / murray_exponent)) if q_c2_candidate_from_new > constants.EPSILON else min_radius
        r_c1_candidate = max(min_radius, r_c1_candidate)
        r_c2_candidate = max(min_radius, r_c2_candidate)

        current_loss = calculate_bifurcation_loss(
            parent_pos,
            c1_pos_candidate, r_c1_candidate, q_c1_candidate_from_new,
            c2_pos_candidate, r_c2_candidate, q_c2_candidate_from_new,
            config
        )

        if current_loss < min_loss_found:
            min_loss_found = current_loss
            best_bifurcation_params = (c1_pos_candidate.copy(), r_c1_candidate, q_c1_candidate_from_new,
                                       c2_pos_candidate.copy(), r_c2_candidate, q_c2_candidate_from_new,
                                       min_loss_found)

    if best_bifurcation_params:
        logger.info(f"Optimal bifurcation for {parent_id} to supply new region chosen (Loss {best_bifurcation_params[6]:.3e}): "
                    f"C1 (R={best_bifurcation_params[1]:.4f}, Q_new={best_bifurcation_params[2]:.2e}), "
                    f"C2 (R={best_bifurcation_params[4]:.4f}, Q_new={best_bifurcation_params[5]:.2e})")
        return best_bifurcation_params
    else:
        logger.debug(f"No suitable (lower loss or valid) bifurcation found for terminal {parent_id} to supply new region.")
        return None

if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG) # Changed to DEBUG for more verbose test output
    
    # Mock config
    test_config = {
        "vascular_properties": {
            "blood_viscosity": 0.0035, 
            "min_segment_length": 0.01, 
            "min_radius": 0.005, 
            "k_murray_scaling_factor": 0.5, 
            "murray_law_exponent": 3.0
        },
        "gbo_growth": {
            # C_met unit: e.g. mW/mm^3 (if E_flow target unit is mW)
            # If E_flow from calculate_segment_flow_energy is in 10^-9 W (nanoWatts) when inputs are mm, Pa.s, mm^3/s
            # And we want E_met to be in same units: C_met * mm^2 * mm = C_met * mm^3
            # So C_met should be in (10^-9 W) / mm^3.
            # If paper uses C_met for W/m^3, and we use mm:
            # C_met_paper (W/m^3) * (1m/1000mm)^3 = C_met_paper * 1e-9 (W/mm^3)
            # Let's use a value that makes E_met somewhat comparable to E_flow for typical values.
            # Example from before: L=1, R=0.1, Q=0.01 -> E_flow ~ 1.1e-7 (in 10^-9 W units, so 1.1e-16 W actual)
            # E_met = C_met * pi * (0.1)^2 * 1. If C_met = 1e-5 (in 10^-9W/mm^3 units), E_met ~ 3e-7.
            "energy_coefficient_C_met_vessel_wall": 1.0e-5, # (Units: 10^-9 W / mm^3 or equivalent)
            "bifurcation_candidate_points": 50, # Increased for better testing
        }
    }
    
    # Ensure constants are properly defined or mocked for standalone execution
    class MockConstants:
        PI = np.pi
        EPSILON = 1e-10 # Slightly smaller epsilon
        DEFAULT_BLOOD_VISCOSITY = 0.0035
        # This default C_MET_VESSEL_WALL in constants.py is likely in W/m^3.
        # The config value above is what's used by the functions.
        DEFAULT_C_MET_VESSEL_WALL = 1.0e5 
        MIN_VESSEL_RADIUS_MM = 0.005
    
    # Overwrite constants only if they are not the actual imported ones (e.g. running file directly)
    if 'constants' not in globals() or not hasattr(constants, 'MIN_VESSEL_RADIUS_MM'):
        constants = MockConstants()
        print("Using MockConstants for standalone test.")


    # Test calculate_segment_flow_energy
    # Using L (mm), R (mm), Q (mm^3/s), mu (Pa.s)
    # Expected output units: 10^-9 W (nanoWatts) or Pa.mm^3/s
    l, r_test, q_test, mu_test = 1.0, 0.1, 0.01, test_config["vascular_properties"]["blood_viscosity"]
    e_flow = calculate_segment_flow_energy(l, r_test, q_test, mu_test)
    
    # c_met_val from config is assumed to be in (10^-9 W)/mm^3 to match E_flow units
    c_met_val = test_config["gbo_growth"]["energy_coefficient_C_met_vessel_wall"] 
    e_met = calculate_segment_metabolic_energy(l, r_test, c_met_val)
    logger.info(f"Test segment: L={l}mm, R={r_test}mm, Q={q_test}mm^3/s, mu={mu_test}Pa.s")
    logger.info(f"Calculated E_flow = {e_flow:.3e} (expected units: Pa.mm^3/s or 10^-9 W)")
    logger.info(f"Calculated E_met (with C_met={c_met_val:.1e}) = {e_met:.3e} (expected units: same as E_flow)")
    # Expected E_flow = (8 * 0.0035 * 1 * 0.01^2) / (pi * 0.1^4) = (2.8e-6) / (pi * 1e-4) approx 2.8e-6 / 3.14e-4 = 0.0089 Pa.mm^3/s
    # My previous manual calc was off. Let's recheck:
    # (8 * 0.0035 * 1.0 * (0.01**2)) / (np.pi * (0.1**4)) = 0.008912676...
    # E_met = 1e-5 * np.pi * (0.1**2) * 1.0 = 3.14159e-7
    # These values seem reasonable relative to each other if C_met is chosen appropriately.

    # Mock parent terminal for find_optimal_bifurcation_for_new_region
    class MockGBOIterationData: # Simplified for this test
        def __init__(self, id, pos, radius, flow): # Removed territory_demand as it's not used by this func
            self.id = id
            self.pos = np.array(pos)
            self.radius = radius
            self.flow = flow # This is flow to *existing* territory, not used by find_optimal_bifurcation_for_new_region

    parent_term = MockGBOIterationData(
        id="p_test_0", 
        pos=np.array([0.,0.,0.]), 
        radius=0.2, 
        flow=0.00 # Flow to existing territory, not directly used for optimizing *new* region supply
    )

    # Mock tissue_data for the new growth region
    num_new_growth_voxels = 50
    # These are flat indices relative to the global tissue_data arrays
    # For the test, let's assume these are the first 'num_new_growth_voxels' in a hypothetical global list
    mock_new_growth_indices_flat = np.arange(num_new_growth_voxels) 
    
    # World coords for these specific new growth voxels
    # Place them in a cluster, e.g., around [1,1,0]
    mock_new_growth_world_coords = np.random.rand(num_new_growth_voxels, 3) * 0.5 + np.array([1.0, 1.0, 0.0])
    
    # For tissue_data, we need the *full* set of domain voxels
    # For this test, we can make tissue_data['world_coords_flat'] just be these new growth voxels
    # And correspondingly for voxel_indices_flat and metabolic_demand_map
    
    mock_tissue_voxel_indices_3d = np.zeros((num_new_growth_voxels, 3), dtype=int)
    for i in range(num_new_growth_voxels): # Dummy 3D indices
        mock_tissue_voxel_indices_3d[i] = [i // 10, i % 10, 0] 

    # Demand for these new growth voxels (q_met, not q_met * dV yet)
    mock_demand_q_met_for_new_voxels = np.random.uniform(low=0.01, high=0.02, size=num_new_growth_voxels) # 1/s (q_met)
    
    # Assume a voxel volume for calculating total demand from q_met
    mock_voxel_vol = 0.001 # mm^3 (e.g., 0.1mm x 0.1mm x 0.1mm)

    # Create the metabolic_demand_map (3D) that would contain these values
    # For simplicity, make it just large enough for our dummy 3D indices
    max_indices = np.max(mock_tissue_voxel_indices_3d, axis=0)
    mock_full_metabolic_demand_map_3d = np.zeros((max_indices[0]+1, max_indices[1]+1, max_indices[2]+1))
    mock_full_metabolic_demand_map_3d[mock_tissue_voxel_indices_3d[:,0],
                                      mock_tissue_voxel_indices_3d[:,1],
                                      mock_tissue_voxel_indices_3d[:,2]] = mock_demand_q_met_for_new_voxels
    
    mock_tissue_data = {
        'world_coords_flat': mock_new_growth_world_coords, # Only contains the new growth voxels for this test
        'voxel_indices_flat': mock_tissue_voxel_indices_3d, # Corresponding 3D indices
        'metabolic_demand_map': mock_full_metabolic_demand_map_3d, # Full 3D q_met map
        'voxel_volume': mock_voxel_vol
    }
    
    bifurcation_result = find_optimal_bifurcation_for_combined_territory(
        parent_term,
        mock_new_growth_indices_flat, # These are indices into the arrays in mock_tissue_data
        mock_tissue_data,
        test_config,
        k_murray_factor=test_config["vascular_properties"]["k_murray_scaling_factor"],
        murray_exponent=test_config["vascular_properties"]["murray_law_exponent"]
    )

    if bifurcation_result:
        c1p, c1r, c1q_new, c2p, c2r, c2q_new, loss = bifurcation_result
        logger.info(f"Optimal Bifurcation Found for {parent_term.id} to supply new region:")
        logger.info(f"  Child 1: Pos={np.round(c1p,3)}, Radius={c1r:.4f}, Flow_new={c1q_new:.3e}")
        logger.info(f"  Child 2: Pos={np.round(c2p,3)}, Radius={c2r:.4f}, Flow_new={c2q_new:.3e}")
        logger.info(f"  Minimized Loss (for new segments): {loss:.3e}")
        # Verify total new flow captured
        total_new_demand_calc = np.sum(mock_demand_q_met_for_new_voxels * mock_voxel_vol)
        logger.info(f"  Sum of child flows from new region: {(c1q_new + c2q_new):.3e}")
        logger.info(f"  Total demand of new region: {total_new_demand_calc:.3e}")
        assert np.isclose(c1q_new + c2q_new, total_new_demand_calc), "Child flows do not sum to total new demand"
    else:
        logger.info(f"No optimal bifurcation found for {parent_term.id} in this test.")# src/visualization.py
from __future__ import annotations # Must be first line

import logging
import os
import numpy as np
import networkx as nx
from typing import Optional, Dict, List, Tuple
import pandas as pd

try:
    import pyvista as pv
    PYVISTA_AVAILABLE = True
except ImportError:
    PYVISTA_AVAILABLE = False
    pv = None

try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    plt = None

from src import io_utils, config_manager, utils, constants

logger = logging.getLogger(__name__)

# --- Plotting helper for distributions ---
def _plot_histogram(data: List[float], title: str, xlabel: str, output_path: str, bins: int = 30, density: bool = False):
    if not MATPLOTLIB_AVAILABLE:
        logger.warning(f"Matplotlib not available. Skipping histogram plot: {title}")
        return
    if not data:
        logger.warning(f"No data to plot for histogram: {title}")
        return
    valid_data = [x for x in data if np.isfinite(x)]
    if not valid_data:
        logger.warning(f"No finite data to plot for histogram (all NaN/Inf): {title}")
        return

    plt.figure(figsize=(8, 6))
    plt.hist(valid_data, bins=bins, color='skyblue', edgecolor='black', density=density)
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel("Frequency" if not density else "Density")
    plt.grid(axis='y', alpha=0.75)
    try:
        plt.savefig(output_path)
        logger.info(f"Saved histogram '{title}' to {output_path}")
    except Exception as e:
        logger.error(f"Error saving histogram '{title}' to {output_path}: {e}")
    finally:
        plt.close()

# --- Functions for Quantitative Analysis ---
def analyze_radii_distribution(graph: nx.DiGraph, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Radii analysis: Graph is empty or None.")
        return
    radii = [data['radius'] for _, data in graph.nodes(data=True)
             if 'radius' in data and data.get('is_synthetic', True) and np.isfinite(data['radius'])]
    if not radii: logger.info("No synthetic nodes with valid radii found for distribution analysis."); return
    df_radii = pd.DataFrame(radii, columns=['radius_mm'])
    csv_path = os.path.join(output_dir, f"{filename_prefix}radii_data.csv")
    df_radii.to_csv(csv_path, index=False); logger.info(f"Saved raw radii data to {csv_path}")
    plot_path = os.path.join(output_dir, f"{filename_prefix}radii_distribution.png")
    _plot_histogram(radii, "Distribution of Vessel Radii (Synthetic Nodes)", "Radius (mm)", plot_path)

def analyze_segment_lengths(graph: nx.DiGraph, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_edges() == 0:
        logger.warning("Segment length analysis: Graph is empty or has no edges.")
        return
    lengths = []
    for u, v, data in graph.edges(data=True):
        if u in graph.nodes and v in graph.nodes and 'length' in data and \
           (graph.nodes[u].get('is_synthetic', True) or graph.nodes[v].get('is_synthetic', True)) and \
           np.isfinite(data['length']):
            lengths.append(data['length'])

    if not lengths: logger.info("No synthetic segments with valid lengths found for distribution analysis."); return
    df_lengths = pd.DataFrame(lengths, columns=['length_mm'])
    csv_path = os.path.join(output_dir, f"{filename_prefix}segment_lengths_data.csv")
    df_lengths.to_csv(csv_path, index=False); logger.info(f"Saved raw segment length data to {csv_path}")
    plot_path = os.path.join(output_dir, f"{filename_prefix}segment_lengths_distribution.png")
    _plot_histogram(lengths, "Distribution of Segment Lengths", "Length (mm)", plot_path)


def analyze_bifurcation_geometry(graph: nx.DiGraph, output_dir: str, murray_exponent: float = 3.0, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Bifurcation geometry analysis: Graph is empty or None.")
        return

    murray_parent_powers: List[float] = []
    murray_children_sum_powers: List[float] = []
    area_ratios_alpha: List[float] = []
    daughter_asymmetry_ratios: List[float] = []
    branching_angles_c1_c2: List[float] = []
    bifurcation_data_for_csv: List[Dict] = []

    logger.info(f"--- Starting Bifurcation Geometry Analysis (Murray Exp: {murray_exponent}) ---")
    bifurcation_nodes_processed = 0

    for node_id, data in graph.nodes(data=True):
        node_type = data.get('type', '')
        is_potential_bifurcation = (node_type == 'synthetic_bifurcation') or \
                                   (node_type == 'synthetic_root_terminal' and data.get('is_flow_root', False))

        if is_potential_bifurcation and graph.out_degree(node_id) == 2:
            bifurcation_nodes_processed += 1
            parent_pos = data.get('pos')
            children_ids = list(graph.successors(node_id))

            r_p_node = data.get('radius')
            q_p_node = data.get('Q_flow') 

            if len(children_ids) != 2 or \
               children_ids[0] not in graph.nodes or \
               children_ids[1] not in graph.nodes:
                logger.debug(f"Bif. Geom. Test: Node {node_id} has invalid children setup. Skipping.")
                continue

            data_c1 = graph.nodes[children_ids[0]]
            data_c2 = graph.nodes[children_ids[1]]
            child1_pos = data_c1.get('pos')
            child2_pos = data_c2.get('pos')
            r_c1_node = data_c1.get('radius')
            r_c2_node = data_c2.get('radius')
            q_c1_node = data_c1.get('Q_flow')
            q_c2_node = data_c2.get('Q_flow')

            if not (r_p_node and np.isfinite(r_p_node) and r_p_node >= constants.EPSILON and
                    r_c1_node and np.isfinite(r_c1_node) and r_c1_node >= constants.EPSILON and
                    r_c2_node and np.isfinite(r_c2_node) and r_c2_node >= constants.EPSILON):
                logger.debug(f"Bif. Geom. Test: Node {node_id} or children have invalid radii. Skipping.")
                continue

            p_power = r_p_node**murray_exponent
            c_sum_power = r_c1_node**murray_exponent + r_c2_node**murray_exponent
            murray_parent_powers.append(p_power)
            murray_children_sum_powers.append(c_sum_power)

            alpha = (r_c1_node**2 + r_c2_node**2) / (r_p_node**2)
            area_ratios_alpha.append(alpha)

            asymmetry_ratio = min(r_c1_node, r_c2_node) / max(r_c1_node, r_c2_node) if max(r_c1_node, r_c2_node) > constants.EPSILON else 1.0
            daughter_asymmetry_ratios.append(asymmetry_ratio)
            
            angle_deg_val = np.nan # Initialize for CSV
            if parent_pos is not None and child1_pos is not None and child2_pos is not None:
                vec1 = child1_pos - parent_pos
                vec2 = child2_pos - parent_pos
                norm_vec1 = np.linalg.norm(vec1)
                norm_vec2 = np.linalg.norm(vec2)
                if norm_vec1 > constants.EPSILON and norm_vec2 > constants.EPSILON:
                    cosine_angle = np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)
                    angle_rad = np.arccos(np.clip(cosine_angle, -1.0, 1.0))
                    angle_deg_val = np.degrees(angle_rad)
                    if np.isfinite(angle_deg_val):
                        branching_angles_c1_c2.append(angle_deg_val)

            bifurcation_data_for_csv.append({
                'b_id': node_id,
                f'rP^{murray_exponent:.1f}': p_power, f'sum_rC^{murray_exponent:.1f}': c_sum_power,
                'area_ratio_alpha': alpha, 'daughter_asymmetry_ratio': asymmetry_ratio,
                'angle_deg': angle_deg_val,
                'rP': r_p_node, 'rC1': r_c1_node, 'rC2': r_c2_node,
                'qP_graph': q_p_node, 'qC1_graph': q_c1_node, 'qC2_graph': q_c2_node
            })

    logger.info(f"Bifurcation Geometry Analysis: Processed {bifurcation_nodes_processed} potential bifurcation nodes.")

    if bifurcation_data_for_csv:
        df_bif_geom = pd.DataFrame(bifurcation_data_for_csv)
        csv_path = os.path.join(output_dir, f"{filename_prefix}bifurcation_geometry_data.csv")
        df_bif_geom.to_csv(csv_path, index=False)
        logger.info(f"Saved bifurcation geometry data to {csv_path}")
    else:
        logger.info("No valid bifurcation data collected for CSV. Skipping plots.")
        return

    if MATPLOTLIB_AVAILABLE and murray_parent_powers:
        plt.figure(figsize=(7, 7))
        plt.scatter(murray_children_sum_powers, murray_parent_powers, alpha=0.6, edgecolors='k', s=40, label="Bifurcations")
        max_val_plot = 0.0
        valid_parent_powers = [p for p in murray_parent_powers if np.isfinite(p)]
        valid_children_sum_powers = [c for c in murray_children_sum_powers if np.isfinite(c)]
        if valid_parent_powers and valid_children_sum_powers:
             max_val_plot = max(max(valid_parent_powers, default=0.0), max(valid_children_sum_powers, default=0.0)) * 1.1
        if max_val_plot < constants.EPSILON : max_val_plot = 1.0

        plt.plot([0, max_val_plot], [0, max_val_plot], 'r--', label=f'Ideal Murray (y=x, exp={murray_exponent:.1f})')
        plt.xlabel(f"Sum of Children Radii^{murray_exponent:.1f} (r$_1^{murray_exponent:.1f}$ + r$_2^{murray_exponent:.1f}$)")
        plt.ylabel(f"Parent Radius^{murray_exponent:.1f} (r$_0^{murray_exponent:.1f}$)")
        plt.title("Murray's Law Compliance Test")
        plt.legend()
        plt.grid(True)
        plt.xlim([0, max_val_plot]); plt.ylim([0, max_val_plot])
        plot_path = os.path.join(output_dir, f"{filename_prefix}murray_law_compliance.png")
        try: plt.savefig(plot_path); logger.info(f"Saved Murray's Law plot to {plot_path}");
        except Exception as e: logger.error(f"Error saving Murray's Law plot: {e}")
        finally: plt.close()
    else: logger.info("Skipping Murray's Law plot (Matplotlib unavailable or no data).")

    _plot_histogram(area_ratios_alpha, "Distribution of Bifurcation Area Ratios (α)", "Area Ratio α = (r₁²+r₂²)/r₀²",
                    os.path.join(output_dir, f"{filename_prefix}area_ratios_distribution.png"), bins=20)
    _plot_histogram(daughter_asymmetry_ratios, "Distribution of Daughter Radius Asymmetry Ratios", "Asymmetry Ratio min(r₁,r₂)/max(r₁,r₂)",
                    os.path.join(output_dir, f"{filename_prefix}daughter_asymmetry_distribution.png"), bins=20)
    _plot_histogram(branching_angles_c1_c2, "Distribution of Branching Angles (Child-Child)", "Angle (degrees)",
                    os.path.join(output_dir, f"{filename_prefix}branching_angles_distribution.png"), bins=18)


def analyze_degree_distribution(graph: nx.DiGraph, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Degree distribution analysis: Graph is empty or None.")
        return
    if not MATPLOTLIB_AVAILABLE:
        logger.warning("Matplotlib not available. Skipping degree distribution plots.")
        return

    degrees = [d for n, d in graph.degree()]
    in_degrees = [d for n, d in graph.in_degree()]
    out_degrees = [d for n, d in graph.out_degree()]

    df_degrees = pd.DataFrame({
        'node_id': list(graph.nodes()),
        'total_degree': degrees,
        'in_degree': in_degrees,
        'out_degree': out_degrees
    })
    csv_path = os.path.join(output_dir, f"{filename_prefix}degree_data.csv")
    df_degrees.to_csv(csv_path, index=False)
    logger.info(f"Saved node degree data to {csv_path}")

    max_bins_degree = 10 # Default max bins for degree plots
    if degrees: max_bins_degree = max(1, max(degrees))
    _plot_histogram(degrees, "Total Node Degree Distribution", "Degree",
                    os.path.join(output_dir, f"{filename_prefix}total_degree_distribution.png"),
                    bins=min(max_bins_degree, 50)) # Cap bins for very high degrees
    if in_degrees: max_bins_degree = max(1, max(in_degrees))
    _plot_histogram(in_degrees, "Node In-Degree Distribution", "In-Degree",
                    os.path.join(output_dir, f"{filename_prefix}in_degree_distribution.png"),
                    bins=min(max_bins_degree, 50))
    if out_degrees: max_bins_degree = max(1, max(out_degrees))
    _plot_histogram(out_degrees, "Node Out-Degree Distribution", "Out-Degree",
                    os.path.join(output_dir, f"{filename_prefix}out_degree_distribution.png"),
                    bins=min(max_bins_degree, 50))

def analyze_network_connectivity(graph: nx.DiGraph, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Network connectivity analysis: Graph is empty or None.")
        return
    undirected_graph = graph.to_undirected()
    num_components = nx.number_connected_components(undirected_graph)
    logger.info(f"Network Connectivity: Number of connected components (undirected) = {num_components}")
    if num_components > 1:
        logger.warning(f"Network has {num_components} disconnected components. Expected 1 for a fully connected structure from seeds.")

def analyze_volumetric_densities(graph: nx.DiGraph, tissue_data: dict, output_dir: str, filename_prefix: str = "final_"):
    if graph is None or graph.number_of_nodes() == 0:
        logger.warning("Volumetric density analysis: Graph is empty or None.")
        return
    if 'domain_mask' not in tissue_data or 'voxel_volume' not in tissue_data:
        logger.warning("Volumetric density analysis: Missing 'domain_mask' or 'voxel_volume' in tissue_data.")
        return

    domain_mask = tissue_data['domain_mask']
    voxel_volume = tissue_data['voxel_volume'] 

    if domain_mask is None or voxel_volume <= 0:
        logger.warning("Volumetric density analysis: Invalid domain_mask or voxel_volume.")
        return

    domain_volume_mm3 = np.sum(domain_mask) * voxel_volume
    if domain_volume_mm3 < constants.EPSILON:
        logger.warning("Volumetric density analysis: Domain volume is zero. Cannot calculate densities.")
        return

    total_vessel_length_mm = sum(data['length'] for u, v, data in graph.edges(data=True) if 'length' in data and np.isfinite(data['length']))
    num_bifurcation_nodes = sum(1 for node_id, data in graph.nodes(data=True) if data.get('type') == 'synthetic_bifurcation')

    vessel_length_density_mm_per_mm3 = total_vessel_length_mm / domain_volume_mm3
    branchpoint_density_per_mm3 = num_bifurcation_nodes / domain_volume_mm3

    logger.info(f"--- Volumetric Densities (Domain Volume: {domain_volume_mm3:.2e} mm^3) ---")
    logger.info(f"  Total vessel length: {total_vessel_length_mm:.2e} mm")
    logger.info(f"  Vessel length density: {vessel_length_density_mm_per_mm3:.2e} mm/mm^3")
    logger.info(f"  Number of bifurcation points: {num_bifurcation_nodes}")
    logger.info(f"  Branchpoint density: {branchpoint_density_per_mm3:.2e} #/mm^3")

    density_data = {
        'domain_volume_mm3': domain_volume_mm3,
        'total_vessel_length_mm': total_vessel_length_mm,
        'vessel_length_density_mm_per_mm3': vessel_length_density_mm_per_mm3,
        'num_bifurcation_nodes': num_bifurcation_nodes,
        'branchpoint_density_per_mm3': branchpoint_density_per_mm3
    }
    df_density = pd.DataFrame([density_data])
    csv_path = os.path.join(output_dir, f"{filename_prefix}volumetric_density_data.csv")
    df_density.to_csv(csv_path, index=False)
    logger.info(f"Saved volumetric density data to {csv_path}")


def plot_vascular_tree_pyvista(
    graph: Optional[nx.DiGraph],
    title: str = "Vascular Tree",
    # ... (other existing parameters: background_color, cmap_radius, etc.)
    output_screenshot_path: Optional[str] = None,
    tissue_masks: Optional[Dict[str, Tuple[np.ndarray, np.ndarray]]] = None,
    gbo_seed_points_world: Optional[List[Tuple[np.ndarray, float, str]]] = None, # Renamed for clarity
    initial_tumor_seed_info: Optional[Dict] = None, # New: {'center_world': np.array, 'radius_world': float}
    # ... (other existing parameters: domain_outline_color, etc.)
    color_by_scalar: Optional[str] = 'radius', 
    scalar_bar_title_override: Optional[str] = None,
    custom_cmap: Optional[str] = None,
    config_for_viz_params: Optional[dict] = None # Pass config for viz specific params
    ):
    if not PYVISTA_AVAILABLE: logger.warning("PyVista not available. Skipping 3D PyVista plot."); return

    # Fetch viz params from config if provided, else use hardcoded defaults
    cfg = config_for_viz_params if config_for_viz_params else {}
    bg_color = config_manager.get_param(cfg, "visualization.pyvista_background_color", "white")
    default_cmap_radius = config_manager.get_param(cfg, "visualization.pyvista_cmap_radius", "viridis")
    seed_color = config_manager.get_param(cfg, "visualization.seed_point_color", "red")
    seed_radius_scale = config_manager.get_param(cfg, "visualization.seed_marker_radius_scale", 5.0)
    tumor_seed_color = config_manager.get_param(cfg, "visualization.tumor_seed_marker_color", "magenta")


    plotter = pv.Plotter(off_screen=output_screenshot_path is not None, window_size=[1200,900])
    plotter.background_color = bg_color
    plotter.add_title(title, font_size=16)

    spacing_for_markers = np.array([1.0, 1.0, 1.0]) # Default

    if tissue_masks:
        mask_colors_default = {
            "GM": "lightblue", "WM": "lightyellow", 
            "domain_mask": config_manager.get_param(cfg, "visualization.domain_mask_color", "lightgray"),
            "CSF": "lightcyan",
            "Tumor_Max_Extent": config_manager.get_param(cfg, "visualization.tumor_max_extent_mask_color", "salmon"),
            "Tumor": config_manager.get_param(cfg, "visualization.active_tumor_mask_color", "darkred") # For active tumor
        }
        mask_opacities_default = {
            "GM": 0.2, "WM": 0.2, 
            "domain_mask": config_manager.get_param(cfg, "visualization.domain_mask_opacity", 0.1),
            "CSF": 0.1,
            "Tumor_Max_Extent": 0.15,
            "Tumor": 0.3 # Active tumor slightly more opaque
        }

        for mask_name, mask_data_tuple in tissue_masks.items():
            if not isinstance(mask_data_tuple, tuple) or len(mask_data_tuple) != 2: continue
            mask_data, affine = mask_data_tuple
            if mask_data is None or not np.any(mask_data) or affine is None: continue

            logger.info(f"Plotting context mask '{mask_name}': Shape={mask_data.shape}, Sum={np.sum(mask_data)}")
            try:
                dims = np.array(mask_data.shape)
                current_spacing = np.abs(np.diag(affine)[:3])
                origin = affine[:3, 3]
                if np.all(current_spacing > constants.EPSILON): # Update if valid spacing
                    spacing_for_markers = current_spacing

                grid = pv.ImageData(dimensions=dims, spacing=current_spacing, origin=origin)
                grid.point_data[mask_name] = mask_data.flatten(order="F").astype(float)
                contour = grid.contour([0.5], scalars=mask_name, rng=[0,1])

                if contour.n_points > 0:
                    color = mask_colors_default.get(mask_name, "grey")
                    opacity = mask_opacities_default.get(mask_name, 0.1)
                    plotter.add_mesh(contour, color=color, opacity=opacity, style='surface')
                    logger.debug(f"Mask '{mask_name}' contour bounds: {contour.bounds}")
                    if mask_name == "domain_mask" and output_screenshot_path: # For initial setup plot mainly
                        debug_contour_path = os.path.join(os.path.dirname(output_screenshot_path), f"debug_plot_{mask_name}_contour.vtk")
                        try: contour.save(debug_contour_path)
                        except Exception as e_save: logger.error(f"Could not save debug contour {mask_name}: {e_save}")

                else: logger.warning(f"No contour generated for mask '{mask_name}'.")
            except Exception as e_mask: logger.error(f"Error plotting mask '{mask_name}': {e_mask}", exc_info=True)
    else:
        logger.info("No tissue masks provided for this plot.")

    # Plot GBO Seed points (if any)
    if gbo_seed_points_world:
        seed_color = config_manager.get_param(cfg, "visualization.seed_point_color", "red") # Assuming cfg is defined from config_for_viz_params
        seed_radius_scale = config_manager.get_param(cfg, "visualization.seed_marker_radius_scale", 2.0)
        for seed_pos, seed_initial_radius, seed_name in gbo_seed_points_world:
            marker_base_size = np.mean(spacing_for_markers) * 0.5
            visual_marker_radius = max(marker_base_size * seed_radius_scale, seed_initial_radius * 2.0, 0.05 * np.mean(spacing_for_markers))
            plotter.add_mesh(pv.Sphere(center=seed_pos, radius=visual_marker_radius), color=seed_color, opacity=0.9)
            plotter.add_point_labels(seed_pos + np.array([0,0,visual_marker_radius*2]), [seed_name], font_size=10, text_color=seed_color)


    # Plot Initial Tumor Seed Sphere (New)
    if initial_tumor_seed_info:
        center_w = initial_tumor_seed_info.get('center_world')
        radius_w = initial_tumor_seed_info.get('radius_world')
        if center_w is not None and radius_w is not None and radius_w > 0:
            logger.info(f"Plotting initial tumor seed sphere at {np.round(center_w,2)} with radius {radius_w:.3f}")
            plotter.add_mesh(pv.Sphere(center=center_w, radius=radius_w), color=tumor_seed_color, opacity=0.5)
            plotter.add_point_labels(center_w + np.array([0,0,radius_w*1.5]), ["Initial Tumor Seed"], font_size=10, text_color=tumor_seed_color)


    # Plot Vascular Graph (if any)
    # ... (The existing graph plotting logic from the previous "complete" visualization.py version) ...
    # ... This includes creating tree_mesh, handling point/cell data for scalars ...
    # ... Make sure to use `default_cmap_radius` from fetched config ...
    tree_mesh_bounds_logged = False
    if graph is not None and graph.number_of_nodes() > 0:
        points, lines, node_to_idx, idx_counter = [], [], {}, 0
        point_data_arrays: Dict[str, List[float]] = {'radius': [], 'pressure': []}
        edge_data_arrays: Dict[str, List[float]] = {'flow_solver': []}
        min_voxel_dim = np.min(spacing_for_markers) if np.any(spacing_for_markers > 0) else 0.01
        min_plot_radius = max(constants.MIN_VESSEL_RADIUS_MM * 0.1, min_voxel_dim * 0.05, 1e-4)

        for node_id, data in graph.nodes(data=True):
            if 'pos' in data and np.all(np.isfinite(data['pos'])):
                points.append(data['pos'])
                point_data_arrays['radius'].append(max(data.get('radius', min_plot_radius), min_plot_radius) if np.isfinite(data.get('radius', min_plot_radius)) else min_plot_radius)
                point_data_arrays['pressure'].append(data.get('pressure', np.nan))
                node_to_idx[node_id] = idx_counter; idx_counter += 1
        if not points: logger.warning("No valid nodes with positions in graph for tree plotting.")
        else:
            points_np = np.array(points)
            for u, v, data in graph.edges(data=True):
                if u in node_to_idx and v in node_to_idx:
                    lines.extend([2, node_to_idx[u], node_to_idx[v]])
                    edge_data_arrays['flow_solver'].append(data.get('flow_solver', np.nan))
            tree_mesh = pv.PolyData()
            if points_np.shape[0] > 0:
                tree_mesh.points = points_np
                for key, arr in point_data_arrays.items():
                    if arr and len(arr) == tree_mesh.n_points: tree_mesh.point_data[key] = np.array(arr)
                logger.info(f"Vascular tree mesh for plot: {tree_mesh.n_points} points. Bounds: {tree_mesh.bounds}")
                tree_mesh_bounds_logged = True
                if lines:
                    tree_mesh.lines = np.array(lines)
                    if tree_mesh.n_cells > 0:
                        for key, arr in edge_data_arrays.items():
                             if arr and len(arr) == tree_mesh.n_cells: tree_mesh.cell_data[key] = np.array(arr)
                        active_scalars_on_points = color_by_scalar in tree_mesh.point_data and np.any(np.isfinite(tree_mesh.point_data[color_by_scalar]))
                        active_scalars_on_cells = color_by_scalar in tree_mesh.cell_data and np.any(np.isfinite(tree_mesh.cell_data[color_by_scalar]))
                        sargs = {'title': scalar_bar_title_override if scalar_bar_title_override else color_by_scalar.replace("_"," ").title(), 'color':'black', 'vertical':True, 'position_y': 0.05, 'position_x': 0.85, 'height': 0.3, 'n_labels': 5}
                        current_cmap_actual = custom_cmap if custom_cmap else (default_cmap_radius if color_by_scalar == 'radius' else 'coolwarm')
                        preference = 'point' if active_scalars_on_points else ('cell' if active_scalars_on_cells else None)
                        if preference:
                            plotter.add_mesh(tree_mesh, scalars=color_by_scalar, line_width=3, cmap=current_cmap_actual, render_lines_as_tubes=True, scalar_bar_args=sargs, preference=preference)
                        else:
                            if 'radius' in tree_mesh.point_data and np.any(np.isfinite(tree_mesh.point_data['radius'])):
                                plotter.add_mesh(tree_mesh, scalars='radius', line_width=3, cmap=default_cmap_radius, render_lines_as_tubes=True, scalar_bar_args={'title': 'Radius (mm)', **sargs})
                            else: plotter.add_mesh(tree_mesh, color="darkgrey", line_width=2, render_lines_as_tubes=True)
                    # ... (Glyphing for points only if no lines was here, can be added back if needed) ...
    if not tree_mesh_bounds_logged: logger.info("No vascular graph provided or it was empty.")

    plotter.camera_position = 'iso'
    plotter.enable_parallel_projection()
    plotter.add_axes(interactive=True)
    if output_screenshot_path: plotter.show(auto_close=True, screenshot=output_screenshot_path)
    else: plotter.show()


# Modify generate_final_visualizations or add a new function for initial setup plot
def visualize_initial_setup(
    config: dict, output_dir: str, tissue_data: dict, 
    initial_arterial_graph: Optional[nx.DiGraph]
    ):
    logger.info("Generating visualization of initial simulation setup...")
    
    masks_to_plot: Dict[str, Tuple[np.ndarray, np.ndarray]] = {}
    mask_keys_to_plot = ['domain_mask', 'GM', 'WM', 'CSF', 'Tumor_Max_Extent']
    for key in mask_keys_to_plot:
        if tissue_data.get(key) is not None and np.any(tissue_data[key]) and tissue_data.get('affine') is not None:
            masks_to_plot[key] = (tissue_data[key], tissue_data['affine'])
            logger.info(f"Added '{key}' to initial setup plot.")

    gbo_seeds_viz_data: List[Tuple[np.ndarray, float, str]] = []
    if not initial_arterial_graph or initial_arterial_graph.number_of_nodes() == 0: # Only plot config seeds if no VTP graph
        config_gbo_seeds = config_manager.get_param(config, "gbo_growth.seed_points", [])
        if config_gbo_seeds and isinstance(config_gbo_seeds, list):
            for i, seed_info in enumerate(config_gbo_seeds):
                if isinstance(seed_info, dict) and 'position' in seed_info:
                    gbo_seeds_viz_data.append((np.array(seed_info['position']),
                                               float(seed_info.get('initial_radius',0.1)),
                                               seed_info.get('id',f"GBO_Seed_{i}")))

    initial_tumor_seed_plot_info: Optional[Dict] = None
    tumor_seed_config = config_manager.get_param(config, "tumor_angiogenesis.initial_tumor_seed", {})
    if tissue_data.get('Tumor_Max_Extent') is not None and np.any(tissue_data['Tumor_Max_Extent']):
        center_ijk = tumor_seed_config.get('center_voxel_ijk_relative_to_image_grid')
        radius_vox = tumor_seed_config.get('radius_voxels')
        if center_ijk and radius_vox and tissue_data.get('affine') is not None:
            center_world = utils.voxel_to_world(np.array(center_ijk), tissue_data['affine'])[0]
            # Approximate world radius - this is tricky as voxels can be anisotropic
            # For visualization, average voxel spacing * radius_vox is a decent estimate
            avg_voxel_spacing = np.mean(np.abs(np.diag(tissue_data['affine'])[:3]))
            radius_world = radius_vox * avg_voxel_spacing
            initial_tumor_seed_plot_info = {'center_world': center_world, 'radius_world': radius_world}

    plot_title = "Initial Simulation Setup: Masks, Arterial Tree/Seeds, Tumor Seed"
    screenshot_path = os.path.join(output_dir, "initial_setup_visualization.png")

    if PYVISTA_AVAILABLE:
        plot_vascular_tree_pyvista(
            graph=initial_arterial_graph, # Plot the parsed VTP tree if available
            title=plot_title,
            output_screenshot_path=screenshot_path,
            tissue_masks=masks_to_plot,
            gbo_seed_points_world=gbo_seeds_viz_data,
            initial_tumor_seed_info=initial_tumor_seed_plot_info,
            color_by_scalar='radius', # Color initial tree by radius
            config_for_viz_params=config # Pass config for viz params
        )
    else:
        logger.warning("PyVista not available, skipping initial setup 3D plot.")


def generate_final_visualizations(
    config: dict, output_dir: str, tissue_data: dict, vascular_graph: Optional[nx.DiGraph],
    perfusion_map: Optional[np.ndarray] = None, pressure_map_tissue: Optional[np.ndarray] = None,
    plot_context_masks: bool = True
    ):
    # ... (This function remains largely the same as the one from the previous "complete" response)
    # ... (It will call plot_vascular_tree_pyvista for radius, flow, pressure plots of the FINAL graph)
    # ... (Ensure it passes `config_for_viz_params=config` to plot_vascular_tree_pyvista)
    logger.info("Generating final visualizations and quantitative analyses...")
    masks_to_plot_for_pyvista: Optional[Dict[str, Tuple[np.ndarray, np.ndarray]]] = None
    if plot_context_masks:
        masks_to_plot_for_pyvista = {}
        # Plot active tumor for final viz, not Tumor_Max_Extent unless specifically desired
        mask_keys = ['domain_mask', 'GM', 'WM', 'CSF', 'Tumor'] # 'Tumor' here is the active one
        for key in mask_keys:
            if tissue_data.get(key) is not None and np.any(tissue_data[key]) and tissue_data.get('affine') is not None:
                masks_to_plot_for_pyvista[key] = (tissue_data[key], tissue_data['affine'])
        if not masks_to_plot_for_pyvista: masks_to_plot_for_pyvista = None
    else: logger.info("Context mask plotting disabled for final visualizations.")

    # GBO seeds are less relevant for final plot if tree exists, but tumor seed might be if small
    # For simplicity, let's not replot GBO seeds here if a tree exists.
    # Tumor seed sphere could be plotted if desired, using its initial config.

    if vascular_graph and vascular_graph.number_of_nodes() > 0:
        # ... (Saving VTP, quantitative analyses - as before) ...
        final_tree_vtp_path = os.path.join(output_dir, "final_plot_vascular_tree.vtp")
        io_utils.save_vascular_tree_vtp(vascular_graph, final_tree_vtp_path, radius_attr='radius', pressure_attr='pressure', flow_attr='flow_solver')
        logger.info(f"Final vascular tree saved for analysis: {final_tree_vtp_path}")
        analyze_radii_distribution(vascular_graph, output_dir)
        analyze_segment_lengths(vascular_graph, output_dir)
        analyze_bifurcation_geometry(vascular_graph, output_dir, murray_exponent=config_manager.get_param(config, "vascular_properties.murray_law_exponent", 3.0))
        analyze_degree_distribution(vascular_graph, output_dir)
        analyze_network_connectivity(vascular_graph, output_dir)
        analyze_volumetric_densities(vascular_graph, tissue_data, output_dir)

        if PYVISTA_AVAILABLE:
            plot_suffix = "" if plot_context_masks else "_no_context"
            # Radius Plot
            pv_plot_title_radius = f"Final Vasculature (Radius, {vascular_graph.number_of_nodes()}N, {vascular_graph.number_of_edges()}E)"
            pv_screenshot_path_radius = os.path.join(output_dir, f"final_vascular_tree_radius_3D{plot_suffix}.png")
            plot_vascular_tree_pyvista(graph=vascular_graph, title=pv_plot_title_radius, output_screenshot_path=pv_screenshot_path_radius,
                                       tissue_masks=masks_to_plot_for_pyvista, color_by_scalar='radius', config_for_viz_params=config)
            # Flow Plot
            if any('flow_solver' in data for _,_,data in vascular_graph.edges(data=True) if 'flow_solver' in data and np.any(np.isfinite(data['flow_solver']))):
                pv_plot_title_flow = f"Final Vasculature (Edge Flow)"
                pv_screenshot_path_flow = os.path.join(output_dir, f"final_vascular_tree_flow_3D{plot_suffix}.png")
                plot_vascular_tree_pyvista(graph=vascular_graph, title=pv_plot_title_flow, output_screenshot_path=pv_screenshot_path_flow,
                                           tissue_masks=masks_to_plot_for_pyvista, color_by_scalar='flow_solver', custom_cmap='coolwarm', 
                                           scalar_bar_title_override="Flow (mm³/s)", config_for_viz_params=config)
            # Pressure Plot
            if any('pressure' in data for _,data in vascular_graph.nodes(data=True) if 'pressure' in data and np.any(np.isfinite(data['pressure']))):
                pv_plot_title_pressure = f"Final Vasculature (Node Pressure)"
                pv_screenshot_path_pressure = os.path.join(output_dir, f"final_vascular_tree_pressure_3D{plot_suffix}.png")
                plot_vascular_tree_pyvista(graph=vascular_graph, title=pv_plot_title_pressure, output_screenshot_path=pv_screenshot_path_pressure,
                                           tissue_masks=masks_to_plot_for_pyvista, color_by_scalar='pressure', custom_cmap='coolwarm', 
                                           scalar_bar_title_override="Pressure (Pa)", config_for_viz_params=config)
    # ... (rest of generate_final_visualizations)
    logger.info("Final visualizations and analyses generation complete.")


def generate_final_visualizations(
    config: dict, output_dir: str, tissue_data: dict, vascular_graph: Optional[nx.DiGraph],
    perfusion_map: Optional[np.ndarray] = None, pressure_map_tissue: Optional[np.ndarray] = None,
    plot_context_masks: bool = True # New argument for controlling mask plotting
    ):
    logger.info("Generating final visualizations and quantitative analyses...")
    
    masks_to_plot_for_pyvista: Optional[Dict[str, Tuple[np.ndarray, np.ndarray]]]
    if plot_context_masks:
        masks_to_plot_for_pyvista = {}
        if tissue_data.get('domain_mask') is not None and tissue_data.get('affine') is not None:
            masks_to_plot_for_pyvista["domain_mask"] = (tissue_data['domain_mask'], tissue_data['affine'])
        if tissue_data.get('GM') is not None and tissue_data.get('affine') is not None:
            masks_to_plot_for_pyvista["GM"] = (tissue_data['GM'], tissue_data['affine'])
        if tissue_data.get('WM') is not None and tissue_data.get('affine') is not None:
            masks_to_plot_for_pyvista["WM"] = (tissue_data['WM'], tissue_data['affine'])
        if tissue_data.get('Tumor') is not None and tissue_data.get('affine') is not None:
            masks_to_plot_for_pyvista["Tumor"] = (tissue_data['Tumor'], tissue_data['affine'])
        if not masks_to_plot_for_pyvista: # If dict is still empty
            logger.info("No valid masks found in tissue_data to plot for context.")
            masks_to_plot_for_pyvista = None # Explicitly set to None
    else:
        logger.info("Context mask plotting is disabled for this visualization call.")
        masks_to_plot_for_pyvista = None


    seed_points_viz_data: List[Tuple[np.ndarray, float, str]] = []
    config_seeds = config_manager.get_param(config, "gbo_growth.seed_points", [])
    if config_seeds and isinstance(config_seeds, list):
        for i, seed_info in enumerate(config_seeds):
            if isinstance(seed_info, dict) and 'position' in seed_info:
                seed_points_viz_data.append((np.array(seed_info.get('position')),
                                             float(seed_info.get('initial_radius',0.1)),
                                             seed_info.get('id',f"S_{i}")))
            else: logger.warning(f"Skipping invalid seed_info in config: {seed_info}")

    if vascular_graph and vascular_graph.number_of_nodes() > 0:
        final_tree_vtp_path = os.path.join(output_dir, "final_plot_vascular_tree.vtp")
        io_utils.save_vascular_tree_vtp(vascular_graph, final_tree_vtp_path,
                                        radius_attr='radius', pressure_attr='pressure', flow_attr='flow_solver')
        logger.info(f"Final vascular tree saved for analysis: {final_tree_vtp_path}")

        analyze_radii_distribution(vascular_graph, output_dir)
        analyze_segment_lengths(vascular_graph, output_dir)
        analyze_bifurcation_geometry(vascular_graph, output_dir,
                                     murray_exponent=config_manager.get_param(config, "vascular_properties.murray_law_exponent", 3.0))
        analyze_degree_distribution(vascular_graph, output_dir)
        analyze_network_connectivity(vascular_graph, output_dir)
        analyze_volumetric_densities(vascular_graph, tissue_data, output_dir)

        # Plotting with Radius (always try this if graph exists)
        pv_plot_title_radius = f"Vasculature (Radius, {vascular_graph.number_of_nodes()}N, {vascular_graph.number_of_edges()}E)"
        pv_screenshot_path_radius = os.path.join(output_dir, f"final_vascular_tree_radius_3D{'' if plot_context_masks else '_no_context'}.png")
        if PYVISTA_AVAILABLE:
            plot_vascular_tree_pyvista(
                graph=vascular_graph, title=pv_plot_title_radius, output_screenshot_path=pv_screenshot_path_radius,
                tissue_masks=masks_to_plot_for_pyvista, seed_points_world=seed_points_viz_data,
                color_by_scalar='radius', custom_cmap=config_manager.get_param(config, "visualization.pyvista_cmap_radius", "viridis")
            )

            # Plotting with Flow
            if any('flow_solver' in data for _,_,data in vascular_graph.edges(data=True) if 'flow_solver' in data and np.any(np.isfinite(data['flow_solver']))):
                pv_plot_title_flow = f"Vasculature (Edge Flow, {vascular_graph.number_of_nodes()}N, {vascular_graph.number_of_edges()}E)"
                pv_screenshot_path_flow = os.path.join(output_dir, f"final_vascular_tree_flow_3D{'' if plot_context_masks else '_no_context'}.png")
                plot_vascular_tree_pyvista(
                    graph=vascular_graph, title=pv_plot_title_flow, output_screenshot_path=pv_screenshot_path_flow,
                    tissue_masks=masks_to_plot_for_pyvista, seed_points_world=seed_points_viz_data,
                    color_by_scalar='flow_solver', custom_cmap='coolwarm', scalar_bar_title_override="Flow (mm³/s)"
                )
            else: logger.info("Skipping flow plot: No valid 'flow_solver' data on edges.")

            # Plotting with Pressure
            if any('pressure' in data for _,data in vascular_graph.nodes(data=True) if 'pressure' in data and np.any(np.isfinite(data['pressure']))):
                pv_plot_title_pressure = f"Vasculature (Node Pressure, {vascular_graph.number_of_nodes()}N, {vascular_graph.number_of_edges()}E)"
                pv_screenshot_path_pressure = os.path.join(output_dir, f"final_vascular_tree_pressure_3D{'' if plot_context_masks else '_no_context'}.png")
                plot_vascular_tree_pyvista(
                    graph=vascular_graph, title=pv_plot_title_pressure, output_screenshot_path=pv_screenshot_path_pressure,
                    tissue_masks=masks_to_plot_for_pyvista, seed_points_world=seed_points_viz_data,
                    color_by_scalar='pressure', custom_cmap='coolwarm', scalar_bar_title_override="Pressure (Pa)"
                )
            else: logger.info("Skipping pressure plot: No valid 'pressure' data on nodes.")
        else: logger.warning("PyVista not available, skipping 3D plots.")
    else: # No valid vascular_graph
        logger.warning("Vascular graph is empty or None. Skipping VTP save and quantitative analyses.")
        if PYVISTA_AVAILABLE:
            plot_vascular_tree_pyvista(None, title=f"Tissue Context & Seeds (No Vasculature){' (No Context Masks)' if not plot_context_masks else ''}",
                                       output_screenshot_path=os.path.join(output_dir, f"context_only_plot{'' if plot_context_masks else '_no_context'}.png"),
                                       tissue_masks=masks_to_plot_for_pyvista, seed_points_world=seed_points_viz_data)

    logger.info("Final visualizations and analyses generation complete.")# src/angiogenesis.py
from __future__ import annotations # Must be first line

import numpy as np
import networkx as nx
import logging
import os
from scipy.spatial import KDTree
from scipy.ndimage import binary_erosion, binary_dilation, gaussian_filter
from typing import Tuple, List, Dict, Optional, Set, Callable

from src import utils, data_structures, constants, config_manager, io_utils
from src.vascular_growth import GBOIterationData

logger = logging.getLogger(__name__)

DEFAULT_RIM_THICKNESS_VOXELS = 3
DEFAULT_VEGF_PRODUCTION_RIM = 1.0
DEFAULT_MIN_TUMOR_TERMINAL_DEMAND = 1e-5

# --- Tumor Morphology and State Update Functions ---
# (initialize_active_tumor_from_seed, update_tumor_rim_and_core,
#  update_metabolic_demand_for_tumor, update_vegf_field_rim_driven,
#  grow_tumor_mass_within_defined_segmentation, coopt_and_modify_vessels,
#  find_angiogenic_sprouting_candidates - These functions remain the same as the previous complete version)
# For brevity, I will assume these functions are present as in the previous complete response.
# If you need them explicitly here, let me know. I'll paste them from the previous version.

# --- PASTE THE FOLLOWING FUNCTIONS FROM THE PREVIOUS RESPONSE HERE ---
# initialize_active_tumor_from_seed
# update_tumor_rim_and_core
# update_metabolic_demand_for_tumor
# update_vegf_field_rim_driven
# grow_tumor_mass_within_defined_segmentation
# coopt_and_modify_vessels
# find_angiogenic_sprouting_candidates
# --- END OF PASTED FUNCTIONS ---

# --- Angiogenesis Core Functions (Sprouting, Growth, Co-option) ---
# (Functions like find_angiogenic_sprouting_candidates, coopt_and_modify_vessels are assumed to be here
#  from the previous "complete" version provided)

# --- PASTE find_angiogenic_sprouting_candidates and coopt_and_modify_vessels here from previous response ---
def initialize_active_tumor_from_seed(tissue_data: dict, config: dict) -> bool:
    """
    Initializes a small 'active' tumor (tissue_data['Tumor'])
    within the bounds of a pre-defined 'Tumor_Max_Extent'.
    If specific seed coordinates are not provided in config, it attempts to find a seed automatically.
    """
    tumor_max_extent_array = tissue_data.get('Tumor_Max_Extent')
    if tumor_max_extent_array is None or not np.any(tumor_max_extent_array):
        logger.error("Cannot initialize seed: 'Tumor_Max_Extent' not found, is None, or is empty in tissue_data.")
        if 'shape' in tissue_data and tissue_data['shape'] is not None:
            tissue_data['Tumor'] = np.zeros(tissue_data['shape'], dtype=bool)
        return False

    tumor_seed_config = config_manager.get_param(config, "tumor_angiogenesis.initial_tumor_seed", {})
    seed_strategy = tumor_seed_config.get("strategy", "auto_center") # New: "auto_center", "auto_random", "manual"
    radius_vox = tumor_seed_config.get('radius_voxels', 3)
    shape = tissue_data['shape']
    center_vox_ijk = None

    if seed_strategy == "manual":
        center_vox_ijk_manual = tumor_seed_config.get('center_voxel_ijk_relative_to_image_grid')
        if not center_vox_ijk_manual or not isinstance(center_vox_ijk_manual, list) or len(center_vox_ijk_manual) != 3:
            logger.error("Tumor seed strategy is 'manual' but 'center_voxel_ijk_relative_to_image_grid' not properly defined. Aborting seed.")
            tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
            return False
        center_vox_ijk = np.array(center_vox_ijk_manual)
        if not utils.is_voxel_in_bounds(center_vox_ijk, shape):
             logger.error(f"Manual tumor seed center {center_vox_ijk} is outside image dimensions {shape}.")
             tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
             return False
    elif seed_strategy == "auto_center":
        from scipy.ndimage import center_of_mass
        if np.any(tumor_max_extent_array):
            # Calculate the center of mass of the Tumor_Max_Extent mask
            # Ensure it's integer coordinates and within bounds
            com_float = center_of_mass(tumor_max_extent_array)
            center_vox_ijk = np.round(com_float).astype(int)
            # Clip to ensure it's within array bounds strictly (for safety with radius)
            for d in range(3):
                center_vox_ijk[d] = np.clip(center_vox_ijk[d], radius_vox, shape[d] - 1 - radius_vox)
            logger.info(f"Automatic seed strategy 'auto_center': calculated COM {com_float}, using seed center {center_vox_ijk}.")
        else: # Should have been caught by the first check, but for safety
            logger.error("'auto_center' seed strategy failed: Tumor_Max_Extent is empty.")
            tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
            return False
    elif seed_strategy == "auto_random":
        true_indices = np.array(np.where(tumor_max_extent_array)).T
        if true_indices.shape[0] > 0:
            random_idx = np.random.choice(true_indices.shape[0])
            center_vox_ijk = true_indices[random_idx]
            logger.info(f"Automatic seed strategy 'auto_random': selected random seed center {center_vox_ijk}.")
        else: # Should have been caught, but for safety
            logger.error("'auto_random' seed strategy failed: Tumor_Max_Extent is empty.")
            tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
            return False
    else:
        logger.error(f"Unknown tumor seed strategy: {seed_strategy}. Choose 'manual', 'auto_center', or 'auto_random'.")
        tissue_data['Tumor'] = np.zeros(shape, dtype=bool)
        return False

    # Create a spherical seed mask at the determined center
    coords = np.ogrid[:shape[0], :shape[1], :shape[2]]
    distance_sq = ((coords[0] - center_vox_ijk[0])**2 +
                   (coords[1] - center_vox_ijk[1])**2 +
                   (coords[2] - center_vox_ijk[2])**2)
    initial_seed_mask = distance_sq <= radius_vox**2

    # Active tumor is the seed AND where it overlaps with the max extent
    tissue_data['Tumor'] = initial_seed_mask & tumor_max_extent_array

    if not np.any(tissue_data['Tumor']):
        logger.warning(f"Initial tumor seed (strategy: {seed_strategy}) at {center_vox_ijk} with radius {radius_vox} "
                       f"resulted in an empty active tumor region within Tumor_Max_Extent. "
                       f"This might happen if Tumor_Max_Extent is very thin or small at the chosen seed location, "
                       f"or if the seed radius is too small to capture any 'True' voxels of Tumor_Max_Extent.")
        # tissue_data['Tumor'] is already an all-False array of the right shape here
        return False

    logger.info(f"Initialized active tumor seed (strategy: {seed_strategy}) within Tumor_Max_Extent: "
                f"center_vox={center_vox_ijk}, radius_vox={radius_vox}, "
                f"num_active_tumor_voxels={np.sum(tissue_data['Tumor'])}")
    return True

def update_tumor_rim_and_core(tissue_data: dict, config: dict):
    """Identifies tumor rim and core based on current active tissue_data['Tumor'] mask."""
    active_tumor_mask_local = tissue_data.get('Tumor') # Get the array first
    if active_tumor_mask_local is None or not np.any(active_tumor_mask_local): # Check if None or all False
        # Ensure these keys exist even if tumor is empty, to prevent KeyErrors later
        if 'shape' in tissue_data and tissue_data['shape'] is not None:
            tissue_data['tumor_rim_mask'] = np.zeros(tissue_data['shape'], dtype=bool)
            tissue_data['tumor_core_mask'] = np.zeros(tissue_data['shape'], dtype=bool)
        else:
            logger.error("Cannot initialize empty rim/core masks as tissue_data['shape'] is missing.")
        return

    # Now we know active_tumor_mask_local is a valid NumPy array with at least one True value
    active_tumor_mask_bool = active_tumor_mask_local.astype(bool) # Ensure boolean for morphology operations
    params = config_manager.get_param(config, "tumor_angiogenesis.tumor_morphology", {})
    rim_thickness = params.get("rim_thickness_voxels", DEFAULT_RIM_THICKNESS_VOXELS)

    if rim_thickness <= 0: # No rim, all core (or all rim if tumor is very small and erosion makes it disappear)
        # If no rim thickness, the whole active tumor is considered rim for VEGF production,
        # and there's no distinct core by erosion.
        # Or, another interpretation: all is core. Let's assume all is rim for VEGF.
        eroded_core = np.zeros_like(active_tumor_mask_bool)
        # If tumor is smaller than rim_thickness, erosion might make it disappear.
        # In such a case, consider the whole tumor as rim.
        if np.sum(active_tumor_mask_bool) > 0 and np.sum(binary_erosion(active_tumor_mask_bool, iterations=rim_thickness, border_value=0)) == 0:
             # If erosion results in nothing, but tumor exists, the whole thing is effectively rim
             tissue_data['tumor_rim_mask'] = active_tumor_mask_bool.copy()
             tissue_data['tumor_core_mask'] = np.zeros_like(active_tumor_mask_bool)
        else:
             eroded_core = binary_erosion(active_tumor_mask_bool, iterations=rim_thickness, border_value=0)
             tissue_data['tumor_core_mask'] = eroded_core
             tissue_data['tumor_rim_mask'] = active_tumor_mask_bool & (~eroded_core)

    else: # rim_thickness > 0
        eroded_core = binary_erosion(active_tumor_mask_bool, iterations=rim_thickness, border_value=0)
        tissue_data['tumor_core_mask'] = eroded_core
        tissue_data['tumor_rim_mask'] = active_tumor_mask_bool & (~eroded_core)
    
    logger.debug(f"Updated tumor rim ({np.sum(tissue_data.get('tumor_rim_mask', np.array([])))} vox) and core ({np.sum(tissue_data.get('tumor_core_mask', np.array([])))} vox).")

def update_metabolic_demand_for_tumor(tissue_data: dict, config: dict):
    """Updates metabolic_demand_map based on tumor rim and core."""
    active_tumor_mask_local = tissue_data.get('Tumor') # Get the array first
    if active_tumor_mask_local is None or not np.any(active_tumor_mask_local): # Check if None or all False
        # No active tumor, so no specific tumor metabolic demand to set.
        # The metabolic_demand_map should reflect healthy tissue or be zero in these areas.
        logger.debug("No active tumor present; skipping tumor-specific metabolic demand update.")
        return

    # Ensure metabolic_demand_map exists and has the correct shape
    if 'metabolic_demand_map' not in tissue_data or \
       tissue_data.get('metabolic_demand_map') is None or \
       tissue_data['metabolic_demand_map'].shape != tissue_data['shape']:
        # If it's missing or wrong shape, it should have been initialized correctly in load_initial_data
        # or after the first tumor growth step. For safety, ensure it's there.
        if 'shape' in tissue_data and tissue_data['shape'] is not None:
            tissue_data['metabolic_demand_map'] = np.zeros(tissue_data['shape'], dtype=float)
            logger.warning("Re-initialized metabolic_demand_map during tumor demand update due to mismatch or absence.")
        else:
            logger.error("Cannot update metabolic demand for tumor: tissue_data['shape'] is missing.")
            return


    rates = config_manager.get_param(config, "tissue_properties.metabolic_rates", {})
    q_met_rim = rates.get("tumor_rim", constants.Q_MET_TUMOR_RIM_PER_ML)
    q_met_core = rates.get("tumor_core", constants.Q_MET_TUMOR_CORE_PER_ML)
    voxel_vol = tissue_data['voxel_volume']

    # metabolic_demand_map should already exist and be initialized (e.g. with healthy demands)
    # We are now OVERWRITING the demand in tumor regions.

    if tissue_data.get('tumor_rim_mask') is not None and np.any(tissue_data['tumor_rim_mask']):
        tissue_data['metabolic_demand_map'][tissue_data['tumor_rim_mask']] = q_met_rim * voxel_vol
    
    if tissue_data.get('tumor_core_mask') is not None and np.any(tissue_data['tumor_core_mask']):
        tissue_data['metabolic_demand_map'][tissue_data['tumor_core_mask']] = q_met_core * voxel_vol
    
    # What about active tumor voxels that are neither rim nor core (e.g., if rim_thickness is 0 or tumor is tiny)?
    # The current update_tumor_rim_and_core logic tries to ensure rim+core covers the active tumor.
    # If there are any 'Tumor' voxels not covered by rim or core (shouldn't happen with current logic),
    # their metabolic rate would remain unchanged (i.e., healthy rate).
    # This is generally fine, as rim/core should define the tumor's metabolic activity.

    logger.debug("Updated metabolic demand map for active tumor regions (rim/core).")

def update_vegf_field_rim_driven(tissue_data: dict, config: dict) -> bool:
    """VEGF produced primarily by the tumor rim. Updates tissue_data['VEGF_field']."""
    tumor_rim_mask_local = tissue_data.get('tumor_rim_mask') # Get the array first
    if tumor_rim_mask_local is None or not np.any(tumor_rim_mask_local): # Check if None or all False
        logger.debug("No tumor rim to produce VEGF. Setting VEGF field to zero.")
        # Ensure VEGF_field exists even if empty
        if 'shape' in tissue_data and tissue_data['shape'] is not None:
            tissue_data['VEGF_field'] = np.zeros(tissue_data['shape'], dtype=float)
        else:
            logger.error("Cannot initialize empty 'VEGF_field' as tissue_data['shape'] is missing.")
        return True # Or False if this state is considered an error for VEGF generation

    # Now we know tumor_rim_mask_local is a valid NumPy array with at least one True value
    vegf_config = config_manager.get_param(config, "tumor_angiogenesis.vegf_settings", {})
    vegf_prod_rim = vegf_config.get("production_rate_rim", DEFAULT_VEGF_PRODUCTION_RIM)
    
    vegf_field = np.zeros(tissue_data['shape'], dtype=float) # Initialize with correct shape
    vegf_field[tumor_rim_mask_local] = vegf_prod_rim # Use the fetched local variable
    
    # Optional: Core contribution (if you add this logic back)
    # vegf_prod_core = vegf_config.get("production_rate_core", DEFAULT_VEGF_PRODUCTION_CORE)
    # tumor_core_mask_local = tissue_data.get('tumor_core_mask')
    # if tumor_core_mask_local is not None and np.any(tumor_core_mask_local):
    #     vegf_field[tumor_core_mask_local] += vegf_prod_core # Additive or max?

    if vegf_config.get("apply_diffusion_blur", True):
        sigma = vegf_config.get("diffusion_blur_sigma", 2.5)
        if sigma > 0 and np.any(vegf_field): # Only blur if there's something to blur
            try:
                vegf_field = gaussian_filter(vegf_field, sigma=sigma)
                logger.debug(f"Applied Gaussian blur (sigma={sigma}) to VEGF field.")
            except Exception as e: # Catch potential errors from gaussian_filter if field is weird
                logger.warning(f"Could not apply Gaussian blur to VEGF field: {e}")
    
    tissue_data['VEGF_field'] = vegf_field
    logger.info(f"Updated rim-driven VEGF field. Max VEGF: {np.max(vegf_field) if np.any(vegf_field) else 0:.2e}")
    return True

def grow_tumor_mass_within_defined_segmentation(tissue_data: dict, config: dict) -> bool:
    tumor_growth_params = config_manager.get_param(config, "tumor_angiogenesis.tumor_growth", {})
    expansion_voxels_per_step = tumor_growth_params.get("expansion_voxels_per_step", 100)
    
    # Get masks
    current_active_tumor_mask = tissue_data.get('Tumor')
    tumor_max_extent_mask = tissue_data.get('Tumor_Max_Extent')
    active_tumor_rim_mask = tissue_data.get('tumor_rim_mask') # This is calculated based on 'Tumor'

    # Check for None before np.any or other operations
    if current_active_tumor_mask is None or \
       tumor_max_extent_mask is None or \
       active_tumor_rim_mask is None: # active_tumor_rim_mask could be None if 'Tumor' was None when it was calculated
        logger.error("grow_tumor_mass: One or more required masks (Tumor, Tumor_Max_Extent, tumor_rim_mask) is None.")
        return False

    if np.all(current_active_tumor_mask == tumor_max_extent_mask): # This is fine, compares two arrays
        logger.info("Tumor Growth: Active tumor has filled 'Tumor_Max_Extent'.")
        return False
    
    # Determine source for dilation based on rim or whole active tumor
    source_for_dilation = active_tumor_rim_mask if np.any(active_tumor_rim_mask) else current_active_tumor_mask
    if not np.any(source_for_dilation): # Correctly checks if the chosen source is empty
        logger.info("Tumor Growth: Source for dilation (rim or active tumor) is empty. Cannot grow.")
        return False # Added this return

    # Ensure domain_mask is also valid before using it in the bitwise AND
    domain_mask_for_growth = tissue_data.get('domain_mask')
    if domain_mask_for_growth is None:
        logger.error("grow_tumor_mass: 'domain_mask' is None. Cannot determine growth candidates.")
        return False

    growth_candidates_mask = binary_dilation(source_for_dilation) & \
                             (~current_active_tumor_mask) & \
                             tumor_max_extent_mask & \
                             domain_mask_for_growth # Use the fetched domain_mask

    candidate_voxel_indices = np.array(np.where(growth_candidates_mask)).T
    if candidate_voxel_indices.shape[0] == 0:
        logger.info("Tumor Growth: No suitable healthy voxels for expansion within constraints.")
        return False
        
    num_to_convert = min(expansion_voxels_per_step, candidate_voxel_indices.shape[0])
    if num_to_convert > 0:
        chosen_indices_idx = np.random.choice(candidate_voxel_indices.shape[0], num_to_convert, replace=False)
        voxels_to_add = candidate_voxel_indices[chosen_indices_idx]
        
        new_active_tumor_mask = current_active_tumor_mask.copy()
        gm_mask = tissue_data.get('GM') # Fetch GM and WM once
        wm_mask = tissue_data.get('WM')

        for vox_idx_tuple in map(tuple, voxels_to_add):
            new_active_tumor_mask[vox_idx_tuple] = True
            if gm_mask is not None and utils.is_voxel_in_bounds(vox_idx_tuple, gm_mask.shape) and gm_mask[vox_idx_tuple]: # Check bounds for safety
                gm_mask[vox_idx_tuple] = False
            if wm_mask is not None and utils.is_voxel_in_bounds(vox_idx_tuple, wm_mask.shape) and wm_mask[vox_idx_tuple]: # Check bounds for safety
                wm_mask[vox_idx_tuple] = False
        
        tissue_data['Tumor'] = new_active_tumor_mask
        # Update GM and WM in tissue_data if they were modified
        if gm_mask is not None: tissue_data['GM'] = gm_mask
        if wm_mask is not None: tissue_data['WM'] = wm_mask

        logger.info(f"Tumor Growth: Expanded by {num_to_convert} vox. Active: {np.sum(new_active_tumor_mask)}, Max: {np.sum(tumor_max_extent_mask)}")
        return True
    return False

def coopt_and_modify_vessels(graph: nx.DiGraph, tissue_data: dict, config: dict):
    active_tumor_mask_local = tissue_data.get('Tumor') # Get the array
    if active_tumor_mask_local is None or not np.any(active_tumor_mask_local): # Corrected check
        logger.debug("Co-option: No active tumor to co-opt vessels from.")
        return

    affine = tissue_data.get('affine')
    if affine is None:
        logger.error("Co-option: Affine matrix not found in tissue_data. Cannot perform co-option.")
        return

    cooption_params = config_manager.get_param(config, "tumor_angiogenesis.cooption", {})
    radius_dilation_factor_mean = cooption_params.get("radius_dilation_factor_mean", 1.1)
    radius_dilation_factor_std = cooption_params.get("radius_dilation_factor_std", 0.05)
    permeability_Lp_tumor_factor = cooption_params.get("permeability_Lp_factor_tumor", 10.0)
    
    nodes_coopted_this_step: Set[str] = set()
    for node_id, data in graph.nodes(data=True):
        if data.get('is_tumor_vessel', False): continue
        
        node_pos = data.get('pos')
        if node_pos is None: continue # Skip nodes without position

        pos_vox_int = np.round(utils.world_to_voxel(node_pos, affine)).astype(int)
        if utils.is_voxel_in_bounds(pos_vox_int, active_tumor_mask_local.shape) and \
           active_tumor_mask_local[tuple(pos_vox_int)]:
            nodes_coopted_this_step.add(node_id)
            
    if not nodes_coopted_this_step:
        logger.debug("Co-option: No new healthy vessels found within active tumor for co-option this step.")
        return

    for node_id in nodes_coopted_this_step:
        node_data = graph.nodes[node_id] # Get node data once
        node_data['is_tumor_vessel'] = True
        node_data['vessel_origin_type'] = 'coopted_healthy'
        
        original_radius = node_data.get('radius', constants.MIN_VESSEL_RADIUS_MM) # Use default if radius missing
        dilation = max(0.5, np.random.normal(radius_dilation_factor_mean, radius_dilation_factor_std))
        new_radius = max(constants.MIN_VESSEL_RADIUS_MM, original_radius * dilation)
        node_data['radius'] = new_radius
        # logger.debug(f"Co-opted node {node_id}. Type: {node_data.get('type')}->coopted_healthy. R: {original_radius:.4f}->{new_radius:.4f}") # Already logged in main loop

        # Mark connected edges and update their radii if they originate from this node
        for u, v, edge_data in graph.out_edges(node_id, data=True): # Edges where this node is the source
            edge_data['is_tumor_vessel'] = True
            edge_data['radius'] = new_radius # Edge takes radius of its (now co-opted) source node
            edge_data['permeability_Lp_factor'] = permeability_Lp_tumor_factor
        for u, v, edge_data in graph.in_edges(node_id, data=True): # Edges where this node is the target
            edge_data['is_tumor_vessel'] = True # The segment is now within tumor influence
            edge_data['permeability_Lp_factor'] = permeability_Lp_tumor_factor
            # Radius of incoming edge is determined by its source node (u), which might also get co-opted

    logger.info(f"Co-opted and modified {len(nodes_coopted_this_step)} nodes and their adjacent edges.")

def find_angiogenic_sprouting_candidates(graph: nx.DiGraph, tissue_data: dict, config: dict) -> List[Tuple[str, str, np.ndarray, np.ndarray]]:
    candidates = []
    vegf_field_local = tissue_data.get('VEGF_field') # Get the array
    if vegf_field_local is None or not np.any(vegf_field_local): # Corrected check
        logger.debug("Sprouting candidates: No VEGF field or VEGF field is all zero.")
        return candidates

    affine = tissue_data.get('affine')
    if affine is None:
        logger.error("Sprouting candidates: Affine matrix not found in tissue_data.")
        return candidates

    sprouting_params = config_manager.get_param(config, "tumor_angiogenesis.sprouting", {})
    min_vegf = sprouting_params.get("min_vegf_concentration", 0.2)
    min_parent_r = sprouting_params.get("min_parent_vessel_radius_mm", 0.02)
    
    grad_ax0, grad_ax1, grad_ax2 = np.gradient(vegf_field_local) # Use local copy

    for u, v, edge_data in graph.edges(data=True):
        node_u_data = graph.nodes[u]
        node_v_data = graph.nodes[v] # Get v data too for position

        parent_radius = node_u_data.get('radius', 0) # Use default 0 if radius missing
        if parent_radius < min_parent_r: 
            continue
        
        pos_u = node_u_data.get('pos')
        pos_v = node_v_data.get('pos')
        if pos_u is None or pos_v is None: continue # Skip if positions are missing

        sprout_origin = (pos_u + pos_v) / 2.0
        sprout_vox_int = np.round(utils.world_to_voxel(sprout_origin, affine)).astype(int)

        if not utils.is_voxel_in_bounds(sprout_vox_int, vegf_field_local.shape): 
            continue
            
        if vegf_field_local[tuple(sprout_vox_int)] >= min_vegf:
            g_ax0 = grad_ax0[tuple(sprout_vox_int)]
            g_ax1 = grad_ax1[tuple(sprout_vox_int)]
            g_ax2 = grad_ax2[tuple(sprout_vox_int)]
            sprout_dir_vox = np.array([g_ax0, g_ax1, g_ax2])
            
            # Transform gradient to world space direction
            # affine[:3,:3] is the rotation/scaling part
            sprout_dir_world = utils.normalize_vector(affine[:3,:3] @ sprout_dir_vox) 

            if np.linalg.norm(sprout_dir_world) > constants.EPSILON:
                candidates.append((u, v, sprout_origin, sprout_dir_world))
    
    max_sprouts = sprouting_params.get("max_new_sprouts_per_iteration", 5)
    if len(candidates) > max_sprouts:
        indices = np.random.choice(len(candidates), max_sprouts, replace=False)
        selected_candidates = [candidates[i] for i in indices]
        logger.info(f"Selected {len(selected_candidates)} sprouts from {len(candidates)} original candidates.")
        return selected_candidates
    elif candidates: 
        logger.info(f"Found {len(candidates)} sprouting candidates.")
    else:
        logger.debug("No sprouting candidates found this step.")
    return candidates


def attempt_anastomosis_tip_to_segment(
    term_gbo: GBOIterationData,
    graph: nx.DiGraph, # The main angiogenic graph
    vessel_kdtree: KDTree, # KDTree of (positions of midpoints of all non-parent segments)
    segment_midpoints_data: List[Dict], # List of {'pos': mid_pos, 'u': seg_u, 'v': seg_v, 'radius': seg_radius}
    config: dict,
    next_synthetic_node_id_ref: List[int] # Pass as list to modify in place
) -> bool:
    """
    Attempts to anastomose the given angiogenic terminal (term_gbo) to a nearby existing segment.
    Modifies graph and term_gbo if successful.
    Returns True if anastomosis occurred, False otherwise.
    """
    anastomosis_params = config_manager.get_param(config, "tumor_angiogenesis.anastomosis", {})
    search_radius = term_gbo.radius * anastomosis_params.get("search_radius_factor", 3.0)
    min_angle_deg = anastomosis_params.get("min_fusion_angle_deg", 120.0) # Angle between tip's last segment and segment to target
    max_dist_to_midpoint_factor = anastomosis_params.get("max_dist_to_midpoint_factor", 1.5) # Tip must be close to midpoint

    if vessel_kdtree is None or not segment_midpoints_data: return False

    nearby_indices = vessel_kdtree.query_ball_point(term_gbo.pos, r=search_radius)
    if not nearby_indices: return False

    parent_of_tip_pos = graph.nodes[term_gbo.parent_id]['pos']
    tip_growth_vector = term_gbo.pos - parent_of_tip_pos # Vector of the last segment of the tip

    best_target_seg_info = None
    min_dist_sq = float('inf')

    for idx in nearby_indices:
        target_seg = segment_midpoints_data[idx]
        target_midpoint = target_seg['pos']
        target_u, target_v = target_seg['u'], target_seg['v']

        # Avoid self-anastomosis or anastomosis with immediate parent segment from bifurcation
        if term_gbo.parent_id == target_u or term_gbo.parent_id == target_v: continue
        # Avoid if target is the segment the tip just grew from (if parent_id was a midpoint)
        # This check needs to be more robust if parent_id can be a segment point.
        # For now, assume parent_id is a node.

        dist_sq = utils.distance_squared(term_gbo.pos, target_midpoint)
        if dist_sq < min_dist_sq and dist_sq < (term_gbo.radius * max_dist_to_midpoint_factor)**2 :
            # Check angle: vector from tip to target_midpoint vs. tip_growth_vector
            vec_tip_to_target = target_midpoint - term_gbo.pos
            if np.linalg.norm(tip_growth_vector) > constants.EPSILON and np.linalg.norm(vec_tip_to_target) > constants.EPSILON:
                cos_angle = np.dot(tip_growth_vector, vec_tip_to_target) / \
                            (np.linalg.norm(tip_growth_vector) * np.linalg.norm(vec_tip_to_target))
                angle_deg = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))
                if angle_deg >= min_angle_deg: # Tip should be "aiming away" or sideways relative to target for good fusion
                    min_dist_sq = dist_sq
                    best_target_seg_info = target_seg
    
    if best_target_seg_info:
        target_u = best_target_seg_info['u']
        target_v = best_target_seg_info['v']
        target_midpoint_pos = best_target_seg_info['pos'] # This will be the new anastomosis node

        logger.info(f"Anastomosis: Tip {term_gbo.id} (parent {term_gbo.parent_id}) fusing with segment {target_u}-{target_v} at {np.round(target_midpoint_pos,2)}.")

        # 1. Create new anastomosis node at target_midpoint_pos
        anastomosis_node_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
        # Radius of anastomosis node can be average or based on fusing vessels
        fused_radius = (term_gbo.radius + best_target_seg_info['radius']) / 2.0
        data_structures.add_node_to_graph(graph, anastomosis_node_id, pos=target_midpoint_pos, radius=fused_radius,
                                          type='anastomosis_point', is_tumor_vessel=True, vessel_origin_type='anastomosis')

        # 2. Remove old target segment (target_u, target_v)
        original_target_edge_data = graph.edges[target_u, target_v].copy() # Assuming directed u->v
        graph.remove_edge(target_u, target_v)

        # 3. Add new segments: target_u -> anastomosis_node, anastomosis_node -> target_v
        data_structures.add_edge_to_graph(graph, target_u, anastomosis_node_id, **original_target_edge_data) # Update length/radius
        graph.edges[target_u, anastomosis_node_id]['length'] = utils.distance(graph.nodes[target_u]['pos'], target_midpoint_pos)
        graph.edges[target_u, anastomosis_node_id]['radius'] = graph.nodes[target_u]['radius'] # Takes radius of upstream node

        data_structures.add_edge_to_graph(graph, anastomosis_node_id, target_v, **original_target_edge_data) # Update length/radius
        graph.edges[anastomosis_node_id, target_v]['length'] = utils.distance(target_midpoint_pos, graph.nodes[target_v]['pos'])
        graph.edges[anastomosis_node_id, target_v]['radius'] = fused_radius # Takes radius of new anastomosis node

        # 4. Connect the parent of the fusing tip to the anastomosis_node
        # The segment was (term_gbo.parent_id) -> term_gbo.id (which is at term_gbo.pos)
        # New segment is (term_gbo.parent_id) -> anastomosis_node_id
        tip_parent_node_id = term_gbo.parent_id
        data_structures.add_edge_to_graph(graph, tip_parent_node_id, anastomosis_node_id,
                                          length=utils.distance(graph.nodes[tip_parent_node_id]['pos'], target_midpoint_pos),
                                          radius=graph.nodes[tip_parent_node_id]['radius'], # Or fused_radius?
                                          type='angiogenic_segment', is_tumor_vessel=True,
                                          permeability_Lp_factor=graph.edges[tip_parent_node_id, term_gbo.id].get('permeability_Lp_factor'))


        # 5. Remove the old tip node (term_gbo.id) and its incoming segment
        graph.remove_edge(tip_parent_node_id, term_gbo.id)
        graph.remove_node(term_gbo.id)
        
        term_gbo.stop_growth = True # Mark this GBOIterationData as done
        return True
    return False


# --- Main Angiogenesis Orchestration ---
def simulate_tumor_angiogenesis_fixed_extent(
    config: dict,
    tissue_data: dict,
    base_vascular_tree: nx.DiGraph,
    output_dir: str,
    perfusion_solver_func: Callable[[nx.DiGraph, dict, Optional[float], Optional[Dict[str, float]]], Optional[nx.DiGraph]]
) -> nx.DiGraph:
    logger.info("--- Starting Tumor Angiogenesis Simulation (Growing within Fixed Extent) ---")
    
    main_params = config_manager.get_param(config, "tumor_angiogenesis", {})
    num_macro_iterations = main_params.get("num_macro_iterations", 20)
    tumor_growth_steps_per_macro = main_params.get("tumor_growth.steps_per_macro_iter", 1)
    angiogenesis_steps_per_macro = main_params.get("angiogenesis.steps_per_macro_iter", 3)
    flow_solve_interval_macro = main_params.get("flow_solve_interval_macro_iters", 5)
    save_interval_macro = main_params.get("save_intermediate_interval_macro_iters", 1)

    sprouting_params = main_params.get("sprouting", {})
    initial_sprout_radius = sprouting_params.get("initial_sprout_radius_mm", constants.MIN_VESSEL_RADIUS_MM * 1.1)
    initial_sprout_length = sprouting_params.get("initial_sprout_length_mm", initial_sprout_radius * 4)
    
    extension_params = main_params.get("extension", {})
    extension_step_length = extension_params.get("step_length_mm", initial_sprout_radius * 2)
    
    branching_params = main_params.get("angiogenic_branching", {})
    branch_probability_factor_vegf = branching_params.get("branch_probability_factor_vegf", 0.1) # Prob = factor * vegf_norm
    branch_angle_spread_deg = branching_params.get("branch_angle_spread_deg", 60.0)


    if not initialize_active_tumor_from_seed(tissue_data, config):
        logger.error("Failed to initialize active tumor seed. Aborting angiogenesis.")
        return base_vascular_tree.copy()

    angiogenic_graph = base_vascular_tree.copy()
    # Pass next_synthetic_node_id as a list so its modification is seen by caller
    max_id_num = 0
    for node_id_str_val in angiogenic_graph.nodes():
        if isinstance(node_id_str_val, str) and node_id_str_val.startswith('s_'):
            try: max_id_num = max(max_id_num, int(node_id_str_val.split('_')[1]))
            except (ValueError, IndexError): pass
    next_synthetic_node_id_ref = [max_id_num + 10000] # List to pass by reference
    
    active_angiogenic_terminals: List[GBOIterationData] = []

    for macro_iter in range(num_macro_iterations):
        logger.info(f"===== Macro Iteration {macro_iter + 1} / {num_macro_iterations} =====")

        any_tumor_growth_this_macro = False
        for _ in range(tumor_growth_steps_per_macro):
            update_tumor_rim_and_core(tissue_data, config)
            if grow_tumor_mass_within_defined_segmentation(tissue_data, config): any_tumor_growth_this_macro = True
            else: break 
        
        update_tumor_rim_and_core(tissue_data, config); update_metabolic_demand_for_tumor(tissue_data, config)
        update_vegf_field_rim_driven(tissue_data, config)
        coopt_and_modify_vessels(angiogenic_graph, tissue_data, config)

        for ag_step in range(angiogenesis_steps_per_macro):
            logger.debug(f"  Angiogenesis Step {ag_step + 1} (Active Tips Before Sprouting: {len(active_angiogenic_terminals)})")
            
            # --- 2a. Sprouting ---
            new_sprouts_info = find_angiogenic_sprouting_candidates(angiogenic_graph, tissue_data, config)
            if new_sprouts_info:
                logger.debug(f"    Found {len(new_sprouts_info)} new sprout candidates this AG step.")

            for parent_u, parent_v, sprout_origin, sprout_dir in new_sprouts_info:
                bif_node_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
                
                parent_u_data = angiogenic_graph.nodes[parent_u]
                parent_v_data = angiogenic_graph.nodes[parent_v] # Get parent_v data as well
                parent_u_radius = parent_u_data['radius']
                
                # Determine vessel_origin_type for the new bifurcation node
                # If parent_u was already a tumor vessel (coopted or angiogenic), the bif is too.
                # Otherwise, it's a bifurcation on a healthy vessel that's now leading to tumor growth.
                bif_origin_type = parent_u_data.get('vessel_origin_type', 'healthy_parent_of_sprout')
                if parent_u_data.get('is_tumor_vessel'):
                    bif_origin_type = parent_u_data.get('vessel_origin_type', 'coopted_healthy') # Default if type was missing

                data_structures.add_node_to_graph(
                    angiogenic_graph, bif_node_id,
                    pos=sprout_origin,
                    radius=parent_u_radius, # Bifurcation point takes radius of the parent segment it's on
                    type='angiogenic_bifurcation',
                    is_tumor_vessel=True, # The bifurcation itself is part of the tumor response
                    vessel_origin_type=bif_origin_type
                )
                
                # Get original edge data before removing
                # Important: Check if edge still exists, could have been modified by another sprout from same segment in same AG step (unlikely but good check)
                if not angiogenic_graph.has_edge(parent_u, parent_v):
                    logger.warning(f"Sprouting target edge {parent_u}-{parent_v} no longer exists. Skipping this sprout.")
                    # Rollback bif_node_id? Or just let it be an isolated node that might get pruned.
                    # For now, continue, but this indicates a potential complex interaction.
                    # To properly handle, would need to process sprouts sequentially and update graph immediately.
                    # Current find_angiogenic_sprouting_candidates finds all then processes.
                    if angiogenic_graph.has_node(bif_node_id): angiogenic_graph.remove_node(bif_node_id) # Clean up unused bif
                    next_synthetic_node_id_ref[0] -=1 # Decrement counter
                    continue

                edge_data_orig = angiogenic_graph.edges[parent_u, parent_v].copy()
                angiogenic_graph.remove_edge(parent_u, parent_v)

                # Add new segments: parent_u -> bif_node_id and bif_node_id -> parent_v
                # These new segments inherit properties and get updated lengths/radii
                data_structures.add_edge_to_graph(angiogenic_graph, parent_u, bif_node_id, **edge_data_orig)
                angiogenic_graph.edges[parent_u, bif_node_id]['length'] = utils.distance(parent_u_data['pos'], sprout_origin)
                angiogenic_graph.edges[parent_u, bif_node_id]['radius'] = parent_u_radius # Takes radius of parent_u

                data_structures.add_edge_to_graph(angiogenic_graph, bif_node_id, parent_v, **edge_data_orig)
                angiogenic_graph.edges[bif_node_id, parent_v]['length'] = utils.distance(sprout_origin, parent_v_data['pos'])
                angiogenic_graph.edges[bif_node_id, parent_v]['radius'] = parent_u_radius # New segment from bif also takes bif radius

                # Mark these split segments as tumor vessels if the original parent was, or if the bif is
                # The bifurcation node itself is marked is_tumor_vessel=True
                # Segments connected to it that are part of the original path should also be marked.
                perm_factor_to_set = default_permeability_factor # Default for new tumor-related segments
                if parent_u_data.get('is_tumor_vessel'):
                    # If parent_u was already a tumor vessel, its perm factor might be already set
                    perm_factor_to_set = edge_data_orig.get('permeability_Lp_factor', default_permeability_factor)

                for e_start, e_end in [(parent_u, bif_node_id), (bif_node_id, parent_v)]:
                    angiogenic_graph.edges[e_start, e_end]['is_tumor_vessel'] = True # Part of the angiogenic event path
                    angiogenic_graph.edges[e_start, e_end]['permeability_Lp_factor'] = perm_factor_to_set


                # Create the new angiogenic sprout (terminal node and its GBOIterationData)
                sprout_tip_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
                sprout_tip_pos = sprout_origin + sprout_dir * initial_sprout_length
                
                # Initial flow for angiogenic sprout can be very small or based on a minimal tumor demand
                sprout_initial_flow = DEFAULT_MIN_TUMOR_TERMINAL_DEMAND 
                
                sprout_gbo = GBOIterationData(
                    terminal_id=sprout_tip_id,
                    pos=sprout_tip_pos,
                    radius=initial_sprout_radius,
                    flow=sprout_initial_flow, 
                    parent_id=bif_node_id
                )
                sprout_gbo.length_from_parent = initial_sprout_length
                active_angiogenic_terminals.append(sprout_gbo)

                data_structures.add_node_to_graph(
                    angiogenic_graph, sprout_tip_id,
                    pos=sprout_tip_pos,
                    radius=initial_sprout_radius,
                    type='angiogenic_terminal',
                    is_tumor_vessel=True,
                    vessel_origin_type='angiogenic_sprout', # Clearly mark its origin
                    parent_id=bif_node_id, 
                    Q_flow=sprout_gbo.flow
                )
                data_structures.add_edge_to_graph(
                    angiogenic_graph, bif_node_id, sprout_tip_id,
                    length=initial_sprout_length,
                    radius=initial_sprout_radius, # Edge to new tip takes tip's radius
                    type='angiogenic_segment',
                    is_tumor_vessel=True,
                    permeability_Lp_factor=default_permeability_factor # New angiogenic segments are leaky
                )
                logger.debug(f"    Created new sprout: {sprout_tip_id} from new bif {bif_node_id} on original edge {parent_u}-{parent_v}.")
            
            
            # --- Growth of Active Angiogenic Terminals ---
            next_iter_active_terminals_this_ag_step = []
            
            # Prepare KDTree for anastomosis (only if there are terminals and potential targets)
            vessel_kdtree = None
            segment_midpoints_data = []
            if active_angiogenic_terminals and angiogenic_graph.number_of_edges() > 0:
                midpoints_pos_list = []
                for u, v, data in angiogenic_graph.edges(data=True):
                    # Exclude very new segments connected to active tips to avoid self-anastomosis with parent segment immediately
                    # This check might need refinement
                    is_parent_of_active_tip = any(term.parent_id == u and term.id == v for term in active_angiogenic_terminals)
                    if not is_parent_of_active_tip:
                        mid_pos = (angiogenic_graph.nodes[u]['pos'] + angiogenic_graph.nodes[v]['pos']) / 2.0
                        midpoints_pos_list.append(mid_pos)
                        segment_midpoints_data.append({'pos': mid_pos, 'u': u, 'v': v, 'radius': data.get('radius', constants.MIN_VESSEL_RADIUS_MM)})
                if midpoints_pos_list:
                    vessel_kdtree = KDTree(np.array(midpoints_pos_list))

            newly_branched_terminals_this_ag_step = [] # To hold children from branching
            for term_gbo in active_angiogenic_terminals:
                if term_gbo.stop_growth: continue
                
                # Attempt Anastomosis
                if attempt_anastomosis_tip_to_segment(term_gbo, angiogenic_graph, vessel_kdtree, segment_midpoints_data, config, next_synthetic_node_id_ref):
                    # term_gbo.stop_growth is set by the function
                    continue # Fused, so process next terminal

                # Attempt Branching (Simplified Stochastic)
                current_pos_vox_int = np.round(utils.world_to_voxel(term_gbo.pos, tissue_data['affine'])).astype(int)
                if not utils.is_voxel_in_bounds(current_pos_vox_int, tissue_data['VEGF_field'].shape):
                    term_gbo.stop_growth = True; continue
                
                vegf_at_tip = tissue_data['VEGF_field'][tuple(current_pos_vox_int)]
                normalized_vegf = vegf_at_tip / (np.max(tissue_data['VEGF_field']) + constants.EPSILON)
                prob_branch = branch_probability_factor_vegf * normalized_vegf
                
                if np.random.rand() < prob_branch:
                    logger.debug(f"Angiogenic terminal {term_gbo.id} branching (VEGF: {vegf_at_tip:.2f}, P_branch: {prob_branch:.2f})")
                    # Change current terminal to bifurcation
                    angiogenic_graph.nodes[term_gbo.id]['type'] = 'angiogenic_bifurcation'
                    # Create two new child GBOIterationData objects
                    parent_growth_dir = utils.normalize_vector(term_gbo.pos - angiogenic_graph.nodes[term_gbo.parent_id]['pos'])
                    
                    for i_child in range(2):
                        child_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
                        # Perturb direction slightly
                        angle_offset = np.deg2rad(np.random.uniform(-branch_angle_spread_deg/2, branch_angle_spread_deg/2))
                        # This is a 2D rotation logic, needs proper 3D random vector perturbation
                        # For simplicity, create a random perturbation and add to parent_growth_dir then renormalize
                        random_perturb = utils.normalize_vector(np.random.rand(3) - 0.5) * 0.5 # Scale of perturbation
                        child_dir = utils.normalize_vector(parent_growth_dir + random_perturb)
                        if np.linalg.norm(child_dir) < constants.EPSILON: child_dir = parent_growth_dir # Fallback

                        child_pos = term_gbo.pos + child_dir * extension_step_length # Initial small extension
                        child_radius = term_gbo.radius # Or slightly smaller
                        child_flow = term_gbo.flow / 2 # Split flow (very rough)

                        child_gbo = GBOIterationData(child_id, child_pos, child_radius, child_flow, parent_id=term_gbo.id)
                        child_gbo.length_from_parent = extension_step_length
                        newly_branched_terminals_this_ag_step.append(child_gbo)

                        data_structures.add_node_to_graph(angiogenic_graph, child_id, pos=child_pos, radius=child_radius,
                                                          type='angiogenic_terminal', is_tumor_vessel=True, vessel_origin_type='angiogenic_sprout',
                                                          parent_id=term_gbo.id, Q_flow=child_flow)
                        data_structures.add_edge_to_graph(angiogenic_graph, term_gbo.id, child_id, length=extension_step_length,
                                                          radius=child_radius, type='angiogenic_segment', is_tumor_vessel=True,
                                                          permeability_Lp_factor=cooption_params.get("permeability_Lp_factor_tumor", 10.0))
                    term_gbo.stop_growth = True # Parent tip stops, children take over
                    continue

                # Extension (if not anastomosed or branched)
                grad_ax0_f, grad_ax1_f, grad_ax2_f = np.gradient(tissue_data['VEGF_field'])
                g_ax0 = grad_ax0_f[tuple(current_pos_vox_int)]; g_ax1 = grad_ax1_f[tuple(current_pos_vox_int)]; g_ax2 = grad_ax2_f[tuple(current_pos_vox_int)]
                growth_dir_vox = np.array([g_ax0, g_ax1, g_ax2])
                growth_dir_world = utils.normalize_vector(tissue_data['affine'][:3,:3] @ growth_dir_vox)

                if np.linalg.norm(growth_dir_world) > constants.EPSILON:
                    new_pos = term_gbo.pos + growth_dir_world * extension_step_length
                    old_tip_id = term_gbo.id
                    angiogenic_graph.nodes[old_tip_id]['type'] = 'angiogenic_segment_point'
                    new_tip_id = f"s_{next_synthetic_node_id_ref[0]}"; next_synthetic_node_id_ref[0] += 1
                    
                    term_gbo.id = new_tip_id; term_gbo.pos = new_pos; term_gbo.parent_id = old_tip_id
                    term_gbo.length_from_parent = extension_step_length
                    
                    data_structures.add_node_to_graph(angiogenic_graph, new_tip_id, pos=new_pos, radius=term_gbo.radius,
                                                      type='angiogenic_terminal', is_tumor_vessel=True, vessel_origin_type='angiogenic_sprout',
                                                      parent_id=old_tip_id, Q_flow=term_gbo.flow)
                    data_structures.add_edge_to_graph(angiogenic_graph, old_tip_id, new_tip_id, length=extension_step_length,
                                                      radius=term_gbo.radius, type='angiogenic_segment', is_tumor_vessel=True,
                                                      permeability_Lp_factor=cooption_params.get("permeability_Lp_factor_tumor", 10.0))
                    next_iter_active_terminals_this_ag_step.append(term_gbo)
                else:
                    next_iter_active_terminals_this_ag_step.append(term_gbo) # Stalled, keep for next try
            
            active_angiogenic_terminals = [t for t in next_iter_active_terminals_this_ag_step if not t.stop_growth]
            active_angiogenic_terminals.extend(newly_branched_terminals_this_ag_step) # Add new children from branching

            if not active_angiogenic_terminals and not new_sprouts_info : break # from ag_steps
        
        # --- Flow Solve & Adaptation ---
        if (macro_iter + 1) % flow_solve_interval_macro == 0:
            logger.info(f"Running global flow solver and adaptation (Macro Iter {macro_iter + 1})...")
            # ... (Flow solve and differentiated radius adaptation logic - as in previous complete version) ...
            # This part needs careful Q_flow assignment to all terminals (healthy, coopted, angiogenic)
            all_terminals_in_graph = [nid for nid, data in angiogenic_graph.nodes(data=True) if angiogenic_graph.out_degree(nid) == 0 and angiogenic_graph.in_degree(nid) > 0]
            for term_id in all_terminals_in_graph:
                term_node_data = angiogenic_graph.nodes[term_id]
                term_pos_vox = np.round(utils.world_to_voxel(term_node_data['pos'], tissue_data['affine'])).astype(int)
                demand = term_node_data.get('Q_flow', 0.0) # Keep existing Q_flow if not overridden

                if utils.is_voxel_in_bounds(term_pos_vox, tissue_data['shape']):
                    if tissue_data.get('Tumor') is not None and tissue_data['Tumor'][tuple(term_pos_vox)]:
                        # TODO: Better demand based on local tumor voxel demand sum
                        demand = config_manager.get_param(config, "tumor_angiogenesis.min_tumor_terminal_demand", DEFAULT_MIN_TUMOR_TERMINAL_DEMAND)
                    elif not term_node_data.get('is_tumor_vessel'): # Healthy terminal in healthy tissue
                        # This demand should come from GBO's Voronoi refinement for healthy tissue
                        # For now, if not set, use a default. This part needs robust integration with healthy GBO state.
                        demand = term_node_data.get('Q_flow', constants.INITIAL_TERMINAL_FLOW_Q)
                term_node_data['Q_flow'] = demand
            
            temp_graph_for_solver = angiogenic_graph.copy()
            solved_graph = perfusion_solver_func(temp_graph_for_solver, config, None, None)

            if solved_graph:
                angiogenic_graph = solved_graph
                min_r_healthy = config_manager.get_param(config, "vascular_properties.min_radius")
                k_m = config_manager.get_param(config, "vascular_properties.k_murray_scaling_factor")
                m_exp = config_manager.get_param(config, "vascular_properties.murray_law_exponent")
                adapt_params = main_params.get("adaptation", {})
                tumor_radius_factor = adapt_params.get("tumor_radius_factor", 1.0)
                min_r_tumor = adapt_params.get("min_tumor_vessel_radius_mm", min_r_healthy * 1.1)

                for node_id, data in angiogenic_graph.nodes(data=True):
                    if data.get('type') == 'measured_root' or data.get('is_flow_root'): continue
                    
                    actual_node_flow = 0.0 # Recalculate based on solved edge flows
                    if angiogenic_graph.out_degree(node_id) == 0 and angiogenic_graph.in_degree(node_id) > 0: # Sink
                        for _, _, edge_data_in in angiogenic_graph.in_edges(node_id, data=True):
                            actual_node_flow += abs(edge_data_in.get('flow_solver', 0.0))
                    elif angiogenic_graph.out_degree(node_id) > 0: # Source-like or bifurcation
                        for _, _, edge_data_out in angiogenic_graph.out_edges(node_id, data=True):
                            actual_node_flow += abs(edge_data_out.get('flow_solver', 0.0))
                    
                    if abs(actual_node_flow) > constants.EPSILON:
                        target_r = k_m * (abs(actual_node_flow) ** (1.0 / m_exp))
                        if data.get('is_tumor_vessel'):
                            target_r *= tumor_radius_factor
                            data['radius'] = max(min_r_tumor, target_r)
                        else: data['radius'] = max(min_r_healthy, target_r)
                    else: data['radius'] = min_r_tumor if data.get('is_tumor_vessel') else min_r_healthy
                logger.info("Global flow solve and differentiated radius adaptation complete.")


        if (macro_iter + 1) % save_interval_macro == 0:
            # ... (saving logic as before) ...
            logger.info(f"Saving intermediate state for macro iteration {macro_iter + 1}...")
            io_utils.save_vascular_tree_vtp(angiogenic_graph, os.path.join(output_dir, f"angiogenesis_iter_{macro_iter+1}.vtp"))
            io_utils.save_nifti_image(tissue_data['Tumor'].astype(np.uint8), tissue_data['affine'], os.path.join(output_dir, f"active_tumor_mask_iter_{macro_iter+1}.nii.gz"))
            if 'VEGF_field' in tissue_data and tissue_data['VEGF_field'] is not None:
                 io_utils.save_nifti_image(tissue_data['VEGF_field'].astype(np.float32), tissue_data['affine'], os.path.join(output_dir, f"vegf_field_iter_{macro_iter+1}.nii.gz"))

        if np.all(tissue_data['Tumor'] == tissue_data['Tumor_Max_Extent']) and not any_tumor_growth_this_macro:
            logger.info(f"Tumor filled Tumor_Max_Extent. Stopping macro iterations at {macro_iter + 1}.")
            break

    logger.info(f"--- Tumor Angiogenesis (Fixed Extent) Finished. Final graph: {angiogenic_graph.number_of_nodes()} N, {angiogenic_graph.number_of_edges()} E ---")
    return angiogenic_graph# src/utils.py
import numpy as np
import random
import logging
import os
import shutil
from typing import Tuple 

logger = logging.getLogger(__name__)

def set_rng_seed(seed: int):
    """Sets the random seed for Python's random, NumPy, and potentially other libraries."""
    random.seed(seed)
    np.random.seed(seed)
    logger.info(f"Random Number Generator seed set to: {seed}")
    # Add other libraries like TensorFlow/PyTorch if used:
    # tf.random.set_seed(seed)
    # torch.manual_seed(seed)

def get_voxel_volume_from_affine(affine: np.ndarray) -> float:
    """
    Calculates the volume of a single voxel from the NIfTI affine matrix.
    Assumes the affine matrix maps voxel coordinates to physical coordinates.
    The volume is the absolute value of the determinant of the first 3x3 submatrix.

    Args:
        affine (np.ndarray): The 4x4 affine matrix.

    Returns:
        float: The volume of a single voxel.
    """
    return abs(np.linalg.det(affine[:3, :3]))

def voxel_to_world(voxel_coords: np.ndarray, affine: np.ndarray) -> np.ndarray:
    """
    Converts voxel coordinates to world (physical) coordinates.

    Args:
        voxel_coords (np.ndarray): A (N, 3) array of voxel coordinates (i, j, k).
        affine (np.ndarray): The 4x4 NIfTI affine matrix.

    Returns:
        np.ndarray: A (N, 3) array of world coordinates (x, y, z).
    """
    voxel_coords = np.asarray(voxel_coords)
    if voxel_coords.ndim == 1:
        voxel_coords = voxel_coords.reshape(1, -1)
    
    # Add homogeneous coordinate
    homogeneous_coords = np.hstack((voxel_coords, np.ones((voxel_coords.shape[0], 1))))
    
    # Apply affine transformation
    world_coords_homogeneous = homogeneous_coords @ affine.T
    
    return world_coords_homogeneous[:, :3]

def world_to_voxel(world_coords: np.ndarray, affine: np.ndarray) -> np.ndarray:
    """
    Converts world (physical) coordinates to voxel coordinates.
    Uses the inverse of the affine matrix. Resulting voxel coordinates might be fractional.

    Args:
        world_coords (np.ndarray): A (N, 3) array of world coordinates (x, y, z).
        affine (np.ndarray): The 4x4 NIfTI affine matrix.

    Returns:
        np.ndarray: A (N, 3) array of voxel coordinates (i, j, k).
    """
    world_coords = np.asarray(world_coords)
    if world_coords.ndim == 1:
        world_coords = world_coords.reshape(1, -1)

    # Add homogeneous coordinate
    homogeneous_coords = np.hstack((world_coords, np.ones((world_coords.shape[0], 1))))
    
    # Invert affine matrix
    inv_affine = np.linalg.inv(affine)
    
    # Apply inverse affine transformation
    voxel_coords_homogeneous = homogeneous_coords @ inv_affine.T
    
    return voxel_coords_homogeneous[:, :3]

def distance_squared(p1: np.ndarray, p2: np.ndarray) -> float:
    """Computes the squared Euclidean distance between two 3D points."""
    return np.sum((p1 - p2)**2)

def distance(p1: np.ndarray, p2: np.ndarray) -> float:
    """Computes the Euclidean distance between two 3D points."""
    return np.sqrt(np.sum((p1 - p2)**2))

def normalize_vector(v: np.ndarray) -> np.ndarray:
    """Normalizes a vector."""
    norm = np.linalg.norm(v)
    if norm == 0:
        return v
    return v / norm

def create_output_directory(base_dir: str, sim_name: str = "gbo_sim", timestamp: bool = True) -> str:
    """
    Creates a unique output directory.
    Example: base_dir/YYYYMMDD_HHMMSS_sim_name or base_dir/sim_name
    """
    from datetime import datetime
    if timestamp:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        dir_name = f"{ts}_{sim_name}"
    else:
        dir_name = sim_name
    
    full_path = os.path.join(base_dir, dir_name)
    
    if os.path.exists(full_path):
        # Option 1: Overwrite (dangerous)
        # shutil.rmtree(full_path) 
        # Option 2: Add a suffix
        count = 1
        new_full_path = f"{full_path}_{count}"
        while os.path.exists(new_full_path):
            count += 1
            new_full_path = f"{full_path}_{count}"
        full_path = new_full_path
        logger.warning(f"Output directory {os.path.join(base_dir, dir_name)} existed. Using {full_path} instead.")

    os.makedirs(full_path, exist_ok=True)
    logger.info(f"Created output directory: {full_path}")
    return full_path

def is_voxel_in_bounds(voxel_coord: np.ndarray, shape: Tuple[int, ...]) -> bool:
    """Checks if a voxel coordinate is within the bounds of a given shape."""
    voxel_coord = np.asarray(voxel_coord) # Ensure it's a numpy array
    if voxel_coord.ndim == 0 or voxel_coord.shape[0] != len(shape): # Check for scalar or mismatched dimensions
        return False
    return all(0 <= voxel_coord[d] < shape[d] for d in range(len(shape)))



if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    # Test RNG seed
    set_rng_seed(123)
    print(f"Random float after seed 123: {random.random()}")
    set_rng_seed(123)
    print(f"Random float after seed 123 again: {random.random()}")
    print(f"Numpy random array after seed 123: {np.random.rand(3)}")
    set_rng_seed(123)
    print(f"Numpy random array after seed 123 again: {np.random.rand(3)}")

    # Test affine transformations
    # A typical NIfTI affine for 1mm isotropic voxels, origin at corner
    dummy_affine = np.array([
        [1.0, 0.0, 0.0, 0.0],
        [0.0, 1.0, 0.0, 0.0],
        [0.0, 0.0, 1.0, 0.0],
        [0.0, 0.0, 0.0, 1.0]
    ])
    # A more realistic affine (e.g. -1mm x, 1mm y, 1mm z, with an offset)
    # RAS orientation: X points Left to Right, Y Posterior to Anterior, Z Inferior to Superior
    # If voxel (0,0,0) is at world (-90, 90, -120) and voxels are 1mm:
    # This means i maps to -X, j maps to +Y, k maps to +Z (LPI orientation for data array)
    # If data array is stored radiological (i from R->L), then first column of affine is positive.
    # Assuming standard interpretation (voxel index increases, world coordinate increases along axis basis vector)
    # Let's use a simple affine for testing:
    test_affine = np.array([
        [-1.0, 0.0, 0.0, 100.0],  # Voxel i -> World -x direction; (0,0,0) maps to x=100
        [0.0, 1.0, 0.0, -50.0],  # Voxel j -> World +y direction; (0,0,0) maps to y=-50
        [0.0, 0.0, 2.0, -20.0],  # Voxel k -> World +z direction, 2mm thick; (0,0,0) maps to z=-20
        [0.0, 0.0, 0.0, 1.0]
    ])

    print(f"Voxel volume for test_affine: {get_voxel_volume_from_affine(test_affine)} mm^3 (expected 2)")

    voxel_pts = np.array([[0,0,0], [10,20,5]])
    world_pts = voxel_to_world(voxel_pts, test_affine)
    print(f"Voxel points:\n{voxel_pts}")
    print(f"Converted to World points:\n{world_pts}")
    # Expected for [0,0,0]: [100, -50, -20]
    # Expected for [10,20,5]: [-1*10+100, 1*20-50, 2*5-20] = [90, -30, -10]

    reconverted_voxel_pts = world_to_voxel(world_pts, test_affine)
    print(f"Reconverted to Voxel points:\n{reconverted_voxel_pts}")
    assert np.allclose(voxel_pts, reconverted_voxel_pts), "Voxel-World-Voxel conversion failed"

    # Test distance
    p1 = np.array([0,0,0])
    p2 = np.array([3,4,0])
    print(f"Distance squared between {p1} and {p2}: {distance_squared(p1, p2)} (expected 25)")
    print(f"Distance between {p1} and {p2}: {distance(p1, p2)} (expected 5)")

    # Test output directory
    base_output = "temp_test_output"
    os.makedirs(base_output, exist_ok=True)
    path1 = create_output_directory(base_output, "my_sim")
    path2 = create_output_directory(base_output, "my_sim") # Should create my_sim_1
    print(f"Path1: {path1}")
    print(f"Path2: {path2}")
    shutil.rmtree(base_output)
    print("Cleaned up temp_test_output directory.")# src/io_utils.py
import nibabel as nib
import numpy as np
import pyvista as pv
import networkx as nx
import os
import logging
import yaml
from typing import Any, Union, Dict, List, Tuple

logger = logging.getLogger(__name__)

def load_nifti_image(filepath: str) -> tuple[np.ndarray, np.ndarray, Any] | tuple[None, None, None]:
    """
    Loads a NIfTI image.

    Args:
        filepath (str): Path to the .nii or .nii.gz file.

    Returns:
        tuple: (data_array, affine_matrix, header) or (None, None, None) if loading fails.
               data_array is usually in (L,P,I) or (R,A,S) depending on how it was saved.
               Affine maps voxel indices to world space (often RAS).
    """
    if not os.path.exists(filepath):
        logger.warning(f"NIfTI file not found: {filepath}. Skipping.")
        return None, None, None
    try:
        img = nib.load(filepath)
        data = img.get_fdata()
        affine = img.affine
        header = img.header
        logger.info(f"Loaded NIfTI image: {filepath}, Shape: {data.shape}, Voxel size from affine: {np.diag(affine)[:3]}")
        return data.astype(np.float32), affine, header # Cast to float for calculations
    except Exception as e:
        logger.error(f"Error loading NIfTI file {filepath}: {e}")
        return None, None, None

def save_nifti_image(data_array: np.ndarray, affine: np.ndarray, filepath: str, header: nib.Nifti1Header = None):
    """
    Saves a NumPy array as a NIfTI image.

    Args:
        data_array (np.ndarray): The image data.
        affine (np.ndarray): The affine matrix for the image.
        filepath (str): Path to save the .nii or .nii.gz file.
        header (nib.Nifti1Header, optional): NIfTI header. If None, a minimal one is created.
    """
    try:
        # Ensure data type is compatible, e.g. float32 or int16
        # Nifti1Image constructor will handle appropriate dtype based on data.
        # If data is boolean, convert to int8 or uint8
        if data_array.dtype == bool:
            data_array = data_array.astype(np.uint8)
            
        img = nib.Nifti1Image(data_array, affine, header=header)
        nib.save(img, filepath)
        logger.info(f"Saved NIfTI image to: {filepath}")
    except Exception as e:
        logger.error(f"Error saving NIfTI file {filepath}: {e}")
        raise

def load_arterial_centerlines_vtp(filepath: str) -> pv.PolyData | None:
    """
    Loads arterial centerlines from a VTP file.
    Assumes the VTP file contains points and lines representing vessel segments.
    It should ideally have a 'radius' point data array.

    Args:
        filepath (str): Path to the .vtp file.

    Returns:
        pyvista.PolyData: The loaded PolyData object or None if loading fails.
    """
    if not os.path.exists(filepath):
        logger.warning(f"Arterial centerline file not found: {filepath}. Skipping.")
        return None
    try:
        mesh = pv.read(filepath)
        logger.info(f"Loaded arterial centerlines from VTP: {filepath}")
        if 'radius' not in mesh.point_data:
            logger.warning(f"VTP file {filepath} does not contain 'radius' point data. Defaulting or errors might occur.")
        # Could add more checks here, e.g., for line connectivity
        return mesh
    except Exception as e:
        logger.error(f"Error loading VTP file {filepath}: {e}")
        return None

def load_arterial_centerlines_txt(filepath: str, radius_default: float = 0.1) -> pv.PolyData | None:
    """
    Loads arterial centerlines from a TXT file and converts to PyVista PolyData.
    Expected TXT format:
    Each line: x y z [radius]
    If radius is not present, radius_default is used.
    Segments are assumed to connect consecutive points.
    A more robust format might specify connectivity explicitly. For now, assume polylines.

    Args:
        filepath (str): Path to the .txt file.
        radius_default (float): Default radius if not specified in the file.

    Returns:
        pyvista.PolyData: A PolyData object representing the centerlines, or None if loading fails.
    """
    if not os.path.exists(filepath):
        logger.warning(f"Arterial centerline TXT file not found: {filepath}. Skipping.")
        return None
    
    points = []
    radii = []
    try:
        with open(filepath, 'r') as f:
            for line_num, line in enumerate(f):
                line = line.strip()
                if not line or line.startswith('#'): # Skip empty lines or comments
                    continue
                parts = list(map(float, line.split()))
                if len(parts) == 3:
                    points.append(parts)
                    radii.append(radius_default)
                elif len(parts) == 4:
                    points.append(parts[:3])
                    radii.append(parts[3])
                else:
                    logger.warning(f"Skipping malformed line {line_num+1} in {filepath}: {line}")
        
        if not points:
            logger.error(f"No valid points found in TXT file: {filepath}")
            return None

        points_np = np.array(points)
        radii_np = np.array(radii)

        # Create PolyData: assumes a single polyline for simplicity
        # For multiple disconnected arteries, the TXT format would need to be richer or
        # processed to identify separate polylines.
        num_points = len(points_np)
        lines = np.empty((num_points - 1, 3), dtype=int)
        lines[:, 0] = 2  # Each line segment has 2 points
        lines[:, 1] = np.arange(num_points - 1)
        lines[:, 2] = np.arange(1, num_points)
        
        poly = pv.PolyData(points_np, lines=lines)
        poly.point_data['radius'] = radii_np
        
        logger.info(f"Loaded arterial centerlines from TXT: {filepath}, {num_points} points.")
        return poly

    except Exception as e:
        logger.error(f"Error loading TXT file {filepath}: {e}")
        return None


def save_vascular_tree_vtp(graph: nx.DiGraph, filepath: str,
                           pos_attr='pos', radius_attr='radius', pressure_attr='pressure', flow_attr='flow'):
    """
    Saves a vascular tree (NetworkX graph) to a VTP file.
    Nodes store positions and radii. Edges define connectivity.
    """
    points = []
    point_radii = []
    point_pressures = []
    lines_connectivity = [] # Changed name for clarity, this is for pv.PolyData(points, lines=HERE)
    edge_flows = [] 

    node_to_idx = {node_id: i for i, node_id in enumerate(graph.nodes())}

    for node_id, data in graph.nodes(data=True):
        if pos_attr not in data:
            logger.warning(f"Node {node_id} missing '{pos_attr}' attribute. Skipping for point data.")
            continue
        points.append(data[pos_attr])
        point_radii.append(data.get(radius_attr, 0.0))
        point_pressures.append(data.get(pressure_attr, np.nan)) 

    if not points:
        logger.error("No points to save in the vascular tree. VTP file will be empty or invalid.")
        empty_poly = pv.PolyData()
        empty_poly.save(filepath)
        logger.error(f"Saved an empty VTP file to: {filepath} due to no valid points in the graph.")
        return

    # Build the lines array for PolyData constructor
    # Format: [n_points_in_line0, pt0_idx, pt1_idx, n_points_in_line1, ptA_idx, ptB_idx, ...]
    raw_lines_for_pv = []
    for u, v, data in graph.edges(data=True):
        if u in node_to_idx and v in node_to_idx:
            raw_lines_for_pv.extend([2, node_to_idx[u], node_to_idx[v]]) # Each line segment has 2 points
            edge_flows.append(data.get(flow_attr, np.nan)) 
        else:
            logger.warning(f"Edge ({u}-{v}) references missing node. Skipping this edge for line connectivity.")

    # Create PolyData object
    # If there are lines, pass them to the constructor.
    # Otherwise, it's just a point cloud.
    if raw_lines_for_pv:
        poly_data = pv.PolyData(np.array(points), lines=np.array(raw_lines_for_pv))
    else:
        poly_data = pv.PolyData(np.array(points)) # Will be a point cloud if no edges

    logger.debug(f"PolyData created. Number of points: {poly_data.n_points}, Number of cells (lines): {poly_data.n_cells}")
    logger.debug(f"Number of edge_flows collected: {len(edge_flows)}")
    
    # Add point data
    if points: # Check if points list is not empty before trying to assign
        poly_data.point_data[radius_attr] = np.array(point_radii)
        if any(not np.isnan(p) for p in point_pressures): 
            poly_data.point_data[pressure_attr] = np.array(point_pressures)
    
    # Add cell data (flow) only if there are cells and corresponding flow data
    if edge_flows and poly_data.n_cells > 0:
        if poly_data.n_cells == len(edge_flows):
            poly_data.cell_data[flow_attr] = np.array(edge_flows)
        else:
            # This case should ideally not be hit if graph processing and polydata creation are correct
            logger.error(
                f"Critical mismatch assigning cell data! "
                f"PolyData n_cells: {poly_data.n_cells}, "
                f"Number of flow values: {len(edge_flows)}. "
                f"Flow data will NOT be saved for cells."
            )
            # Decide: either don't add flow data, or pad/truncate (not ideal)
            # For now, we won't add it if there's a mismatch.
    
    try:
        poly_data.save(filepath)
        logger.info(f"Saved vascular tree to VTP: {filepath}")
    except Exception as e:
        logger.error(f"Error saving vascular tree to VTP {filepath}: {e}")
        raise

def save_simulation_parameters(config: dict, filepath: str):
    """Saves the simulation configuration to a YAML file."""
    try:
        with open(filepath, 'w') as f:
            yaml.dump(config, f, sort_keys=False, indent=4)
        logger.info(f"Saved simulation parameters to: {filepath}")
    except Exception as e:
        logger.error(f"Error saving simulation parameters to {filepath}: {e}")
        raise


if __name__ == '__main__':
    # Setup basic logging for testing
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Create dummy data directory
    test_data_dir = "temp_io_test_data"
    os.makedirs(test_data_dir, exist_ok=True)

    # --- Test NIfTI I/O ---
    dummy_nifti_path = os.path.join(test_data_dir, "dummy.nii.gz")
    shape = (10, 10, 10)
    affine = np.eye(4)
    affine[0,0] = affine[1,1] = affine[2,2] = 0.5 # 0.5mm isotropic voxels
    data = np.random.rand(*shape).astype(np.float32)
    
    print(f"\n--- Testing NIfTI I/O ---")
    save_nifti_image(data, affine, dummy_nifti_path)
    loaded_data, loaded_affine, _ = load_nifti_image(dummy_nifti_path)
    
    if loaded_data is not None:
        assert np.allclose(data, loaded_data), "NIfTI data mismatch"
        assert np.allclose(affine, loaded_affine), "NIfTI affine mismatch"
        print("NIfTI I/O test successful.")
    else:
        print("NIfTI loading failed.")

    # Test loading non-existent NIfTI
    load_nifti_image("non_existent.nii.gz")


    # --- Test VTP I/O (arterial centerlines) ---
    print(f"\n--- Testing VTP I/O (arterial centerlines) ---")
    dummy_vtp_path = os.path.join(test_data_dir, "dummy_arteries.vtp")
    # Create a simple PyVista PolyData object
    points = np.array([[0,0,0], [1,1,0], [2,0,0]], dtype=float)
    lines = np.array([2, 0, 1, 2, 1, 2]) # Connect point 0-1, then 1-2
    poly = pv.PolyData(points, lines=lines)
    poly.point_data['radius'] = np.array([0.5, 0.4, 0.3])
    poly.save(dummy_vtp_path)
    
    loaded_poly = load_arterial_centerlines_vtp(dummy_vtp_path)
    if loaded_poly:
        assert loaded_poly.n_points == 3, "VTP point count mismatch"
        assert 'radius' in loaded_poly.point_data, "VTP radius data missing"
        print("VTP arterial centerline I/O test successful.")
    else:
        print("VTP loading failed.")
    
    # Test loading non-existent VTP
    load_arterial_centerlines_vtp("non_existent.vtp")

    # --- Test TXT I/O (arterial centerlines) ---
    print(f"\n--- Testing TXT I/O (arterial centerlines) ---")
    dummy_txt_path = os.path.join(test_data_dir, "dummy_arteries.txt")
    with open(dummy_txt_path, "w") as f:
        f.write("# Test TXT file\n")
        f.write("0 0 0 0.5\n")
        f.write("1 1 0 0.4\n")
        f.write("2 0 0\n") # Test with default radius
        f.write("3 1 1 0.2\n")
    
    loaded_poly_txt = load_arterial_centerlines_txt(dummy_txt_path, radius_default=0.1)
    if loaded_poly_txt:
        assert loaded_poly_txt.n_points == 4, "TXT point count mismatch"
        assert 'radius' in loaded_poly_txt.point_data, "TXT radius data missing"
        expected_radii = np.array([0.5, 0.4, 0.1, 0.2])
        assert np.allclose(loaded_poly_txt.point_data['radius'], expected_radii), "TXT radii mismatch"
        print(f"TXT loaded radii: {loaded_poly_txt.point_data['radius']}")
        print("TXT arterial centerline I/O test successful.")
    else:
        print("TXT loading failed.")

    # --- Test Vascular Tree (NetworkX to VTP) Save ---
    print(f"\n--- Testing Vascular Tree (NetworkX to VTP) Save ---")
    graph = nx.DiGraph()
    # Add nodes with positions and radii
    graph.add_node(0, pos=np.array([0,0,0]), radius=0.5, pressure=100.0)
    graph.add_node(1, pos=np.array([1,0,0]), radius=0.4, pressure=90.0)
    graph.add_node(2, pos=np.array([1,1,0]), radius=0.3, pressure=80.0)
    # Add edges with flow
    graph.add_edge(0, 1, flow=10.0)
    graph.add_edge(1, 2, flow=8.0)

    tree_vtp_path = os.path.join(test_data_dir, "vascular_tree.vtp")
    save_vascular_tree_vtp(graph, tree_vtp_path)
    
    # Verify by loading it back with PyVista
    if os.path.exists(tree_vtp_path):
        loaded_tree_poly = pv.read(tree_vtp_path)
        assert loaded_tree_poly.n_points == 3, "Saved tree VTP point count mismatch"
        assert 'radius' in loaded_tree_poly.point_data, "Saved tree VTP radius missing"
        assert 'pressure' in loaded_tree_poly.point_data, "Saved tree VTP pressure missing"
        # Check if flow is present as cell data
        if loaded_tree_poly.n_cells > 0 : # n_cells corresponds to number of lines/edges
             assert 'flow' in loaded_tree_poly.cell_data, "Saved tree VTP flow missing"
        print("Vascular tree (NetworkX to VTP) save test successful.")
    else:
        print("Vascular tree VTP save failed.")
        
    # Test saving an empty graph
    empty_graph = nx.DiGraph()
    empty_tree_vtp_path = os.path.join(test_data_dir, "empty_vascular_tree.vtp")
    save_vascular_tree_vtp(empty_graph, empty_tree_vtp_path)
    if os.path.exists(empty_tree_vtp_path):
        loaded_empty_tree_poly = pv.read(empty_tree_vtp_path)
        assert loaded_empty_tree_poly.n_points == 0, "Empty graph should result in VTP with 0 points."
        print("Saving empty graph to VTP test successful.")

    # --- Test Saving Simulation Parameters ---
    print(f"\n--- Testing Saving Simulation Parameters ---")
    dummy_params_path = os.path.join(test_data_dir, "sim_params.yaml")
    test_config = {"param1": 10, "nested": {"param2": "test"}}
    save_simulation_parameters(test_config, dummy_params_path)
    # Verify by loading
    with open(dummy_params_path, 'r') as f:
        loaded_params = yaml.safe_load(f)
    assert loaded_params["param1"] == 10, "Param save/load mismatch"
    print("Simulation parameter saving test successful.")

    # Clean up dummy data directory
    import shutil
    shutil.rmtree(test_data_dir)
    print(f"\nCleaned up temporary test directory: {test_data_dir}")# src/data_structures.py
import networkx as nx
import numpy as np
import logging

logger = logging.getLogger(__name__)

# --- Vascular Tree (NetworkX Graph) Conventions ---
# Nodes in the graph represent points in 3D space (e.g., bifurcations, terminals, points along a segment).
# Edges represent vessel segments connecting these points.

# Node Attributes:
# - 'id': (any, unique) Unique identifier for the node. Often the NetworkX node key itself.
# - 'pos': (np.ndarray, shape (3,)) 3D coordinates [x, y, z] in physical units (e.g., mm). REQUIRED.
# - 'radius': (float) Radius of the vessel at this node/point in physical units. REQUIRED for many operations.
# - 'type': (str) Type of node, e.g., 'root', 'bifurcation', 'terminal', 'segment_point'.
# - 'pressure': (float) Blood pressure at this node (computed during perfusion modeling).
# - 'flow_demand': (float) For terminal nodes, the flow Q_i required by its territory.
# - 'territory_voxels': (list or np.ndarray) Indices or coordinates of voxels supplied by this terminal.
# - 'parent_measured_terminal_id': (any) For synthetic terminals, ID of the measured artery terminal they originated from.
# - 'is_tumor_vessel': (bool) True if this node is part of a tumor-induced vessel.

# Edge Attributes (for edge u -> v):
# - 'length': (float) Length of the vessel segment in physical units. Can be calculated from node positions.
# - 'radius': (float) Radius of the segment. Can be average of u and v radii, or u's radius if flow is from u to v.
#           Consistency needed: often derived from flow and Murray's law.
# - 'flow': (float) Blood flow rate through the segment (computed during perfusion modeling).
# - 'resistance': (float) Hydraulic resistance of the segment (computed for perfusion modeling).
# - 'is_tumor_vessel': (bool) True if this segment is part of a tumor-induced vessel.


def create_empty_vascular_graph() -> nx.DiGraph:
    """Creates an empty directed graph for the vascular tree."""
    return nx.DiGraph()

def add_node_to_graph(graph: nx.DiGraph, node_id: any, pos: np.ndarray, radius: float, 
                      node_type: str = 'default', **kwargs):
    """
    Adds a node with standard attributes to the vascular graph.
    
    Args:
        graph (nx.DiGraph): The graph to add the node to.
        node_id (any): Unique ID for the node.
        pos (np.ndarray): 3D position [x,y,z].
        radius (float): Vessel radius at this node.
        node_type (str): Type of node.
        **kwargs: Additional attributes to set for the node.
    """
    if not isinstance(pos, np.ndarray) or pos.shape != (3,):
        raise ValueError("Position 'pos' must be a 3-element NumPy array.")
    if not isinstance(radius, (int, float)) or radius < 0:
        raise ValueError("Radius must be a non-negative number.")

    attrs = {
        'pos': pos,
        'radius': radius,
        'type': node_type,
    }
    attrs.update(kwargs) # Add any extra attributes
    graph.add_node(node_id, **attrs)
    # logger.debug(f"Added node {node_id} with attributes: {attrs}")

def add_edge_to_graph(graph: nx.DiGraph, u_id: any, v_id: any, **kwargs):
    """
    Adds an edge with standard attributes to the vascular graph.
    Length is automatically calculated if node positions exist.
    
    Args:
        graph (nx.DiGraph): The graph to add the edge to.
        u_id (any): ID of the source node.
        v_id (any): ID of the target node.
        **kwargs: Additional attributes to set for the edge.
    """
    if not graph.has_node(u_id) or not graph.has_node(v_id):
        logger.error(f"Cannot add edge ({u_id}-{v_id}): one or both nodes do not exist.")
        raise ValueError(f"Nodes {u_id} or {v_id} not in graph.")

    attrs = {}
    # Calculate length
    pos_u = graph.nodes[u_id].get('pos')
    pos_v = graph.nodes[v_id].get('pos')
    if pos_u is not None and pos_v is not None:
        length = np.linalg.norm(pos_u - pos_v)
        attrs['length'] = length
    else:
        logger.warning(f"Could not calculate length for edge ({u_id}-{v_id}) due to missing node positions.")

    # Example: Edge radius could be based on upstream node or average
    # For now, let's assume it might be set explicitly or derived later
    # radius_u = graph.nodes[u_id].get('radius')
    # if radius_u is not None:
    #    attrs['radius'] = radius_u 

    attrs.update(kwargs) # Add any extra attributes
    graph.add_edge(u_id, v_id, **attrs)
    # logger.debug(f"Added edge ({u_id}-{v_id}) with attributes: {attrs}")


# --- Tissue Data Structure ---
# Represented as a dictionary of NumPy arrays, plus affine and voxel volume.
# Example:
# tissue_data = {
#     'WM': wm_array,         # (X, Y, Z) binary or fractional mask
#     'GM': gm_array,         # (X, Y, Z)
#     'Tumor': tumor_array,   # (X, Y, Z)
#     'CSF': csf_array,       # (X, Y, Z)
#     'domain_mask': combined_mask, # (X, Y, Z) boolean array defining relevant voxels
#     'metabolic_demand_map': demand_map, # (X, Y, Z) float array of q_met per voxel
#     'affine': affine_matrix, # 4x4 np.ndarray
#     'voxel_volume': volume_per_voxel, # float (mm^3 or m^3)
#     'world_coords_flat': world_coords_of_domain_voxels # (N_domain_voxels, 3)
#     'voxel_indices_flat': voxel_indices_of_domain_voxels # (N_domain_voxels, 3)
# }

def get_metabolic_demand_map(tissue_segmentations: dict, config: dict, voxel_volume: float) -> np.ndarray:
    """
    Generates a metabolic demand map (q_met * dV) from tissue segmentations.
    
    Args:
        tissue_segmentations (dict): Dictionary of tissue type arrays (e.g., 'WM', 'GM', 'Tumor').
                                     Values are masks (0 or 1, or fractional 0-1).
        config (dict): Configuration dictionary with metabolic rates.
        voxel_volume (float): Volume of a single voxel (e.g., in mm^3).

    Returns:
        np.ndarray: A 3D array of the same shape as segmentations, where each voxel
                    contains the total metabolic demand (e.g., in mm^3_blood/s).
    """
    from src import config_manager as cfg_mgr # to use get_param

    # Get shape from one of the segmentations
    shape = None
    for seg_name, seg_array in tissue_segmentations.items():
        if seg_array is not None:
            shape = seg_array.shape
            break
    if shape is None:
        logger.error("No valid tissue segmentations provided to create metabolic map.")
        return None

    demand_map = np.zeros(shape, dtype=np.float32)
    
    q_rates = cfg_mgr.get_param(config, "tissue_properties.metabolic_rates")

    if 'GM' in tissue_segmentations and tissue_segmentations['GM'] is not None:
        demand_map += tissue_segmentations['GM'] * q_rates.get('gm', 0.0)
    if 'WM' in tissue_segmentations and tissue_segmentations['WM'] is not None:
        demand_map += tissue_segmentations['WM'] * q_rates.get('wm', 0.0)
    if 'CSF' in tissue_segmentations and tissue_segmentations['CSF'] is not None:
        demand_map += tissue_segmentations['CSF'] * q_rates.get('csf', 0.0) # usually 0
    
    # Tumor can have rim/core distinction if available, or a single tumor type
    if 'Tumor' in tissue_segmentations and tissue_segmentations['Tumor'] is not None:
        # Simple model: use 'tumor_rim' for all tumor voxels if 'tumor_core' isn't specified
        # or if the tumor segmentation isn't further divided.
        # A more complex model would require separate Tumor_Rim and Tumor_Core segmentations.
        tumor_rate = q_rates.get('tumor_rim', q_rates.get('tumor', 0.0)) # Fallback to 'tumor' if 'tumor_rim' not there
        demand_map += tissue_segmentations['Tumor'] * tumor_rate
    elif 'Tumor_Rim' in tissue_segmentations and tissue_segmentations['Tumor_Rim'] is not None:
         demand_map += tissue_segmentations['Tumor_Rim'] * q_rates.get('tumor_rim', 0.0)
         if 'Tumor_Core' in tissue_segmentations and tissue_segmentations['Tumor_Core'] is not None:
             demand_map += tissue_segmentations['Tumor_Core'] * q_rates.get('tumor_core', 0.0)
             
    return demand_map * voxel_volume # Now it's total demand per voxel (e.g. mm^3/s)

if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)

    # Test vascular graph functions
    g = create_empty_vascular_graph()
    add_node_to_graph(g, 0, pos=np.array([0.,0.,0.]), radius=0.5, node_type='root', pressure=100)
    add_node_to_graph(g, 1, pos=np.array([1.,0.,0.]), radius=0.4, node_type='bifurcation')
    add_node_to_graph(g, 2, pos=np.array([2.,1.,0.]), radius=0.3, node_type='terminal', flow_demand=5)
    add_node_to_graph(g, 3, pos=np.array([2.,-1.,0.]), radius=0.3, node_type='terminal', flow_demand=5)

    add_edge_to_graph(g, 0, 1, flow=10)
    add_edge_to_graph(g, 1, 2, flow=5)
    add_edge_to_graph(g, 1, 3, flow=5)

    print("\n--- Vascular Graph Test ---")
    print(f"Nodes: {g.nodes(data=True)}")
    print(f"Edges: {g.edges(data=True)}")
    assert np.isclose(g.edges[(0,1)]['length'], 1.0)
    assert g.nodes[2]['flow_demand'] == 5

    # Test metabolic demand map
    print("\n--- Metabolic Demand Map Test ---")
    dummy_config = {
        "tissue_properties": {
            "metabolic_rates": {
                "gm": 0.01, "wm": 0.003, "csf": 0.0, "tumor_rim": 0.02
            }
        }
    }
    gm_seg = np.zeros((3,3,3), dtype=np.uint8)
    gm_seg[1,1,1] = 1
    wm_seg = np.zeros((3,3,3), dtype=np.uint8)
    wm_seg[0,0,0] = 1
    
    tissue_segs = {'GM': gm_seg, 'WM': wm_seg}
    voxel_vol = 2.0 # mm^3

    demand_map = get_metabolic_demand_map(tissue_segs, dummy_config, voxel_vol)
    if demand_map is not None:
        print(f"Demand map at (1,1,1) (GM): {demand_map[1,1,1]} (Expected: 0.01 * 2.0 = 0.02)")
        assert np.isclose(demand_map[1,1,1], 0.01 * voxel_vol)
        print(f"Demand map at (0,0,0) (WM): {demand_map[0,0,0]} (Expected: 0.003 * 2.0 = 0.006)")
        assert np.isclose(demand_map[0,0,0], 0.003 * voxel_vol)
        print(f"Demand map at (2,2,2) (Background): {demand_map[2,2,2]} (Expected: 0.0)")
        assert np.isclose(demand_map[2,2,2], 0.0)
        print("Metabolic demand map test successful.")
    else:
        print("Metabolic demand map test failed.")